<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习之激活函数</title>
      <link href="/2020/01/18/shen-du-xue-xi-zhi-ji-huo-han-shu/"/>
      <url>/2020/01/18/shen-du-xue-xi-zhi-ji-huo-han-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="一、为什么要使用激活函数"><a href="#一、为什么要使用激活函数" class="headerlink" title="一、为什么要使用激活函数"></a>一、为什么要使用激活函数</h1><p>​    神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比”阶跃函数”更佳平滑.</p><p>​    阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。</p><p>​    激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。</p><p>​    输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。</p><h1 id="二、常见的激活函数"><a href="#二、常见的激活函数" class="headerlink" title="二、常见的激活函数"></a>二、常见的激活函数</h1><p>深度学习是基于人工神经网络的结构，信号经过非线性的激活函数，传递到下一层神经元，经过该层神经元的激活处理后继续往下传递，如此循环往复，直到输出层。正是由于这些非线性函数的反复叠加，才使得神经网络有足够的非线性拟合，选择不同的激活函数将影响整个深度神经网络的效果。下面简单介绍几种常用的激活函数。</p><h2 id="1、阶跃函数"><a href="#1、阶跃函数" class="headerlink" title="1、阶跃函数:"></a>1、阶跃函数:</h2><p>$$<br>h(x) =\begin{cases}1, &amp; x &gt; 0 \\0, &amp; x \leq 0\end{cases}<br>$$</p><p>​    <img src="n2.png" alt></p><h2 id="2、sigmoid函数（S函数）"><a href="#2、sigmoid函数（S函数）" class="headerlink" title="2、sigmoid函数（S函数）"></a>2、sigmoid函数（S函数）</h2><p>$$<br>h(x) = \dfrac {1}{1+e^{-x}}<br>$$</p><p><img src="n1.png" alt></p><p>Sigmoid函数是传统的神经网络和深度学习领域开始时使用频率最高的激活函数，是便于求导的平滑函数，但是容易出现梯度消失问题（gradient vanishing）。函数输出并不是zero-centered，即Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢，并且所使用的幂运算相对来讲比较耗时。</p><h2 id="2、tanh函数"><a href="#2、tanh函数" class="headerlink" title="2、tanh函数"></a>2、tanh函数</h2><p>$$<br>h(x) = \dfrac {e^x - e^{-x}}{e^x + e^{-x}}<br>$$</p><p><img src="n5.png" alt></p><p>tanh函数解决了zero-centered的输出问题，但梯度消失的问题和幂运算的问题仍然存在。</p><h2 id="3、Relu函数"><a href="#3、Relu函数" class="headerlink" title="3、Relu函数"></a>3、Relu函数</h2><p>$$<br>h(x) = \{0, x\}<br>$$</p><p><img src="n6.png" alt></p><p>ReLU函数本质上是一个取最大值函数，非全区间可导，但是在计算过程中可以取sub-gradient（次梯度）。</p><p>ReLU在正区间内解决了梯度消失问题，只需要判断输入是否大于0，所以计算速度非常快，收敛速度远远快于Sigmoid和tanh函数。但ReLU函数的输出同样不是zero-centered，并且存在<strong>Dead ReLU Problem</strong>，即某些神经元可能永远不会参与计算，导致其相应的参数无法被更新。有两个主要原因可能会导致这种情况产生：参数初始化及学习速率太高，从而导致在训练过程中参数更新过大，使网络进入这种情况。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用Adagrad等自动调节学习率的算法。</p><p>尽管存在这两个问题，ReLU目前仍是最常用的activation function，<strong>在搭建人工神经网络的时候推荐优先尝试！</strong></p><h2 id="4、Leaky-ReLU函数"><a href="#4、Leaky-ReLU函数" class="headerlink" title="4、Leaky ReLU函数"></a>4、Leaky ReLU函数</h2><p>Leaky ReLU函数表达式：<br>$$<br>f(x) = max\{ 0.01x, x\}<br>$$</p><p>Leaky ReLU函数示意图:</p><p><img src="07.png" alt></p><p>Leaky ReLU函数的提出是为了解决Dead ReLU Problem，将ReLU的前半段设为0.01x而非0。理论上来讲，Leaky ReLU函数有ReLU函数的所有优点，外加不会有Dead ReLU问题，但是在实际操作中并没有完全证明Leaky ReLU函数总是好于ReLU函数。</p><h2 id="5、指数线性单元ELU函数"><a href="#5、指数线性单元ELU函数" class="headerlink" title="5、指数线性单元ELU函数"></a>5、指数线性单元ELU函数</h2><p>$$<br>h(x) =\begin{cases}\alpha(e^x-1), &amp; x \leq 0 \\x, &amp; x &gt; 0\end{cases}<br>$$</p><p>指数线性单元（Exponential Linear Unit）激活函数由Djork等人提出，被证实有较高的噪声鲁棒性，同时能够使得神经元的平均激活均值趋近为0，对噪声更具有鲁棒性。由于需要计算指数，计算量较大。</p><h2 id="6、SELU函数"><a href="#6、SELU函数" class="headerlink" title="6、SELU函数"></a>6、SELU函数</h2><p>SELU函数表达式为:<br>$$<br>h(x) =\lambda \begin{cases}\alpha(e^x-1), &amp; x \leq 0 \\x, &amp; x &gt; 0\end{cases}<br>$$</p><p>自归一化神经网络（Self-Normalizing Neural Networks）中提出只需要把激活函数换成SELU就能使得输入在经过一定层数之后变成固定的分布。SELU是给ELU乘上系数λ，即SELU(x)=λ·ELU(x)。</p><h2 id="7、softmax函数"><a href="#7、softmax函数" class="headerlink" title="7、softmax函数"></a>7、softmax函数</h2><p>Softmax函数可视为Sigmoid函数的泛化形式，其本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于(0，1)之间。一般用于多分类神经网络输出。<br>$$<br>\sigma(x) =  \dfrac {e^{a_k}}{ \sum_{i=1}^n e^{a^i}   }<br>$$<br><strong>注意</strong>: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。</p><p>改进**: 先进行归一化，再求值</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1010</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">990</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#  array([nan, nan, nan])  没有计算正确的值</span>mi <span class="token operator">=</span> np<span class="token punctuation">.</span>min<span class="token punctuation">(</span>a<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 990                                 </span>ma <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span>                                 nor <span class="token operator">=</span> <span class="token punctuation">(</span>a<span class="token operator">-</span>mi<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>ma<span class="token operator">-</span>mi<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 归一化 array([1. , 0.5, 0. ])   </span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>nor<span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>nor<span class="token punctuation">)</span><span class="token punctuation">)</span>                 <span class="token comment" spellcheck="true"># array([0.50648039, 0.30719589, 0.18632372])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​    一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此<strong>输出层的softmax函数一般会被省略</strong></p><p>​    求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设计模式之单例模式</title>
      <link href="/2020/01/15/she-ji-mo-shi-zhi-dan-li-mo-shi/"/>
      <url>/2020/01/15/she-ji-mo-shi-zhi-dan-li-mo-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="一、什么是单例模式"><a href="#一、什么是单例模式" class="headerlink" title="一、什么是单例模式"></a>一、什么是单例模式</h1><p>单例模式（Singleton Pattern，也称为单件模式），使用最广泛的设计模式之一。其意图是保证一个类仅有一个实例，并提供一个访问它的全局访问点，该实例被所有程序模块共享。</p><p>定义一个单例类：</p><ol><li>私有化它的构造函数，以防止外界创建单例类的对象；</li><li>使用类的私有静态指针变量指向类的唯一实例；</li><li>使用一个公有的静态方法获取该实例;</li></ol><h1 id="二、懒汉版（Lazy-Singleton）"><a href="#二、懒汉版（Lazy-Singleton）" class="headerlink" title="二、懒汉版（Lazy Singleton）"></a>二、懒汉版（Lazy Singleton）</h1><p>懒汉版（Lazy Singleton）：单例实例在第一次使用时才进行初始化，这叫做延迟初始化。</p><pre class="line-numbers language-c++"><code class="language-c++">// version 1.0class Singleton{private:    Singleton()= default;    ~Singleton() = default;    Singleton(const Singleton &)= default;    Singleton&operator=(const Singleton&)= default;public:    static Singleton* getInstance(){        if (nullptr == instance){            instance = new Singleton;        }        return instance;    };private:    static Singleton* instance;};Singleton* Singleton::instance = nullptr;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>问题1：</strong>Lazy Singleton存在内存泄露的问题，有两种解决方法：</p><ol><li>使用智能指针</li><li>使用静态的嵌套类对象</li></ol><p>对于第二种解决方法，代码如下：</p><pre class="line-numbers language-c++"><code class="language-c++">// version 1.1class Singleton{private:    Singleton()= default;    ~Singleton() = default;    Singleton(const Singleton &)= default;    Singleton&operator=(const Singleton&)= default;public:    static Singleton* getInstance(){        if (nullptr == instance){            instance = new Singleton;        }        return instance;    };private:    static Singleton* instance;    class Deletor{    public:        ~Deletor(){            if (nullptr != Singleton::instance){                delete Singleton::instance;                Singleton::instance = nullptr;            }            std::cout << "删除instance" << std::endl;        }    };    static Deletor deletor;};Singleton* Singleton::instance = nullptr;Singleton::Deletor Singleton::deletor;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在程序运行结束时，系统会调用静态成员<code>deletor</code>的析构函数，该析构函数会删除单例的唯一实例。使用这种方法释放单例对象有以下特征：</p><ul><li>在单例类内部定义专有的嵌套类。</li><li>在单例类内定义私有的专门用于释放的静态成员。</li><li>利用程序在结束时析构全局变量的特性，选择最终的释放时机。</li></ul><p>在单例类内再定义一个嵌套类，总是感觉很麻烦。</p><p><strong>问题2：</strong>这个代码在单线程环境下是正确无误的，但是当拿到多线程环境下时这份代码就会出现race condition，注意version 1.0与version 1.1都不是线程安全的。要使其线程安全，能在多线程环境下实现单例模式，我们首先想到的是利用同步机制来正确的保护我们的shared data。可以使用双检测锁模式（DCL: Double-Checked Locking Pattern）：</p><pre class="line-numbers language-c++"><code class="language-c++">static Singleton* getInstance(){    if (nullptr == instance){        std::lock_guard<std::mutex> lock(m); // 基于作用域的加锁，超出作用域，自动调用析构函数解锁        if (nullptr == instance){            instance = new Singleton;        }    }    return instance;};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意，线程安全问题仅出现在第一次初始化（new）过程中，而在后面获取该实例的时候并不会遇到，也就没有必要再使用lock。双检测锁很好地解决了这个问题，它通过加锁前检测是否已经初始化，避免了每次获取实例时都要首先获取锁资源。</p><p>加入DCL后，其实还是有问题的，关于memory model。在某些内存模型中（虽然不常见）或者是由于编译器的优化以及运行时优化等等原因，使得instance虽然已经不是<code>nullptr</code>但是其所指对象还没有完成构造，这种情况下，另一个线程如果调用getInstance()就有可能使用到一个不完全初始化的对象。换句话说，就是代码中第2行：<code>if(instance == NULL)</code>和第六行<code>instance = new Singleton();</code>没有正确的同步，在某种情况下会出现new返回了地址赋值给instance变量而Singleton此时还没有构造完全，当另一个线程随后运行到第2行时将不会进入if从而返回了不完全的实例对象给用户使用，造成了严重的错误。</p><p><strong>Best of All:</strong></p><p>C++11规定了local static在多线程条件下的初始化行为，要求编译器保证了内部静态变量的线程安全性。在C++11标准下，《Effective C++》提出了一种更优雅的单例模式实现，使用函数内的 local static 对象。这样，只有当第一次访问<code>getInstance()</code>方法时才创建实例。这种方法也被称为Meyers’ Singleton。C++0x之后该实现是线程安全的，C++0x之前仍需加锁。</p><pre class="line-numbers language-c++"><code class="language-c++">class Singleton{private:    Singleton() = default;    ~Singleton() = default;    Singleton(const Singleton &)= default;    Singleton&operator=(const Singleton&)= default;public:    static Singleton& getInstance(){        static Singleton instance;        return instance;    }};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="三、饿汉版（Eager-Singleton）"><a href="#三、饿汉版（Eager-Singleton）" class="headerlink" title="三、饿汉版（Eager Singleton）"></a>三、饿汉版（Eager Singleton）</h1><p>饿汉版（Eager Singleton）：指单例实例在程序运行时被立即执行初始化</p><pre class="line-numbers language-c++"><code class="language-c++">class Singleton{private:    Singleton()= default;    ~Singleton() = default;    Singleton(const Singleton &)= default;    Singleton&operator=(const Singleton&)= default;    static Singleton instance;public:    static Singleton& getInstance(){        return instance;    }};Singleton Singleton::instance;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>由于在main函数之前初始化，所以没有线程安全的问题。但是潜在问题在于no-local static对象（函数外的static对象）在不同编译单元中的初始化顺序是未定义的。也即，static Singleton instance;和static Singleton&amp; getInstance()二者的初始化顺序不确定，如果在初始化完成之前调用 getInstance() 方法会返回一个未定义的实例。</p><h1 id="四、总结："><a href="#四、总结：" class="headerlink" title="四、总结："></a>四、总结：</h1><ul><li>Eager Singleton 虽然是线程安全的，但存在潜在问题；</li><li>Lazy Singleton通常需要加锁来保证线程安全，但局部静态变量版本在C++11后是线程安全的；</li><li>局部静态变量版本（Meyers Singleton）最优雅。</li></ul><h1 id="五、C-中static对象的初始化"><a href="#五、C-中static对象的初始化" class="headerlink" title="五、C++中static对象的初始化"></a>五、C++中static对象的初始化</h1><p><strong>non-local static对象（函数外）</strong></p><p>C++规定，non-local static 对象的初始化发生在main函数执行之前，也即main函数之前的单线程启动阶段，所以不存在线程安全问题。但C++没有规定多个non-local static 对象的初始化顺序，尤其是来自多个编译单元的non-local static对象，他们的初始化顺序是随机的。</p><p><strong>local static 对象（函数内）</strong></p><p>对于local static 对象，其初始化发生在控制流第一次执行到该对象的初始化语句时。多个线程的控制流可能同时到达其初始化语句。</p><p>在C++11之前，在多线程环境下local static对象的初始化并不是线程安全的。具体表现就是：如果一个线程正在执行local static对象的初始化语句但还没有完成初始化，此时若其它线程也执行到该语句，那么这个线程会认为自己是第一次执行该语句并进入该local static对象的构造函数中。这会造成这个local static对象的重复构造，进而产生<strong>内存泄露</strong>问题。所以，local static对象在多线程环境下的重复构造问题是需要解决的。</p><p>而C++11则在语言规范中解决了这个问题。C++11规定，在一个线程开始local static 对象的初始化后到完成初始化前，其他线程执行到这个local static对象的初始化语句就会等待，直到该local static 对象初始化完成。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>​    <a href="https://zhuanlan.zhihu.com/p/37469260" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37469260</a></p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉之分割</title>
      <link href="/2020/01/12/ji-suan-ji-shi-jue-zhi-fen-ge/"/>
      <url>/2020/01/12/ji-suan-ji-shi-jue-zhi-fen-ge/</url>
      
        <content type="html"><![CDATA[<h1 id="一、语义分割和实例分割的定义"><a href="#一、语义分割和实例分割的定义" class="headerlink" title="一、语义分割和实例分割的定义"></a>一、语义分割和实例分割的定义</h1><p><img src="01.png" alt></p><p><img src="02.png" alt></p><p>定义1： <strong>语义分割</strong><br>    将图片中的所有像素进行分类（包括背景） ，不区分具体目标， 仅做像素级分类。 例如， 将图a上面一行图片进行语义分割的结果为下面一行图片。</p><p>定义2： <strong>实例分割</strong><br>    对于有多个目标的图片， 对每个目标完成像素级的分类， 并区分每一个目标（即区分同一个类别但属于不同的目标） 。 例如， 对图b左边一列图片进行实例分割， 可以得到右面一列图片 。</p><h1 id="二、FCN"><a href="#二、FCN" class="headerlink" title="二、FCN"></a>二、FCN</h1><p>语义分割需要对图片的每个像素做分类， 最容易想到的方法是什么？ </p><h2 id="1、最简单的语义分割方法"><a href="#1、最简单的语义分割方法" class="headerlink" title="1、最简单的语义分割方法"></a>1、最简单的语义分割方法</h2><p><img src="03.png" alt></p><p>既然是对原图的每个像素进行分类， 那么将输出层的每一个像素点当作分类任务做一个Softmax即可。 即对于一张W*H*3的图片， 中间经过若干层卷积， 卷积的kernel大小为W*H， 最终通过一个W*H*C（C为之前定义好的类别个数） 的Softmax对原图的每一个像素进行分类， </p><p>这种最简单解决方案的问题是中间卷积层尺度太大， 内存和计算量的消耗也非常大。  </p><h2 id="2、FCN架构"><a href="#2、FCN架构" class="headerlink" title="2、FCN架构"></a>2、FCN架构</h2><p><img src="04.png" alt></p><p>在2015年， Long、 Shelhamer、 Darrell和Noh等人提出了在卷积神经网络内部使用下采样和上采样结合的方式实现图片的语义分割， 该方案的大体结构如上图所示。 在这个过程中， 下采样主要是通过我们之前学习过的Pooling（池化） 和调整卷积的stride（步幅） 来实现的， 上采样的过程其实就是与下采样相反， 主要包括Unpooling（反池化）和Deconvolution（转置卷积） 两种方式。 FCN采用的是双线性插值进行上采样。</p><p><img src="05.png" alt></p><p>上图给出了几种常见的Unpooling方法， 其中a和简单易懂， 这里不做赘述， 值得一提的是c中Max Unpooling的方式， 若使用该方法， 则需要在下采样Max Pooling时记录对应Max元素的位置， 在Unpooling的时候将每个元素写回到对应的位置上。 所以，这种方式需要注意下采样和上采样过程对应的问题。 </p><p>不论是Unpooling方式还是Deconvolution的方式， 关键层都是以卷积的方式进行操作， 不涉及类似全连接这种操作， 因此我们通常称这种网络为全卷积网络（Full Connected Network， <a href="https://arxiv.org/abs/1411.4038]" target="_blank" rel="noopener">FCN</a>） 。 </p><h2 id="3、跳跃融合"><a href="#3、跳跃融合" class="headerlink" title="3、跳跃融合"></a>3、跳跃融合</h2><p>论文示图：</p><p><img src="07.png" alt></p><p>跳跃融合是为了获得更加精细的分割结果，我们先看下图：</p><p>整个FCN网络基本原理如图<strong>（只是原理示意图）</strong>：</p><ol><li>image经过多个conv和一个max pooling变为pool1 feature，宽高变为1/2</li><li>pool1 feature再经过多个conv一个max pooling变为pool2 feature，宽高变为1/4</li><li>pool2 feature再经过多个conv一个max pooling变为pool3 feature，宽高变为1/8</li><li>……</li><li>直到pool5 feature，宽高变为1/32。</li></ol><p><img src="01.jpg" alt></p><p>那么：</p><p>​    pool1到pool5是五个最大池化层，因此图中的pool5层的大小是原图image的1/32（1/$2^5$）。</p><ol><li>对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即分割图）。</li><li>对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature<strong>逐点相加</strong>，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。</li><li>对于FCN-8s，首先进行pool4+2x upsampled feature<strong>逐点相加</strong>，然后又进行pool3+2x upsampled<strong>逐点相加</strong>，即进行更多次特征融合。然后对融合的feature进行8倍上采样，并softmax prediction，获得8x upsampled feature prediction。</li></ol><p>作者在原文种给出3种网络结果对比，明显可以看出效果：FCN-32s &lt; FCN-16s &lt; FCN-8s，即<strong>使用多层feature融合有利于提高分割准确性</strong>。</p><p><img src="02.jpg" alt></p><h2 id="4、CNN图像语义分割总结"><a href="#4、CNN图像语义分割总结" class="headerlink" title="4、CNN图像语义分割总结"></a>4、CNN图像语义分割总结</h2><p><strong>CNN图像语义分割也就基本上是这个套路：</strong></p><ol><li><strong>下采样+上采样：Convolution + Deconvolution／Resize</strong></li><li><strong>多尺度特征融合：特征逐点相加／特征channel维度拼接</strong></li><li><strong>获得像素级别的segment map：对每一个像素点进行判断类别</strong></li></ol><h1 id="三、UNet"><a href="#三、UNet" class="headerlink" title="三、UNet"></a>三、UNet</h1><p>全卷积网络可用作语义分割， 最经典的莫过于2015年夺得CVPR最佳论文的《Fully Convolutional Networks for Semantic Segmentation》。 这里扩展一下， 介绍在经典FCN基础上改良之后广泛应用在医疗影像里面的U-Net， 它的网络结构如图下所示。 </p><p><img src="06.png" alt></p><p>U-Net的网络结构非常清晰， 即下采样后经过2次卷积后再次下采样， 而上采样则使用反卷积的方式， 并与对应大小的下采样特征层进行连接， 然后经过2次卷积后再反卷积。 这个网络结构很简单， 因此对于小样本量数据集有很好的效果。 </p><p>与FCN逐点相加不同，U-Net采用将特征在channel维度拼接在一起，形成更“厚”的特征。所以：</p><p><strong>语义分割网络在特征融合时也有2种办法：</strong></p><ol><li><strong>FCN式的逐点相加，对应caffe的EltwiseLayer层，对应tensorflow的tf.add()</strong></li><li><strong>U-Net式的channel维度拼接融合，对应caffe的ConcatLayer层，对应tensorflow的tf.concat()</strong></li></ol><h1 id="四、SegNet"><a href="#四、SegNet" class="headerlink" title="四、SegNet"></a>四、SegNet</h1><p>发表于2016年，作者 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Senior Member</p><p><strong>模型结构：</strong></p><p><img src="08.png" alt></p><p>SegNet与FCN类似，也移除了全连接层，不同的是，采用了编码器-解码器结构，在上采样和下采样中使用了不同的方法。</p><p>SegNet的编码器部分使用的是VGG16的前13层卷积网络,每个编码器层都对应一个解码器层,最终解码器的输出被送入soft-max分类器以独立的为每个像素产生类概率。</p><p>每个编码器由数个蓝色层（卷积层,批归一化层,RELU层）以及一个Pooling层（2x2窗口,步进2,最大池化）组成,输出相当于系数为2的下采样。</p><p><strong>创新点</strong>:</p><p>在 encoder 部分的最大池化操作时<strong>记录了最大值所在位置（索引）</strong>，然后在 decoder upsampling时直接将数据放在原先的位置，，这样在上采样阶段就无需学习。上采样后得到的是一个稀疏特征图，再通过普通的卷积得到稠密特征图，再重复上采样。SegNet 主要比较的是 FCN，FCN解码时用transposed convolutions+双线性插值来获得特征图，再和对应 encoder 的特征图相加得到输出。SegNet 的优势就在于不用保存整个 encoder 部分的特征图，只需保存池化索引，节省内存空间；第二个是不用反卷积，上采样阶段无需学习，尽管上采样完以后还要卷积学习。</p><p><img src="09.png" alt></p><p><strong>优点</strong>:</p><p>SegNet的主要动机是场景理解的应用。因此它在设计的时候考虑了要在预测期间保证内存和计算时间上的效率。</p><h1 id="五、PSPNet"><a href="#五、PSPNet" class="headerlink" title="五、PSPNet"></a>五、PSPNet</h1><p>Pyramid Scene Parsing Network（PSPNet）是CVPR2017上关于场景解析的文章，拿到了2016年ImageNet比赛中scene parsing任务的冠军，当然也常用来做语义分割。这篇文章出发点是在语义分割算法中引入更多的上下文信息（context information）和局部信息， 这样能够避免许多误分割，具体说明如下。</p><p><img src="03.jpg" alt></p><p>​    1） 语义之间存在一定的关联性， 比上图的第一行， 传统FCN将“船”预测成了“汽车”， 但如果考虑上下文信息就可以知道， “汽车”是不可能出现在“河”上面的。<br>​    2） 对于易混淆的类， 如上图第二行的“摩天大楼”， 传统FCN将该物体的一部分分割为“摩天大楼”， 一部分分割为“建筑”， 而实际上这是同一个物体， 或者被分为“摩天大楼”， 或者被分为“建筑”， 而不是两者都有。 产生这种问题的原因是没有考虑局部的物体信息。<br>​    3） 一些细小的物体在语义分割时经常被忽略，如上图的第三行的“枕头”。 “枕头”的花色和床单很接近从而导致被FCN误分类， 如果我们更加关注局部特征的获取， 那么这里就可以更好地区分出“枕头”与“床”。 </p><p><strong>模型结构</strong></p><p><img src="11.png" alt></p><p><em><em>创新点: </em></em>金字塔池化模型</p><ul><li>第一个红色部分是独立的全局池化</li><li>第二个黄色是将特征层分为4块不同的子区域进行池化，后面蓝色、 绿色的层以此类推。</li><li>池化后为了保证不同尺度的权重， 通过1*1的卷积对特征层降维到1/N（N为不同尺度的数量， 这里是4） 。 然后将N个降维后的特征层上采样到原特征层尺寸并与原特征层进行合并， 最终形成多尺度融合后的特征层 。</li></ul><h1 id="六、实例分割"><a href="#六、实例分割" class="headerlink" title="六、实例分割"></a>六、实例分割</h1><p>目标检测是将图片中的物体位置检测出来并给出相应的物体类别， 语义分割是给出每个像素的类别， 只区分类别， 不区分是否为同一个物体， 而实例分割则是要给出类别并区分出不同的物体， 如下图所示:</p><p><img src="12.png" alt></p><p>由于实例分割需要识别出“每一个物体”， 因此需要进行目标检测， 而它又需要得到像素级别分割的结果， 因此还需要将语义分割的方法融合进来， 这实际上是一个多任务学习的方法。 多任务学习通常具有两种方式： 一种是堆叠式， 即将不同的任务通过某种方式连接起来进行训练； 另一种是扁平式， 即每个任务单独训练。</p><p><img src="13.png" alt></p><h2 id="1、层叠式"><a href="#1、层叠式" class="headerlink" title="1、层叠式"></a>1、层叠式</h2><p><img src="14.png" alt></p><p>这里首先介绍一下2015年的一篇文章《Instance-aware Semantic Segmentation via Multi-task Network Cascades》， 该方法虽然不是做实例分割的最好方法， 但这种层叠式的“head”也是一种方法， 可供读者拓宽思路。 层叠式方法具体可分为3个步骤， 如上图所示。<br>    1） 检测——使用Faster R-CNN的RPN方法预测所有潜在物体的bbox（这里只做二分类区分是不是物体） 。<br>    2） 分割——将步骤1） 的ROI压缩到指定尺寸（论文中是14*14*128） ， 通过两个全连接层（用来降维） 后预测每个像素点是不是物体。<br>    3） 分类——将步骤1） 和步骤2） 的结果统一到一个尺寸做元素级点乘， 再过两个全连接层后对每个ROI进行分类预测。 </p><h2 id="2、扁平式"><a href="#2、扁平式" class="headerlink" title="2、扁平式"></a>2、扁平式</h2><p>由于目标检测的结果既有紧贴着目标的矩形框， 又有矩形框里目标的类别， 那么是否可以基于目标检测的结果， 直接在上面做FCN实现像素级分割呢。 答案是肯定的，<br>Mask-RCNN做的就是这样的事情。  Mask-RCNN是在目前业界公认的效果较好的检测模型Faster R-CNN的基础上直接添加第三个分割分支 ,如下图所示:</p><p><img src="15.png" alt></p><p><img src="16.png" alt></p><p>由于Faster R-CNN已经有了bbox和bbox的类别， 因此如何定义第三个分割分支的学习方法就是关键。 Mask-RCNN的head设计如上图所示， 即在Faster R-CNN的最后一个特征层（7*7*2048） 上进行上采样（反卷或反池化） ， 然后再对上采样后的特征层进行像素级预测。 这里需要注意的是， 输出层是14*14*80（80是需要预测的类别个数） ， 而不是Softmax对应1个channel， 原因是Mask-RCNN的作者将每个channel看作一个类别，每个channel只采用2分类的Sigmoid来预测该点是否属于该类物品。 </p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>FCN-UNet: <a href="https://blog.csdn.net/justpsss/article/details/77170004" target="_blank" rel="noopener">https://blog.csdn.net/justpsss/article/details/77170004</a></p><p>​                    <a href="https://zhuanlan.zhihu.com/p/31428783" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31428783</a></p><p>SegNet: <a href="https://www.cnblogs.com/tccbj/p/10746406.html" target="_blank" rel="noopener">https://www.cnblogs.com/tccbj/p/10746406.html</a></p><p>《深度学习与图像识别: 原理与实战》</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上采样之转置卷积</title>
      <link href="/2020/01/12/shang-cai-yang-zhi-zhuan-zhi-juan-ji/"/>
      <url>/2020/01/12/shang-cai-yang-zhi-zhuan-zhi-juan-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文翻译自<a href="https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0" target="_blank" rel="noopener">《Up-sampling with Transposed Convolution》</a>，这篇文章对转置卷积和反卷积有着很好的解释。</p><h1 id="一、对于上采用的需求"><a href="#一、对于上采用的需求" class="headerlink" title="一、对于上采用的需求"></a>一、对于上采用的需求</h1><p>当我们用神经网络生成图片的时候，经常需要将一些低分辨率的图片转换为高分辨率的图片。</p><p><img src="01.png" alt></p><p>对于这种上采样（up-sampling）作，目前有着一些插值方法进行处理：</p><ol><li><p>最近邻插值（Nearest neighbor interpolation）</p></li><li><p>双线性插值（Bi-Linear interpolation）</p></li><li><p>双立方插值（Bi-Cubic interpolation）</p></li></ol><p>以上的这些方法都是一些插值方法，需要我们在决定网络结构的时候进行挑选。这些方法就像是人工特征工程一样，并没有给神经网络学习的余地，神经网络不能自己学习如何更好地进行插值，这个显然是不够理想的。</p><h1 id="二、为什么是转置卷积"><a href="#二、为什么是转置卷积" class="headerlink" title="二、为什么是转置卷积"></a>二、为什么是转置卷积</h1><p>转置卷积（Transposed Convolution）常常在一些文献中也称之为反卷积（Deconvolution）和部分跨越卷积（Fractionally-strided Convolution），因为称之为反卷积容易让人以为和数字信号处理中反卷积混起来，造成不必要的误解，因此下文都将称为转置卷积，并且建议各位不要采用反卷积这个称呼。</p><p>如果我们想要我们的网络可以学习到最好地上采样的方法，我们这个时候就可以采用转置卷积。这个方法不会使用预先定义的插值方法，它具有可以学习的参数。理解转置卷积这个概念是很重要的，因为它在若干重要的文献中都有所应用，如：</p><ol><li>在<a href="https://arxiv.org/pdf/1511.06434v2.pdf" target="_blank" rel="noopener">DCGAN</a>中的生成器将会用随机值转变为一个全尺寸（full-size）的图片，这个时候就需要用到转置卷积。</li><li>在语义分割中，会使用卷积层在编码器中进行特征提取，然后在解码层中进行恢复为原先的尺寸，这样才可以对原来图像的每个像素都进行分类。这个过程同样需要用到转置卷积。</li></ol><h1 id="三、卷积操作"><a href="#三、卷积操作" class="headerlink" title="三、卷积操作"></a>三、卷积操作</h1><p> 让我们回顾下卷积操作是怎么工作的，并且我们将会从一个小例子中直观的感受卷积操作。假设我们有一个4×4的矩阵，我们将在这个矩阵上应用3×3的卷积核，并且不添加任何填充（padding），步进参数（stride）设置为1，就像下图所示，输出为一个2×2的矩阵。</p><p><img src="02.png" alt></p><p>这个卷积操作在输入矩阵和卷积核中，对每个元素的乘积进行相加。因为我们没有任何填充和使用1为步长，因此我们只能对这个操作进行4次，因此我们的输出矩阵尺寸为2×2。</p><p><img src="03.png" alt></p><p>这种卷积操作使得输入值和输出值之间存在有位置上的连接关系，举例来说，输入矩阵左上方的值将会影响到输出矩阵的左上方的值。更具体而言，3×3的卷积核是用来连接输入矩阵中的9个值，并且将其转变为输出矩阵的一个值的。一个卷积操作是一个多对一（many-to-one）的映射关系。让我们记住这个，我们接下来将会用得着。</p><h1 id="四、反过来操作"><a href="#四、反过来操作" class="headerlink" title="四、反过来操作"></a>四、反过来操作</h1><p>现在，假设我们想要反过来操作。我们想要将输入矩阵中的一个值映射到输出矩阵的9个值，这将是一个一对多（one-to-many）的映射关系。这个就像是卷积操作的反操作，其核心观点就是用转置卷积。举个例子，我们对一个2×2的矩阵进行上采样为4×4的矩阵。这个操作将会维护一个1对应9的映射关系。</p><p><img src="04.png" alt></p><p>因此就结论而言，卷积操作是多对一，而转置卷积操作是一对多。</p><p>但是我们将如何具体操作呢？为了接下来的讨论，我们需要定义一个卷积矩阵（convolution matrix）和相应的转置卷积矩阵（transposed convolution matrix）。</p><h1 id="五、卷积矩阵"><a href="#五、卷积矩阵" class="headerlink" title="五、卷积矩阵"></a>五、卷积矩阵</h1><p>我们可以将一个卷积操作用一个矩阵表示。这个表示很简单，无非就是将卷积核重新排列到我们可以用普通的矩阵乘法进行矩阵卷积操作。如下图就是原始的卷积核：</p><p><img src="05.png" alt></p><p>我们对这个3×3的卷积核进行重新排列，得到了下面这个4×16的卷积矩阵：</p><p><img src="06.png" alt></p><p>这个便是卷积矩阵了，这个矩阵的每一行都定义了一个卷积操作。下图将会更加直观地告诉你这个重排列是怎么进行的。每一个卷积矩阵的行都是通过重新排列卷积核的元素，并且添加0补充（zero padding）进行的。</p><p><img src="07.png" alt></p><p>为了将卷积操作表示为卷积矩阵和输入矩阵的向量乘法，我们将输入矩阵4×4摊平（flatten）为一个列向量，形状为16×1，如下图所示。</p><p><img src="08.png" alt></p><p>我们可以将这个4×16的卷积矩阵和1×16的输入列向量进行矩阵乘法，这样我们就得到了输出列向量。</p><p><img src="09.png" alt></p><p>这个输出的4×1的矩阵可以重新塑性为一个2×2的矩阵，而这个矩阵正是和我们一开始通过传统的卷积操作得到的一模一样。</p><p><img src="10.png" alt></p><p>简单来说，这个卷积矩阵除了重新排列卷积核的权重之外就没有啥了，然后卷积操作可以通过表示为卷积矩阵和输入矩阵的列向量形式的矩阵乘积形式进行表达。</p><p>所以各位发现了吗，关键点就在于这个卷积矩阵，你可以从16（4×4）到4（2×2）因为这个卷积矩阵尺寸正是4×16的，然后呢，如果你有一个16×4的矩阵，你就可以从4（2×2）到16（4×4）了，这不就是一个上采样的操作吗？</p><h1 id="六、转置卷积矩阵"><a href="#六、转置卷积矩阵" class="headerlink" title="六、转置卷积矩阵"></a>六、转置卷积矩阵</h1><p>我们想要从4（2×2）到16（4×4），因此我们使用了一个16×4的矩阵，但是还有一件事情需要注意，我们是想要维护一个1到9的映射关系。</p><p>假设我们转置这个卷积矩阵$C  (4×16)$变为$C^T (16×4)$。我们可以对$C^T$和列向量$(4×1)$进行矩阵乘法，从而生成一个16×1的输出矩阵。这个转置矩阵正是将一个元素映射到了9个元素。</p><p><img src="11.png" alt></p><p>这个输出可以塑形为4x4的矩阵：</p><p><img src="12.png" alt></p><p>我们只是对小矩阵$(2×2)$进行上采样为一个更大尺寸的矩阵$(4×4)$。这个转置卷积矩阵维护了一个1个元素到9个元素的映射关系，因为这个关系正表现在了其转置卷积元素上。</p><p><strong>需要注意</strong>的是：这里的转置卷积矩阵的参数，不一定从原始的卷积矩阵中简单转置得到的，转置这个操作只是提供了转置卷积矩阵的形状而已。</p><h1 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h1><p>转置卷积操作构建了和普通的卷积操作一样的连接关系，只不过这个是从反向方向开始连接的。我们可以用它进行上采样。另外，这个转置卷积矩阵的参数是可以学习的，因此我们不需要一些人为预先定义的方法。即使它被称为转置卷积，它并不是意味着我们将一些现存的卷积矩阵简单转置并且使用其转置后的值。</p><p>从本质来说，转置卷积不是一个卷积，但是我们可以将其看成卷积，并且当成卷积这样去用。我们通过在输入矩阵中的元素之间插入0进行补充，从而实现尺寸上采样，然后通过普通的卷积操作就可以产生和转置卷积相同的效果了。你在一些文章中将会发现他们都是这样解释转置卷积的，但是这个因为在卷积操作之前需要通过添加0进行上采样，因此是比较低效率的。</p><p><strong>注意</strong>：转置卷积会导致生成图像中出现棋盘效应（checkerboard artifacts），这篇文章<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">《Deconvolution and Checkerboard Artifacts》</a>推荐了一种上采样的操作（也就是插值操作），这个操作接在一个卷积操作后面以减少这种现象。如果你的主要目的是生成尽可能少棋盘效应的图像，那么这篇文章就值得你去阅读。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉之目标检测</title>
      <link href="/2020/01/10/ji-suan-ji-shi-jue-zhi-mu-biao-jian-ce/"/>
      <url>/2020/01/10/ji-suan-ji-shi-jue-zhi-mu-biao-jian-ce/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目标检测在现实中的应用很广泛，我们需要检测图像中的物体位置以及类别。先明确如下2个定义:</p><p>1）定位+分类: 对于仅有一个目标的图片，检测出该目标所处的位置以及该目标的类别</p><p>2）目标检测: 对于有对个目标的图片，检测出所有目标所处的位置及其类别。</p><p>深度学习用于目标检测的算法从思路上来看，可以分为两大类，一类是<strong>two stage</strong>的方法，先由算法生成一系列候选框，在对候选框内的目标进行分类；另一类是<strong>one stage</strong>的方法，直接回归目标的类别概率和位置（矩形框左上角坐标与右下角坐标或左上角坐标与矩形长宽）。</p><p>One-Stage代表作：YOLO， SSD, Retina-Net</p><p>Two-Stage代表作:    R-CNN,  Fast R-CNN, Faster R-CNN</p><h1 id="一、目标检测算法常用的概念"><a href="#一、目标检测算法常用的概念" class="headerlink" title="一、目标检测算法常用的概念"></a>一、目标检测算法常用的概念</h1><h2 id="1、Bounding-Box-BBox"><a href="#1、Bounding-Box-BBox" class="headerlink" title="1、Bounding Box(BBox)"></a>1、Bounding Box(BBox)</h2><p>bbox是包含物体的最小矩形，该物体应在最小矩形内部。物体检测中关于物体位置的信息输出是一组(x,y,w,h)数据，其中x,y代表着bbox的左上角(或者其他固定点，可自定义)，对应的w,h表示bbox的宽和高.一组(x,y,w,h)可以唯一的确定一个定位框。</p><h2 id="2、IOU-Intersection-over-Union"><a href="#2、IOU-Intersection-over-Union" class="headerlink" title="2、IOU(Intersection over Union)"></a>2、IOU(Intersection over Union)</h2><p>IOU其实就是两个矩形面积的交集除以并集， 如图所示。 一般情况下， 当IOU&gt;=0.5时， 可以认为两个矩形基本相交，所以在这个任务中， 假定在两个矩形框中， 1个矩形代表ROI， 另一个代表真实的矩形框， 那么当ROI和真实矩形框的IOU&gt;=0.5时则认为是正样本， 其余为负样本。 </p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/04.png" alt></p><h2 id="3、非极大抑制-Non-Maximum-Suppression-又称NMS"><a href="#3、非极大抑制-Non-Maximum-Suppression-又称NMS" class="headerlink" title="3、非极大抑制(Non-Maximum Suppression 又称NMS)"></a>3、非极大抑制(Non-Maximum Suppression 又称NMS)</h2><p>非极大值抑制(NMS)可以看做是局部最大值的搜索问题，把不是极大值的抑制掉，在物体检测上，就是对一个目标有多个标定框，使用极大值抑制算法滤掉多余的标定框。</p><h1 id="二、分类-定位"><a href="#二、分类-定位" class="headerlink" title="二、分类+定位"></a>二、分类+定位</h1><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/01.png" alt></p><p>分类问题返回目标是某一类别的概率，定位问题则需要模型返回目标所在的外界矩形框，即目标的（x, y, w, y）四元组。有一个比较容易实现的思路，将定位当作回归问题，具体的步骤如下：</p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/02.png" alt></p><p>​    1）训练一个分类模型，例如，AlexNet、VGGNet等。</p><p>​    2）在分类网络最后一个卷积层的特征图（feature map）上添加“regression head”</p><p>​    3）同时训练“classification head”和“regression head”， 为了同时训练分类和定位（ 定位是回归问题） 两个问题， 最终损失函数是分类和定位两个“head”产生的损失的加权和。 </p><p>​    4）在预测时同时使用分类和回归“head”得到分类+定位的结果。 这里需要强调一下的是， 分类预测出的结果就是C个类别， 回归预测的结果可能有两种： 一种是类别无关， 输出4个值； 一种是类别相关， 输出4*C个值， 这就要看读者想要哪种结果了。 </p><p>补充说明:</p><p>​    神经网络中不同的“head”通常用来训练不同的目标， 每个“head”的损失函数和优化方向均不相同。 如果你想让一个网络实现多个功能， 那么通常是在神经网络后面接多个不同功能的“head”。 </p><ol><li></li></ol><h1 id="三、R-CNN"><a href="#三、R-CNN" class="headerlink" title="三、R-CNN"></a>三、R-CNN</h1><p>R-CNN: selective search + CNN + SVM + regression</p><p>由于目标检测和图像分类的不同，一张图可能存在多个目标，因此为了定位和识别出图片中的目标，一种暴力的目标检测方法就是使用滑动窗口,从左到右,从上到下扫描图片,然后用分类器识别窗口中的目标.为了检测出不同的目标,或者同一目标但大小不同,必须使用不同大小,不同宽高比的滑动窗口。把滑动窗口框出来的图片块resize(因为很多分类器只接受固定大小的图片输入)后,送给CNN分类器,CNN提取出4096个特征.然后使用SVM做分类,用线性回归做bounding box预测。但是这种方法有一个缺点就是： 它会产生很多无用的子区域。而 Selective Search 是另一种基于区域的方法，该方法能大大减少子区域的数量。并且在 R-CNN 和 Fast R-CNN 中均利用该算法来生成 Region Proposals。</p><h2 id="1、selective-search"><a href="#1、selective-search" class="headerlink" title="1、selective search"></a>1、selective search</h2><p>[selective search][<a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf]使用的是按层次合并算法（Hierarchical" target="_blank" rel="noopener">http://www.huppelen.nl/publications/selectiveSearchDraft.pdf]使用的是按层次合并算法（Hierarchical</a> Grouping），基本思路如下：首先使用论文[“Efficient Graph-Based Image Segmentation”][<a href="http://cs.brown.edu/people/pfelzens/segment/]中的方法生成一些起始的小区域，之后使用贪心算法将区域归并到一起：先计算所有临近区域间的相似度（通过颜色，纹理，吻合度，大小等相似度），将最相似的两个区域归并，然后重新计算临近区域间的相似度，归并相似区域直至整幅图像成为一个区域。" target="_blank" rel="noopener">http://cs.brown.edu/people/pfelzens/segment/]中的方法生成一些起始的小区域，之后使用贪心算法将区域归并到一起：先计算所有临近区域间的相似度（通过颜色，纹理，吻合度，大小等相似度），将最相似的两个区域归并，然后重新计算临近区域间的相似度，归并相似区域直至整幅图像成为一个区域。</a></p><p>该算法有三个优势：</p><ol><li>捕捉不同尺度(Capture All Scales):穷举搜索（Exhaustive Selective）通过改变窗口大小来适应物体的不同尺度，选择搜索（Selective Search）同样无法避免这个问题。算法采用了图像分割（Image Segmentation）以及使用一种层次算法（Hierarchical Algorithm）有效地解决了这个问题。</li><li>多样化(Diversification):单一的策略无法应对多种类别的图像。使用颜色（color）、纹理（texture）、大小（size）等多种策略对分割好的区域（region）进行合并。</li><li>计算速度快(Fast to Compute)</li></ol><p>该方法主要包括两个内容：</p><ol><li>Hierarchical Grouping Algorithm</li><li>Diversification Strategies</li></ol><p><strong>1.1 Efficient Graph-Based Image Segmentation</strong></p><p>这篇论文的思想是把图像看作为一个图(graph)，图中的节点就是图像中的像素，而权重就是相邻像素之间的差异性。然后通过贪心算法不断融合差异性小的像素或者区域为一个区域，将图最后划分为数个区域(regions或者是components)，从而完成对图像的分割。</p><p>我们可以思考一件很直观的事情，两个区域之所以成为两个区域，而没有融合成为一个区域，是不是就是意味着这个区域内部元素的差异性要小于两个区域之间的差异性呢？换言之，如果区域之间的差异性比区域内部的差异性还要小的话，那实际上这两个区域就应该融合成为一个区域，因为比它们差异性大的都成为了一个区域了，那这两个区域也应该成为一个区域才对。这便是作者提出的判断两个区域是否应该融合为一个区域的标准，也是这篇文章的一个重要的思想。</p><p>算法步骤步骤：</p><ol start="0"><li>对于图G的所有边，按照权值进行排序（升序）</li><li>$S^0$是一个原始分割，相当于每个顶点当做是一个分割区域</li><li>q = 1,2,…,m 重复Step 3的操作（m为边的条数，也就是每次处理一条边）</li><li>根据上次$S^{q-1}$的构建。选择一条边$o_q = (v_i, v_j)$，如果$v_i$和$v_j$在分割的互不相交的区域中，比较这条边的权值与这两个分割区域之间的最小分割内部差MInt，如果$w(o_q(v_i, v_j))$ &lt;= MInt，那么合并这两个区域，其他区域不变；如果否，什么都不做。</li><li>最后得到的就是所求的分割$S=S^m$</li></ol><p><strong>1.2 Hierarchical Grouping Algorithm（层次合并算法）</strong></p><pre class="line-numbers language-shell"><code class="language-shell">输入: 一张图片输出：候选的目标位置集合L算法：1: 利用切分方法得到候选的区域集合R = {r1,r2,…,rn}2: 初始化相似集合S = ϕ3: foreach 遍历邻居区域对(ri,rj) do4:     计算相似度s(ri,rj)5:     S = S∪s(ri,rj)6: while S not=ϕ do7:     从S中得到最大的相似度s(ri,rj)=max(S)8:     合并对应的区域rt = ri ∪ rj9:     移除ri对应的所有相似度：S = S\s(ri,r*)10:    移除rj对应的所有相似度：S = S\s(r*,rj)11:    计算rt对应的相似度集合St12:    S = S ∪ St13:    R = R ∪ rt14: L = R中所有区域对应的bounding box（即包围该区域的最小矩形框）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>1.3 Diversification Strategies（多样性策略）</strong></p><p>​    作者在这里主要考虑了三种不同的多样性策略：</p><ul><li>颜色空间多样性，考虑RGB、灰度、HSV及其变种等</li><li>相似度多样性，既考虑颜色相似度，又考虑纹理、大小、重叠情况等。</li><li>初始化区域多样性，通过改变阈值初始化原始区域，阈值越大，分割的区域越少。</li></ul><h2 id="2、R-CNN训练过程"><a href="#2、R-CNN训练过程" class="headerlink" title="2、R-CNN训练过程"></a>2、R-CNN训练过程</h2><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/03.png" alt></p><p>（1）选出潜在的目标候选区域（ROI）</p><p>​    R-CNN使用selective search 的方法选出了2000个潜在物体候选框</p><p>（2）训练一个好的特征提取器</p><p>​    R-CNN的提出者使用卷积神经网络AlexNet提取4096维的特征向量， 实际上使用VGGNet、GoogLeNet或ResNet等也可以。  AlexNet等网络要求输入的图片尺寸是固定的， 而ROI尺寸大小不定， 这就需要将每个ROI调整到指定尺寸，调整的方法有很多种，尺寸缩放、包含上下文、不包含上下文等。</p><p>​    为了获得一个好的特征提取器，一般会在ImageNet预训练好的模型基础上做调整，唯一的改动就是将ImageNet中的1000个类别的输出改为（C+1）个输出，其中，C是真实需要预测的类别个数，1是背景类。</p><p>​    提到训练， 就一定要有正样本和负样本 , 通过IOU获取正负样本。</p><p>​    至此， R-CNN的第二步特征提取器就可以开始训练了， 不过在训练过程中应注意， 需要对负样本进行采样， 因为训练数据中正样本太少会导致正负样本极度不平衡。 最终在该步得到的是一个卷积神经网络的特征提取器， 其特征是一个4096维特征向量。 </p><p>（3）训练最终的分类器</p><p>​    下面为每个类别单独训练一个SVM分类器。 这里介绍一个小技巧， SVM的训练也需要选择正负样本， R-CNN的提出者做了一个实验来选择最优IOU阈值， 最终仅仅选择真实值的矩形框作为正样本。     </p><p>（4）训练回归模型</p><p>​    为每个类训练一个回归模型， 用来微调ROI与真实矩形框位置和大小的偏差 </p><h2 id="3、预测阶段的步骤"><a href="#3、预测阶段的步骤" class="headerlink" title="3、预测阶段的步骤"></a>3、预测阶段的步骤</h2><p>​    1） 使用selective search方法先选出2000个ROI。</p><p>​    2） 所有ROI调整为特征提取网络所需的输入大小并进行特征提取， 得到与2000个ROI对应的2000个4096维的特征向量。</p><p>​    3） 将2000个特征向量分别输入到SVM中， 得到每个ROI预测的类别。</p><p>​    4） 通过回归网络微调ROI的位置。</p><p>​    5） 最终使用非极大值抑制（Non-Maximum Suppression， NMS） 方法对同一个类别的ROI进行合并得到最终检测结果。 NMS的原理是得到每个矩形框的分数（置信度）， 如果两个矩形框的IOU超过指定阈值， 则仅仅保留分数大的那个矩形框。 </p><h2 id="4、R-CNN的问题"><a href="#4、R-CNN的问题" class="headerlink" title="4、R-CNN的问题"></a>4、R-CNN的问题</h2><ul><li>不论是训练还是预测， 都需要对selective search出来的2000个ROI全部通过CNN的Forward过程来获取特征， 这个过程花费的时间会非常长。 </li><li>卷积神经网络的特征提取器和用来预测分类的SVM是分开的， 也就是特征提取的过程不会因SVM和回归的调整而更新。 </li><li>R-CNN具有非常复杂的操作流程， 而且每一步都是分裂的， 如特征提取器通过Softmax分类获得，最终的分类结果由SVM获得， 矩形框的位置则是通过回归方式获得 </li></ul><h2 id="5、一些容易困惑的问题"><a href="#5、一些容易困惑的问题" class="headerlink" title="5、一些容易困惑的问题"></a>5、一些容易困惑的问题</h2><ol><li><p>关于正负样本的问题</p><p>一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。</p></li><li><p>CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？</p><p>这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm.</p></li><li><p>SVM训练阶段</p><p>这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000<em>4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096</em>N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。</p></li><li><p>位置精修</p><p>目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。</p></li><li><p>测试阶段</p><p>使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。</p><p>（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。</p></li></ol><h1 id="四、SPPNet"><a href="#四、SPPNet" class="headerlink" title="四、SPPNet"></a>四、SPPNet</h1><p>容易看出这里面存在的一些性能瓶颈：</p><ul><li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li><li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li></ul><p>但是为什么CNN需要固定的输入呢？CNN网络可以分解为卷积网络部分以及全连接网络部分。我们知道卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数都不能固定。</p><p>何凯明团队的SPPNet给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出pooling一下，但是这个pooling窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPPNet在这个想法上继续加入SPM的思路，SPM其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成1份，4份，8份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPPNet首次将这种思想应用在CNN中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？</p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/05.png" alt></p><p>上面这个图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层。通过上述方法虽然解决了CNN输入任意大小图片的问题，但是还是需要重复为每个region proposal提取特征啊。</p><h1 id="五、Fast-R-CNN"><a href="#五、Fast-R-CNN" class="headerlink" title="五、Fast R-CNN"></a>五、Fast R-CNN</h1><p>类似于RCNN，Fast RCNN首先通过Selective Search产生一系列的区域候选，然后通过通过CNN提取每个区域候选的特征，之后训练分类网络以及区域回归网络。对比SPPNet，我们可以看出Fast RCNN的区别所在，首先是将SPP换成了ROI Poling。ROI Poling可以看作是空间金字塔池化的简化版本，它通过将区域候选对应的卷积层特征图进行最大池化就好了。</p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/06.png" alt></p><p><strong>具体训练步骤</strong></p><p>​    1）将整张图片和ROI直接输入到全卷积的CNN中， 得到特征层和对应在特征层上的ROI（特征层的ROI信息可用其几何位置加卷积坐标公式推导得出）。</p><p>​    2）与R-CNN类似， 为了使不同尺寸的ROI可以统一进行训练， Fast R-CNN将每块候选区域通过池化的方法调整到指定的M*N， 此时特征层上将调整后的ROI作为分类器的训练数据。 与R-CNN不同的是， 这里将分类和回归任务合并到一起进行训练，这样就将整个流程串联起来。  即先将整张图通过卷积神经网络进行处理， 然后在特征层上找到ROI对应的位置并取出， 对取出的ROI进行池化（此处的池化方法有很多） 。池化后， 全部2000个M*N个训练数据通过全连接层并分别经过2个head： softmax分类以及L2回归， 最终的损失函数是分类和回归的损失函数的加权和。 利用这种方式即可实现端到端的训练。 </p><p>Fast R-CNN极大地提升了目标检测训练和预测的速度， 如图所示。 从图中我们可以看出，Fast R-CNN将训练时长从R-CNN的84小时下降到了8.75小时， 每张图片平均总预测时长从49秒降低到2.3秒。 从图中我们还可以看出， 在Fast R-CNN预测的这2.3秒中， 真正的预测过程仅占0.32秒， 而Region proposal占用了绝大多数的时间。 </p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/08.png" alt></p><h1 id="六、Faster-R-CNN"><a href="#六、Faster-R-CNN" class="headerlink" title="六、Faster R-CNN"></a>六、Faster R-CNN</h1><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/09.png" alt></p><p>Faster R-CNN作为目标检测的经典方法在现今很多实战项目和比赛中频频出现。 其实， Faster RCNN就是在Fast R-CNN的基础上构建一个小的网络， 直接产生Region Proposal来代替其他方法（如selective search） 得到ROI。 这个小型的网络被称为<br>区域建议网络（Region Proposal Network， RPN） 。Faster R-CNN的训练流程如图所示， 其中的RPN是关键， 其余流程与Fast R-CNN基本一致。 </p><p><strong>RPN原理</strong></p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/10.png" alt></p><p>RPN的核心思想是构建一个小的全卷积网络， 对于任意大小的图片， 输出ROI的具体位置以及该ROI是否为物体。 RPN网络在卷积神经网的最后一个特征层上滑动。 </p><p>上图a中最下面灰色的网格表示卷积神经网络的特征层， 红框表示RPN的输入， 其大小为3*3， 而后连接到256维的一个低维向量上。 这个3*3的窗口滑动经过整个特征层， 并且每次计算都将经过这256维的向量并最终输出2个结果： 该3*3滑动窗口位置中是否有物体， 以及该滑动窗口对应物体的矩形框位置。如果还是不好理解， 那么我们将图a中的RPN顺时针旋转90度， 如图b所示， 现在我们可以很清晰地看出神经网络的结构了， 这里input维度是9， 即图a中的3*3大小 。</p><p>​    为了适应多种形状的物体， RPN定义了k种不同尺度的滑窗（因为有的目标是长的， 有的是扁的，有的是大的， 有的是小的， 统一用一个3*3的滑窗难以很好地拟合多种情况） ， 它有一个专业的名词——anchor， 每个anchor都是以特征层（feature<br>map） 上的像素点为中心并且根据其尺度大小进行后续计算的。 在Faster-RCNN论文中， 滑窗在特征层的每个位置上使用3种大小和3种比例， 共3*3=9种anchor， 在图a中， n=9。 </p><p>​    根据上面的介绍， 我们知道RPN包含2类输出：二分类网络输出是否为物体， 回归网络返回矩形框位置对应的4个值。</p><p>​    接下来， 我们看一下训练过程中的一些细节问题。 首先， 针对分类任务， 对于滑窗产生的每一个anchor都计算该anchor与真实标记矩形框的IOU。 当IOU大于0.7时， 便认为该anchor中含有物体； 当IOU小于0.3时， 便认为该anchor中不包含物体； 当IOU介于0.3～0.7时， 则不参与网络训练的迭代过程。</p><p>​    对于回归任务， 这里定义为anchor中心点的横、纵坐标以及anchor的宽、 高， 学习目标为anchor与真实bbox在这四个值上的偏移。 RPN为一个全卷积网络， 可以用随机梯度下降的方式端到端地进行训练。  </p><p>​    这里需要注意的是， 训练过程中能与真实物体矩形框相交的IOU大于0.7的anchor并不多， 它们绝大多数都是负样本， 因此会导致正负样本比例严重失衡， 从而影响识别效果。 因此， 在RPN训练的过程， 对每个batch进行随机采样（每个batch中有256个<br>样本） 并保证正负样本的比例为1： 1， 而当正样本数量小于128时， 取全部的正样本， 其余的则随机使用负样本进行补全。</p><p>​    使用RPN产生ROI的好处是可以与检测网络共享卷积层， 使用随机梯度下降的方式端到端地进行训练。 接下来我们看下Faster R-CNN的训练过程， 具体<br>步骤如下 :</p><p>​    1） 使用ImageNet模型初始化, 独立训练一个RPN网络。<br>​    2） 仍然用ImageNet模型初始化， 以及第1步里产生的建议区域训练Fast R-CNN， 得到物体的实际类别以及微调的矩形框位置。<br>​    3） 使用第2步中的网络初始化RPN， 固定前面的卷积层， 只调整RPN层的参数。<br>​    4） 固定前面的卷积层， 只训练并调整Fast RCNN的FC层。 </p><p><img src="%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/11.png" alt></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>图像分割：<a href="https://blog.csdn.net/xbcReal/article/details/75897836" target="_blank" rel="noopener">https://blog.csdn.net/xbcReal/article/details/75897836</a></p><p>selective search: <a href="https://www.cnblogs.com/zyly/p/9259392.html" target="_blank" rel="noopener">https://www.cnblogs.com/zyly/p/9259392.html</a></p><p>​                             <a href="https://ranmaosong.github.io/2019/08/10/cv-selective-search/" target="_blank" rel="noopener">https://ranmaosong.github.io/2019/08/10/cv-selective-search/</a></p><p>​                            <a href="https://www.jianshu.com/p/dbf58d84a2aa" target="_blank" rel="noopener">https://www.jianshu.com/p/dbf58d84a2aa</a></p><p>R-CNN: <a href="https://zhuanlan.zhihu.com/p/23006190" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23006190</a></p><p>SPPNet: <a href="https://zhuanlan.zhihu.com/p/27485018" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27485018</a></p><p>Faster R-CNN（详细）: <a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a> </p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之卷积神经网络</title>
      <link href="/2020/01/06/shen-du-xue-xi-zhi-juan-ji-shen-jing-wang-luo/"/>
      <url>/2020/01/06/shen-du-xue-xi-zhi-juan-ji-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<h1 id="一、卷积神经网络概述"><a href="#一、卷积神经网络概述" class="headerlink" title="一、卷积神经网络概述"></a>一、卷积神经网络概述</h1><h2 id="1、CNN简介"><a href="#1、CNN简介" class="headerlink" title="1、CNN简介"></a>1、CNN简介</h2><p>CNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，<br>CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。 </p><h2 id="2、多层神经网络的问题"><a href="#2、多层神经网络的问题" class="headerlink" title="2、多层神经网络的问题"></a>2、多层神经网络的问题</h2><ul><li>参数较多，多层神经网络采用全连接的方式，稍微大点的图片计算复杂程度就会很大。</li><li>丢失空间信息，多层神经网络将图片像素矩阵展平为向量时丢失了图片中包含的二维信息。这种像素之间的位置信息可以帮助我们发现像素中规律。</li></ul><p>CNN通过稀疏互联的层级来解决这个问题，这种局部连接层包含更少的权重并且在空间内共享。</p><h2 id="3、CNN整体架构"><a href="#3、CNN整体架构" class="headerlink" title="3、CNN整体架构"></a>3、CNN整体架构</h2><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/01.png" alt></p><p>CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。 然后跟着全连接神经网络”Affine-ReLU”组合 , 最后还是”Affine-Softmax”层输出最终结果（概率）。Affine为全连接层用来对卷积层提取的特征进行分类。</p><h1 id="二、卷积层"><a href="#二、卷积层" class="headerlink" title="二、卷积层"></a>二、卷积层</h1><h2 id="1、输入输出"><a href="#1、输入输出" class="headerlink" title="1、输入输出"></a>1、输入输出</h2><p>卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。 </p><h2 id="2、卷积运算"><a href="#2、卷积运算" class="headerlink" title="2、卷积运算"></a>2、卷积运算</h2><p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。 卷积运算对输入数据应用滤波器（卷积核）。</p><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02.png" alt></p><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/03.png" alt></p><p>​    对于输入数据，卷积运算以一定间隔滑动<strong>滤波器</strong>，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。</p><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/04.png" alt></p><p>​    在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。包含偏置的卷积运算的处理流如上图。</p><h2 id="3、填充"><a href="#3、填充" class="headerlink" title="3、填充"></a>3、填充</h2><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/05.png" alt></p><p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。 </p><p>使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。 </p><h2 id="4、步幅"><a href="#4、步幅" class="headerlink" title="4、步幅"></a>4、步幅</h2><p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。 </p><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/06.png" alt></p><p>在上图中，对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。像这样，步幅可以指定应用滤波器的间隔 。</p><p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过公式进行计算： </p><p>$$<br>OH = \frac{H + 2P - FH}{S} + 1<br>$$</p><p>$$<br>OW = \frac{W+ 2P - FW}{S} + 1<br>$$</p><p>当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。 </p><h2 id="5、3维数据的卷积运算"><a href="#5、3维数据的卷积运算" class="headerlink" title="5、3维数据的卷积运算"></a>5、3维数据的卷积运算</h2><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/07.png" alt></p><p>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 </p><p>需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。 </p><h2 id="6、批处理"><a href="#6、批处理" class="headerlink" title="6、批处理"></a>6、批处理</h2><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。 </p><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/08.png" alt></p><p>这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。 </p><h1 id="三、池化层"><a href="#三、池化层" class="headerlink" title="三、池化层"></a>三、池化层</h1><p>池化层的主要作用是降低特征图的维度，避免过拟合。CNN中主要有两种池化层，最大池化层和全局平均池化层。</p><h2 id="1、最大池化层"><a href="#1、最大池化层" class="headerlink" title="1、最大池化层"></a>1、最大池化层</h2><p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/09.png" alt></p><p>“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。</p><p>除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。 </p><h2 id="2、池化层的特征"><a href="#2、池化层的特征" class="headerlink" title="2、池化层的特征"></a>2、池化层的特征</h2><ol><li><p>没有要学习的参数</p><p>池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。 </p></li><li><p>通道数不发生变化</p><p>经过池化运算，输入数据和输出数据的通道数不会发生变化。所示，计算是按通道独立进行的。 </p></li><li><p>对微小的位置变化具有鲁棒性（健壮）</p><p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。 </p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之评估指标</title>
      <link href="/2020/01/04/ji-qi-xue-xi-zhi-ping-gu-zhi-biao/"/>
      <url>/2020/01/04/ji-qi-xue-xi-zhi-ping-gu-zhi-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="一、分类模型"><a href="#一、分类模型" class="headerlink" title="一、分类模型"></a>一、分类模型</h1><h2 id="1、混淆矩阵"><a href="#1、混淆矩阵" class="headerlink" title="1、混淆矩阵"></a>1、混淆矩阵</h2><p>在机器学习领域，混淆矩阵(confusion matrix), 又称为可能性表格或者错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）。其每一列代表预测值，每一行代表的是实际的类别。</p><p><img src="%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png" alt></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> skleran<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> confusion_matrixconfmat <span class="token operator">=</span> confusion_matrix<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y_test<span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>confmat<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2、预测误差和准确率"><a href="#2、预测误差和准确率" class="headerlink" title="2、预测误差和准确率"></a>2、预测误差和准确率</h2><p>预测误差（error，ERR）和准确率（accuracy，ACC）都提供了误分类样本数量的相关信息。误差可以理解为预测错误样本的数量与所有被预测样本数量的比值，而准确率计算方法则是正确预测样本的数量与所有被预测样本数量的比值:<br>$$<br>ERR = \frac {FP + FN}{FP + FN + TP + TN}<br>$$<br>预测准确率也可以通过误差直接计算：<br>$$<br>ACC = \frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_scoreaccuracy_score<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>对于类别数量不均衡的分类问题来说，真正率（TPR）与假正率（FPR）是非常有用的性能指标：<br>$$<br>FPR = \frac{FP}{N} = \frac{FP}{FP + TN}<br>$$</p><p>$$<br>TPR = \frac{TP}{P} = \frac{TP}{TP + FN}<br>$$</p><h2 id="3、精确率和召回率"><a href="#3、精确率和召回率" class="headerlink" title="3、精确率和召回率"></a>3、精确率和召回率</h2><p><strong>精确率</strong>是针对我们<strong>预测结果</strong>而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是:<br>$$<br>P = \frac{TP}{TP + FP}<br>$$</p><p>而<strong>召回率</strong>是针对我们原来的<strong>样本</strong>而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。<br>$$<br>R = \frac {TP}{TP + FN}<br>$$</p><p>在信息检索领域，精确率和召回率又被称为<strong>查准率</strong>和<strong>查全率</strong>，</p><p>查准率＝检索出的相关信息量 / 检索出的信息总量<br>查全率＝检索出的相关信息量 / 系统中的相关信息总量  </p><h2 id="4、敏感性和特异性"><a href="#4、敏感性和特异性" class="headerlink" title="4、敏感性和特异性"></a>4、敏感性和特异性</h2><p>医疗领域的混淆矩阵:</p><p><img src="02.jpg" alt></p><p>敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记</p><ul><li>TP：（真阳性）被<strong>正确</strong>诊断为患病的病人。</li><li>TN：（真阴性）被<strong>正确</strong>诊断为健康的健康人。</li><li>FP：（假阳性）被<strong>错误</strong>诊断为患病的健康人。</li><li>FN：（假阴性）被<strong>错误</strong>诊断为健康的病人。</li></ul><p>那么：<br>$$<br>敏感性 =\frac{TP}{TP+FN}<br>$$</p><p>$$<br>特异性 = \frac{TN}{TN+FP}<br>$$</p><p><strong>敏感性</strong>和<strong>特异性</strong>虽然与<strong>查准率</strong>和<strong>查全率</strong>相似，但并不相同。其定义如下：</p><p>在癌症示例中，敏感性和特异性指：</p><ul><li>敏感性：在<strong>患有</strong>癌症的所有人中，诊断正确的人有多少？</li><li>特异性：在<strong>未患</strong>癌症的所有人中，诊断正确的人有多少？</li></ul><p>查准率和查全率的定义如下：</p><ul><li><p>查全率：在<strong>患有癌症</strong>的所有人中，多少人<strong>被诊断</strong>患有癌症？</p></li><li><p>查准率：在<strong>被诊断</strong>患有癌症的所有人中，多少人确实<strong>得了癌症</strong>？</p></li></ul><p>从这里可以看出，敏感性就是查全率，但特异性并不是查准率。</p><h2 id="5、-F-1-和-F-beta"><a href="#5、-F-1-和-F-beta" class="headerlink" title="5、$F_1$和$F_\beta$"></a>5、$F_1$和$F_\beta$</h2><p>把精确率和召回率合成一个值，根据精确率和召回率权重的不同可以分为$F_1$和$F_\beta$</p><p>算术平均数  Arithmetic Mean = (x+y)/2<br>调和平均数  Harmonic Mean = (2xy)/(x+y)<br>调和平均数始终小于算术平均数，因为调和平均数更接近较小的数</p><p>$F_1$值的计算公式如下:<br>$$<br>F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = \frac{2<em>precision</em>recall}{presicion + recall}<br>$$<br>$F_1$值就是精确率和召回率的调和平均值，$F_1$值认为精确率和召回率一样重要。</p><p>$F_\beta$值的计算公式如下:<br>$$<br>F_1 = \frac{1 + \beta^2}{\frac{1}{precision} + \frac{\beta^2}{recall}} = \frac{(1+\beta^2)<em>precision</em>recall}{\beta^2*presicion + recall}<br>$$<br>β 的界限在 0 和 ∞ 之间。如果β=0, 则得出精度。如果β=∞, 则得出召回率<br>在β=1时，$F_β$就是$F_1$值，此时$F_β$认为精确率和召回率一样重要；当β&gt;1时，$F_1$认为召回率更重要；当0&lt;β&lt;1时，认为精确$F_β$率更重要。除了$F_1$值之外，常用的还有$F_2$和$F_{0.5}$。</p><h2 id="6、ROC曲线及AUC值"><a href="#6、ROC曲线及AUC值" class="headerlink" title="6、ROC曲线及AUC值"></a>6、ROC曲线及AUC值</h2><p>AUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图所示：</p><p><img src="02.png" alt></p><p>　ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive}$，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $recall_{negative}$)如下所示：<br>$$<br>TPR = \frac{TP}{P} = \frac{TP}{TP + FN} = recall_{positive}<br>$$</p><p>$$<br>FPR = \frac{FP}{FP + TN} = \frac{FP+TN-TN}{FP+TN} = 1 - \frac{TN}{FP + TN} = 1 - recall_{negative}<br>$$</p><p>通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θ取值范围为[0,1]。</p><p>　　ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive}$值，横坐标FPR就是(1 - $recall_{negative}$)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。</p><p>　　图展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。</p><p>ROC曲线上几个关键点的解释：<br>        ( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型<br>        ( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型<br>        ( TPR=1,FPR=0 ) 理想模型</p><h1 id="二、回归模型"><a href="#二、回归模型" class="headerlink" title="二、回归模型"></a>二、回归模型</h1><h2 id="1、平均绝对误差-Mean-Absolute-Error"><a href="#1、平均绝对误差-Mean-Absolute-Error" class="headerlink" title="1、平均绝对误差(Mean Absolute Error)"></a>1、平均绝对误差(Mean Absolute Error)</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRressionclassifier <span class="token operator">=</span> LinearRression<span class="token punctuation">(</span><span class="token punctuation">)</span>classifier<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>guesses <span class="token operator">=</span> classifier<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>error <span class="token operator">=</span> mean_absolute_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> guesses<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>MAE问题：绝对值函数是不可微分的，不利于使用梯度下降等方法<br>解决：均方误差</p><h2 id="2、平均平方误差-Mean-Squared-Error"><a href="#2、平均平方误差-Mean-Squared-Error" class="headerlink" title="2、平均平方误差(Mean Squared Error)"></a>2、平均平方误差(Mean Squared Error)</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_square_error<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRressionclassifier <span class="token operator">=</span> LinearRression<span class="token punctuation">(</span><span class="token punctuation">)</span>classifier<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>guesses <span class="token operator">=</span> classifier<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>error <span class="token operator">=</span> mean_square_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> guesses<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、决定系数-R-2"><a href="#3、决定系数-R-2" class="headerlink" title="3、决定系数($R^2$ )"></a>3、决定系数($R^2$ )</h2><p>$R^2$分数通过将我们的模型与最简单的可能模型相比得出。那么思考一下，拟合一堆点的最简单的可能模型是什么呢？那就是取所有值的平均值,然后我们可以计算出这个模型的均方误差。<br>    R2 = 1 - (线性回归模型误差/简单模型误差)<br>    如果这个模型不太好 这两个误差将很接近而这个量应接近 1，那么整个 R2 分数应接近 0。<br>    如果模型较好 那么线性回归模型对的均方误差应比简单模型的均方误差小很多，那么这个比例就很小。而 R2 分数将非常接近 1，总结来说 如果 R2 分数接近 1 模型就不错。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> r2_scorer2_score<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="三、参考"><a href="#三、参考" class="headerlink" title="三、参考"></a>三、参考</h1><p><a href="https://www.zhihu.com/question/19645541/answer/91694636" target="_blank" rel="noopener">https://www.zhihu.com/question/19645541/answer/91694636</a></p><p><a href="https://www.zhihu.com/question/30750849" target="_blank" rel="noopener">https://www.zhihu.com/question/30750849</a></p><p><a href="https://www.cnblogs.com/wuliytTaotao/p/9285227.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuliytTaotao/p/9285227.html</a></p><p><a href="https://www.cnblogs.com/gatherstars/p/6084696.html" target="_blank" rel="noopener">https://www.cnblogs.com/gatherstars/p/6084696.html</a></p><p><a href="https://blog.csdn.net/weixin_41043240/article/details/80265577" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41043240/article/details/80265577</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C和C++语法总结</title>
      <link href="/2020/01/03/c-he-c-yu-fa-zong-jie/"/>
      <url>/2020/01/03/c-he-c-yu-fa-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="一、C语言篇"><a href="#一、C语言篇" class="headerlink" title="一、C语言篇"></a>一、C语言篇</h1><h2 id="1、gcc-g"><a href="#1、gcc-g" class="headerlink" title="1、gcc/g++"></a>1、gcc/g++</h2><p><strong>1.1、为什么需要gcc/g++</strong></p><p>编辑器（如vi、记事本）是指我用它来写程序的（编辑代码），而我们写的代码语句，电脑是不懂的，我们需要把它转成电脑能懂的语句，编译器就是这样的转化工具。就是说，<strong>我们用编辑器编写程序，由编译器编译后才可以运行！</strong></p><p><strong>1.2、什么是gcc/g++</strong></p><p>GCC: <strong>GNU编译器套装</strong>（英语：<strong>GNU Compiler Collection</strong>，缩写为<strong>GCC</strong>）,它可以编译C、C++、JAV、Fortran、Pascal、Object-C、Ada等语言。</p><p>gcc: 是GCC中的GUN C Compiler（C编译器）</p><p>g++: 是GCC中的GUN C++ Compiler （C++编译器）</p><p>一个有趣的事实就是，就本质而言，gcc和g++并不是编译器，也不是编译器的集合，它们只是一种驱动器，根据参数中要编译的文件的类型，调用对应的GUN编译器而已，比如，用gcc编译一个c文件的话，会有以下几个步骤：</p><p>Step1：Call a preprocessor, like cpp.</p><p>Step2：Call an actual compiler, like cc or cc1.</p><p>Step3：Call an assembler, like as.</p><p>Step4：Call a linker, like ld</p><p>由于编译器是可以更换的，所以gcc不仅仅可以编译C文件</p><p>所以，更准确的说法是：gcc调用了C compiler，而g++调用了C++ compiler</p><p><strong>1.3、编译命令</strong></p><p>编译命令格式：</p><p>gcc [-option1] … <filename></filename></p><p>g++ [-option1] … <filename></filename></p><ul><li><p>命令、选项和源文件之间使用空格分隔</p></li><li><p>一行命令中可以有零个、一个或多个选项</p></li><li><p>文件名可以包含文件的绝对路径，也可以使用相对路径</p></li><li><p>如果命令中不包含输出可执行文件的文件名，可执行文件的文件名会自动生成一个默认名，Linux平台为<strong>a.out</strong>，Windows平台为<strong>a.exe</strong></p></li></ul><p>gcc、g++编译常用选项说明：</p><table><thead><tr><th><strong>选项</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>-o file</td><td>指定生成的输出文件名为file</td></tr><tr><td>-E(大写)</td><td>只进行预处理</td></tr><tr><td>-S(大写)</td><td>只进行预处理和编译</td></tr><tr><td>-c(小写)</td><td>只进行预处理、编译和汇编</td></tr></tbody></table><p><strong>主要参数</strong></p><pre class="line-numbers language-c++"><code class="language-c++">-g - turn on debugging (so GDB gives morefriendly output)-Wall - turns on most warnings-O or -O2 - turn on optimizations-o - name of the output file-c - output an object file (.o)-I - specify an includedirectory-L - specify a libdirectory-l - link with librarylib.a使用示例：g++ -ohelloworld -I/homes/me/randomplace/include helloworld.C<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>1.4、gcc/g++的区别</strong></p><ol><li><p>对于 *.c和*.cpp文件，gcc分别当做c和cpp文件编译（c和cpp的语法强度是不一样的）</p></li><li><p>对于 *.c和*.cpp文件，g++则统一当做cpp文件编译</p></li><li><p>使用g++编译文件时，<strong>g++会自动链接标准库STL，而gcc不会自动链接STL</strong></p></li><li><p>gcc在编译C文件时，可使用的预定义宏是比较少的</p></li><li><p>gcc在编译cpp文件时/g++在编译c文件和cpp文件时（这时候gcc和g++调用的都是cpp文件的编译器），会加入一些额外的宏，这些宏如下：</p></li></ol><pre class="line-numbers language-c++"><code class="language-c++">#define __GXX_WEAK__ 1#define __cplusplus 1#define __DEPRECATED 1#define __GNUG__ 4#define __EXCEPTIONS 1#define __private_extern__ extern<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="6"><li>在用gcc编译c++文件时，为了能够使用STL，需要加参数 –lstdc++ ，但这并不代表 gcc –lstdc++ 和 g++等价，它们的区别不仅仅是这个</li></ol><h2 id="2、C-C-编译过程"><a href="#2、C-C-编译过程" class="headerlink" title="2、C/C++编译过程"></a>2、C/C++编译过程</h2><p><strong>2.1、编译步骤</strong></p><p>C代码编译成可执行程序经过4步：</p><p>1）预处理：宏定义展开、头文件展开、条件编译等，同时将代码中的注释删除，这里并不会检查语法</p><p>2)   编译：检查语法，将预处理后文件编译生成汇编文件</p><p>3）汇编：将汇编文件生成目标文件（二进制文件）</p><p>4）链接：C语言写的程序是需要依赖各种库的，所以编译之后还需要把库链接到最终的可执行程序中去</p><p><img src="07.png" alt="img"></p><p><strong>2.2、gcc编译过程</strong></p><p>1) 分步编译</p><p>预处理：<code>gcc -E hello.c -o hello.i</code></p><p>编  译：<code>gcc -S hello.i -o hello.s</code></p><p>汇  编：<code>gcc -c hello.s -o hello.o</code></p><p>链  接：<code>gcc   hello.o -o hello_elf</code></p><table><thead><tr><th><strong>选项</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>-E(大写)</td><td>只进行预处理</td></tr><tr><td>-S(大写)</td><td>只进行预处理和编译</td></tr><tr><td>-c(小写)</td><td>只进行预处理、编译和汇编</td></tr><tr><td>-o file</td><td>指定生成的输出文件名为 file</td></tr></tbody></table><table><thead><tr><th><strong><em>\</em>文件后缀**</strong></th><th><strong><em>\</em>含义**</strong></th></tr></thead><tbody><tr><td>.c</td><td>C 语言文件</td></tr><tr><td>.i</td><td>预处理后的 C 语言文件</td></tr><tr><td>.s</td><td>编译后的汇编文件</td></tr><tr><td>.o</td><td>编译后的目标文件</td></tr></tbody></table><p>2) 一步编译</p><p><code>gcc hello.c -o demo</code></p><p><strong>2.3、查找程序所依赖的动态库</strong></p><p>１）Linux平台下，<code>ldd</code>  可执行程序</p><p>２）Windows平台下，需要相应软件(<code>Depends.exe</code>)</p><h2 id="3、CPU内部结构与寄存器"><a href="#3、CPU内部结构与寄存器" class="headerlink" title="3、CPU内部结构与寄存器"></a>3、CPU内部结构与寄存器</h2><p><strong>3.1、CPU总线</strong></p><p><strong>数据总线</strong></p><p>（1） 是CPU与内存或其他器件之间的数据传送的通道。</p><p>（2）数据总线的宽度决定了CPU和外界的数据传送速度。</p><p>（3）每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据（即一个字节）。</p><p>（4）数据总线是数据线数量之和。</p><p>数据总线是CPU与存储器、CPU与I/O接口设备之间传送数据信息（各种指令数据信息）的总线，这些信号通过数据总线往返于CPU与存储器、CPU与I/O接口设备之间，因+此，数据总线上的信息是双向传输的。</p><p><strong>地址总线</strong></p><p>（1）CPU是通过地址总线来指定存储单元的。</p><p>（2）地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据（1024个内存单元）(1B)</p><p>（3）地址总线是地址线数量之和。</p><p>地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。它是单向的，只能从CPU传向外部存储器或I/O端口</p><p>有个说法：64位系统装了64位操作系统，最大物理内存理论上=2的64次方；然而实际上地址总线只用到了35位，所以最大物理内存是32G大小</p><p><strong>控制总线</strong></p><p>（1）CPU通过控制总线对外部器件进行控制。</p><p>（2）控制总线的宽度决定了CPU对外部器件的控制能力。</p><p>（3）控制总线是控制线数量之和。</p><p>控制总线，英文名称：ControlBus，简称：CB。控制总线主要用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和输入输出设备接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的</p><p><strong>3.2、64位和32位系统区别</strong></p><ul><li><p>寄存器是CPU内部最基本的存储单元</p></li><li><p>CPU的主要组成包括了运算器和控制器。运算器是由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。</p></li><li><p>CPU位数=CPU中寄存器的位数=CPU能够一次并行处理的数据宽度（位数）=数据总线宽度</p></li><li><p><strong>CPU的位宽(位数)一般是以 min{ALU位宽、通用寄存器位宽、数据总线位宽}决定的</strong></p></li><li><p>CPU对外是通过总线（地址、控制、数据）来和外部设备交互的，总线的宽度是8位，同时CPU的寄存器也是8位，那么这个CPU就叫8位CPU</p></li><li><p>如果总线是32位，寄存器也是32位的，那么这个CPU就是32位CPU</p></li><li><p>有一种CPU内部的寄存器是32位的，但总线是16位，准32位CPU</p></li><li><p>所有的64位CPU兼容32位的指令，32位要兼容16位的指令，所以在64位的CPU上是可以识别32位的指令</p></li><li><p>在64位的CPU构架上运行了64位的软件操作系统，那么这个系统是64位</p></li><li><p>在64位的CPU构架上，运行了32位的软件操作系统，那么这个系统就是32位</p></li><li><p>64位的软件不能运行在32位的CPU之上</p></li></ul><p><strong>3.3、寄存器、缓存、内存三者关系</strong></p><ol><li><p>寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。</p></li><li><p>内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。</p></li><li><p>寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。</p></li><li><p>Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache（L1 Cache）和二级Cache（L2 Cache），L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。</p></li></ol><p>总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。</p><h2 id="4、内存、地址和指针"><a href="#4、内存、地址和指针" class="headerlink" title="4、内存、地址和指针"></a>4、内存、地址和指针</h2><p><strong>4.1、内存</strong></p><p>内存含义：</p><ul><li><p>存储器：计算机的组成中，用来存储程序和数据，辅助CPU进行运算处理的重要部分。</p></li><li><p>内存：内部存贮器，暂存程序/数据——掉电丢失 SRAM、DRAM、DDR、DDR2、DDR3。</p></li><li><p>外存：外部存储器，长时间保存程序/数据—掉电不丢ROM、ERRROM、FLASH（NAND、NOR）、硬盘、光盘。</p></li></ul><p>内存是沟通CPU与硬盘的桥梁：</p><ul><li><p>暂存放CPU中的运算数据</p></li><li><p>暂存与硬盘等外部存储器交换的数据</p></li></ul><p><strong>4.2、物理存储器和存储地址空间</strong></p><p>有关内存的两个概念：物理存储器和存储地址空间。</p><p>物理存储器：实际存在的具体存储器芯片。</p><ul><li><p>主板上装插的内存条</p></li><li><p>显示卡上的显示RAM芯片</p></li><li><p>各种适配卡上的RAM芯片和ROM芯片</p></li></ul><p>存储地址空间：对存储器编码的范围。我们在软件上常说的内存是指这一层含义。</p><ul><li><p>编码：对每个物理存储单元（一个字节）分配一个号码</p></li><li><p>寻址：可以根据分配的号码找到相应的存储单元，完成数据的读写</p></li></ul><p><strong>4.3、内存地址</strong></p><ul><li><p>将内存抽象成一个很大的一维字符数组。</p></li><li><p>编码就是对内存的每一个字节分配一个32位或64位的编号（与32位或者64位处理器相关）。</p></li><li><p>这个内存编号我们称之为内存地址。</p></li></ul><p>内存中的每一个数据都会分配相应的地址：</p><ul><li><p>char:占一个字节分配一个地址</p></li><li><p>int: 占四个字节分配四个地址</p></li><li><p>float、struct、函数、数组等</p></li></ul><p><strong>4.4、指针和指针变量</strong></p><ul><li><p>内存区的每一个字节都有一个编号，这就是“地址”。</p></li><li><p>如果在程序中定义了一个变量，在对程序进行编译或运行时，系统就会给这个变量分配内存单元，并确定它的内存地址(编号)</p></li><li><p>指针的实质就是内存“地址”。指针就是地址，地址就是指针。</p></li><li><p>指针是内存单元的编号，指针变量是存放地址的变量。</p></li><li><p><strong>通常我们叙述时会把指针变量简称为指针，实际他们含义并不一样</strong></p></li></ul><h2 id="5、存储类型"><a href="#5、存储类型" class="headerlink" title="5、存储类型"></a>5、存储类型</h2><p><strong>5.1、auto</strong></p><ol><li>普通局部变量，自动存储，该对象会自动创建和销毁，调用函数时分配内存，函数结束时释放内存。只在{}内有效，存放在堆栈中一般省略auto,  不会被默认初始化，初值不随机</li><li>全局变量，<strong>不允许声明为auto变量</strong>， register不适用于全局变量，生命周期由定义到程序运行结束，没有初始化会<strong>自动赋值0或空字符</strong>。全局变量属于整个程序，不同文件中不能有同名的全局变量，通过<strong>extern</strong>在其他文件中引用使用</li></ol><p><strong>5.2、static</strong></p><ol><li>静态局部变量，生命周期由定义到程序运行结束，在编译时赋初值，<strong>只初始化一次，没有初始化会自动赋值0或空字符</strong>。只在当前{}内有效</li><li>静态全局变量，生命周期由定义到程序运行结束，在编译时赋初值，只初始化一次，没有初始化会自动赋值0或空字符。从定义到文件结尾起作用，在一个程序中的其他文件中可以定义同名的静态全局变量，因为作用于不冲突。</li></ol><p><strong>5.3、extern</strong></p><ol><li><p>外部变量声明，是指这是一个已在别的地方定义过的对象，这里只是对变量的一次重复引用，不会产生新的变量。</p></li><li><p>使用extern时，注意不能重复定义，否则编译报错</p><pre class="line-numbers language-c"><code class="language-c"><span class="token comment" spellcheck="true">//    程序文件一：</span>    <span class="token keyword">extern</span> <span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//编译警告，extern的变量最好不要初始化</span><span class="token comment" spellcheck="true">//    程序文件二：</span>    <span class="token keyword">extern</span> <span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//重复定义，应改为extern int a;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>如果我们希望该外部变量只能在本文件内使用，而不能被其他文件引用可以在外部变量定义时加static声明。防止别人写的模块误用。</p></li><li><p>在函数外部定义的全局变量，作用域开始于变量定义，结束于程序文件的结束。我们可以extern来声明外部变量来扩展它的作用域。同一个文件内，extern声明之后就可以作用域扩大到声明处到文件结束。比如在一个函数之后定义外部变量a，之后的函数可以使用该变量，但是之前的函数不能使用，加extern可以解决。</p><pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token keyword">extern</span> <span class="token keyword">int</span> g1<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">extern</span> <span class="token keyword">int</span> g2<span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d,%d\n"</span><span class="token punctuation">,</span> g1<span class="token punctuation">,</span>g2<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> g1 <span class="token operator">=</span> <span class="token number">77</span><span class="token punctuation">;</span><span class="token keyword">int</span> g2 <span class="token operator">=</span> <span class="token number">88</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><ol start="5"><li>多个文件时，可以在未定义该外部变量的文件内做extern声明即可以使用。但是需要注意可能执行一个文件时改变了该全局变量的值，影响其他文件的调用。编译时遇到extern，会先在文件内找是否定义了该外部变量。如果未找到则在链接时在其他文件中找。</li></ol><p><strong>5.4、register</strong></p><ol><li><p>寄存器变量，请求编译器将这个变量保存在CPU的寄存器中，从而加快程序的运行.只是建议CPU这样做，非强制,声明变量为register,编译器并不一定会将它处理为寄存器变量</p></li><li><p>动态和静态变量都是存放在内存中，程序中遇到该值时用控制器发指令将变量的值送到运算器中，需要存数再保存到内存中。如果频繁使用一个变量，比如一个函数体内的多次循环每次都引用该局部变量，我们则可以把局部变量的值放到CPU的寄存器中，叫寄存器变量。不需要多次到内存中存取提高效率。</p></li><li><p>但是只能局部自动变量和形参可以做寄存器变量。在函数调用时占用一些寄存器，函数结束时释放。不同系统对register要求也不一样，比如对定义register变量个数，数据类型等限制，有的默认为自动变量处理。所以在程序一般也不用。</p></li><li><p>register是不能取址的。比如 <code>int i</code>；(自动为auto)<code>int *p=&amp;i;</code>是对的， 但<code>register int j; int *p = &amp;j;</code>是错的，因为无法对寄存器的定址。</p><pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;time.h></span></span><span class="token macro property">#<span class="token directive keyword">define</span> TIME 1000000000</span><span class="token keyword">int</span> m<span class="token punctuation">,</span> n <span class="token operator">=</span> TIME<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">/* 全局变量 */</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    time_t start<span class="token punctuation">,</span> stop<span class="token punctuation">;</span>    <span class="token keyword">register</span> <span class="token keyword">int</span> a<span class="token punctuation">,</span> b <span class="token operator">=</span> TIME<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">/* 寄存器变量 */</span>    <span class="token keyword">int</span> x<span class="token punctuation">,</span> y <span class="token operator">=</span> TIME<span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">/* 一般变量   */</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>a <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> a <span class="token operator">&lt;</span> b<span class="token punctuation">;</span> a<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"寄存器变量用时: %d 秒\n"</span><span class="token punctuation">,</span> stop <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>x <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> x <span class="token operator">&lt;</span> y<span class="token punctuation">;</span> x<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"一般变量用时: %d 秒\n"</span><span class="token punctuation">,</span> stop <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>m <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> m <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> m<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"全局变量用时: %d 秒\n"</span><span class="token punctuation">,</span> stop <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>5.5、volatile</strong></p><pre class="line-numbers language-c"><code class="language-c">    程序在使用变量时<span class="token punctuation">,</span> 特别是连续多次使用变量时<span class="token punctuation">,</span> 一般是载入寄存器<span class="token punctuation">,</span> 直接从寄存器存取<span class="token punctuation">,</span> 之后再还回内存<span class="token punctuation">;</span>但如果此变量在返回内存时<span class="token punctuation">,</span> 假如内存中的值已经改变了<span class="token punctuation">(</span>从外部修改了<span class="token punctuation">)</span>怎么办<span class="token operator">?</span>为了避免这种情况的发生<span class="token punctuation">,</span> 可以用 <span class="token keyword">volatile</span> 说明此变量<span class="token punctuation">,</span> 以保证变量的每次使用都是直接从内存存取<span class="token punctuation">.</span>但这样肯定会影响效率<span class="token punctuation">,</span> 幸好它并不常用<span class="token punctuation">.</span>另外<span class="token punctuation">:</span> 如果 <span class="token keyword">const</span> <span class="token keyword">volatile</span> 同时使用<span class="token punctuation">,</span> 这表示此变量只接受外部的修改<span class="token punctuation">.</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token keyword">volatile</span> <span class="token keyword">int</span> num <span class="token operator">=</span> <span class="token number">123</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">getchar</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>5.6、总结</strong></p><table><thead><tr><th>关键字</th><th>类型</th><th>生命周期</th><th>作用域</th><th>修饰对象</th><th>所属区</th></tr></thead><tbody><tr><td>auto[可省略]</td><td>普通局部变量</td><td>定义到{}运行结束</td><td>｛｝</td><td>变量</td><td>栈区</td></tr><tr><td>static</td><td>静态局部变量</td><td>定义到程序运行结束</td><td>｛｝</td><td>变量和函数</td><td>初始化在data段，未初始化在BSS段</td></tr><tr><td></td><td>全局变量</td><td>定义到程序运行结束</td><td>定义到文件结尾</td><td></td><td>初始化在data段，未初始化在BSS段</td></tr><tr><td>extern</td><td>全局变量</td><td>定义到程序运行结束</td><td>声明处到文件结尾</td><td>变量和函数</td><td>初始化在data段，未初始化在BSS段</td></tr><tr><td>static</td><td>全局变量</td><td>整个程序运行期</td><td>声明处到文件结尾</td><td>变量和函数</td><td>初始化在data段，未初始化在BSS段</td></tr><tr><td>register</td><td>寄存器变量</td><td>定义到{}运行结束</td><td>｛｝</td><td>变量</td><td>运行时存储在CPU寄存器</td></tr><tr><td>extern</td><td>函数</td><td>整个程序运行期</td><td>声明处到文件结尾</td><td></td><td>代码区</td></tr><tr><td>static</td><td>函数</td><td>整个程序运行期</td><td>声明处到文件结尾</td><td></td><td>代码区</td></tr></tbody></table><h2 id="6、内存分区"><a href="#6、内存分区" class="headerlink" title="6、内存分区"></a>6、内存分区</h2><p>C代码经过预处理、编译、汇编、链接4步后生成一个可执行程序。在 Linux 下，程序是一个普通的可执行文件，以下列出一个二进制可执行文件的基本情况：</p><p><img src="01.png" alt></p><p>通过上图可以得知，在没有运行程序前，也就是说程序没有加载到内存前，可执行程序内部已经分好3段信息，分别为代码区（text）、数据区（data）和未初始化数据区（bss）3 个部分（有些人直接把data和bss合起来叫做静态区或全局区）。</p><ul><li><p><strong>代码区</strong></p><p>存放 CPU 执行的机器指令。通常代码区是<strong>可共享</strong>的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区通常是<strong>只读的</strong>，使其只读的原因是防止程序意外地修改了它的指令。另外，代码区还规划了局部变量的相关信息。</p></li><li><p><strong>全局初始化数据区/静态数据区（data段）</strong></p><p>该区包含了在程序中明确被初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。</p></li><li><p><strong>未初始化数据区（又叫 bss 区）</strong></p><p>存入的是全局未初始化变量和未初始化静态变量。未初始化数据区的数据在程序开始执行之前被内核初始化为 0 或者空（NULL）。</p></li></ul><p>程序在加载到内存前，代码区和全局区(data和bss)的大小就是固定的，程序运行期间不能改变。然后，运行可执行程序，系统把程序加载到内存，除了根据可执行程序的信息分出代码区（text）、数据区（data）和未初始化数据区（bss）之外，还额外增加了栈区、堆区。</p><p><img src="02.png" alt></p><ul><li><p>代码区（text segment）</p><p>加载的是可执行文件代码段，所有的可执行代码都加载到代码区，这块内存是不可以在运行期间修改的。</p></li><li><p>只读数据区（文字常量区 RO data）</p><p>只读数区是程序使用的一些不会被更改的数据。一般是const修饰的变量以及程序中使用的文字常量。</p></li><li><p>已初始化数据区 （RW data）</p><p>加载的是可执行文件数据段，存储于数据段（全局初始化，静态初始化数据）的数据的生存周期为整个程序运行过程。</p></li><li><p>未初始化数据区（BSS）</p><p>加载的是可执行文件BSS段，位置可以分开亦可以紧靠数据段，存储于数据段的数据（全局未初始化，静态未初始化数据）的生存周期为整个程序运行过程。</p></li><li><p>堆区（heap）</p><p>堆是一个大容器，它的容量要远远大于栈，但没有栈那样先进后出的顺序。用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收。</p></li><li><p>栈区（stack）</p><p>栈是一种先进后出的内存结构，由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。</p></li></ul><h2 id="7、结构体字节对齐"><a href="#7、结构体字节对齐" class="headerlink" title="7、结构体字节对齐"></a>7、结构体字节对齐</h2><p><strong>7.1、内存对齐原因</strong></p><pre class="line-numbers language-c"><code class="language-c"><span class="token keyword">struct</span> data<span class="token punctuation">{</span>    <span class="token keyword">char</span> c<span class="token punctuation">;</span>    <span class="token keyword">int</span> i<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 5还是 8?</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在Linux 32位架构下，假设变量stu存放在内存中的起始地址为0x00，那么c的起始地址为0x00、i的起始地址为0x01，变量stu共占用了5个字节：</p><ul><li><p>对变量c访问：CPU只需要一个读周期</p></li><li><p>变量i访问：</p></li></ul><ol><li><p>首先CPU用一个读周期，从0x00处读取了4个字节(32位架构)，然后将0x01-0x03的3个字节暂存。</p></li><li><p>再花一个读周期读取了从0x04-0x07的4字节数据，将0x04这个字节与刚刚暂存的3个字节进行拼接从而读取到成员变量i的值。</p></li><li><p>读取一个成员变量i，CPU却花费了2个读周期。</p></li></ol><p>如果数据成员i的起始地址被放在了0x04处</p><ul><li><p>读取c成员，花费周期为1  </p></li><li><p>读取i所花费的周期也变成了1</p></li><li><p>引入字节对齐可以避免读取效率的下降，同时也浪费了3个字节的空间(0x01-0x03)</p><p><strong>结构体内部成员对齐是为了实现用空间换取时间。</strong></p></li></ul><p><strong>7.2、内存对齐原则</strong></p><ul><li><p>原则1：数据成员的对齐规则</p><p>1) 最大对齐单位以CPU架构对齐，如Linux 32位最大以4字节对齐，Linux 64位最大以8字节对齐，vs（32位、64位）最大对齐单位为8字节</p><p>2) 需要和<strong>结构体的最大成员</strong>和CPU架构(32位或64位)对比，取小的作为对齐单位</p><p>3) 字节对齐也可以通过程序控制，采用指令：</p><pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(xx)   </span><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(1)     </span><span class="token comment" spellcheck="true">//1字节对齐</span><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(2)     </span><span class="token comment" spellcheck="true">//2字节对齐</span><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(4)     </span><span class="token comment" spellcheck="true">//4字节对齐</span><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(8)     </span><span class="token comment" spellcheck="true">//8字节对齐</span><span class="token macro property">#<span class="token directive keyword">pragma</span> pack(16)    </span><span class="token comment" spellcheck="true">//16字节对齐</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>  #include &lt;stdio.h&gt;</p><p>  #include &lt;stdlib.h&gt;</p><p>  #pragma pack(2)</p><p>  typedef struct<br>  {<br>    int aa1; //4个字节对齐 1111<br>    char bb1;//1个字节对齐 1<br>    short cc1;//2个字节对齐 011<br>    char dd1; //1个字节对齐 1<br>    } testlength;</p><p>  int length = sizeof(testlength); //2个字节对齐，length = 10</p><p>  int main(){<br>      printf(“length=%d\n”, length); // length=10<br>  }</p><pre><code>  尽管通过pragma pack（xx）可以指定字节对齐单位，但需要和结构体的最大成员、CPU架构（32位或64位）对比，取最小的作为对齐单位。- 原则2：数据成员的偏移起点  结构体（struct）的数据成员，第一个数据成员放在偏移量为0的地方，以后每个数据成员存放在偏移量为该数据成员类型大小的整数倍的地方（比如int在32位机器为４字节，则要从4的整数倍地址开始存储）- 原则3：收尾工作   结构体的总大小，也就是sizeof的结果，必须是对齐单位的整数倍，不足的要补齐。## 8、typedef与define的区别- typedef为C语言的关键字，作用是为一种数据类型(基本类型或自定义数据类型)定义一个新名字，不能创建新类型。- 功能差异   typedef仅限于数据类型，而不是能是表达式或具体的值- 执行时间不同     define发生在预处理，typedef发生在编译阶段- 作用域不同    #define没有作用域的限制，而typedef有自己的作用域## 9、C语言文本操作的区别**9.1 二进制文件和文本文件**- b是二进制模式的意思，b只是在Windows有效，在Linux用r和rb的结果是一样的- Unix和Linux下所有的文本文件行都是\n结尾，而Windows所有的文本文件行都是\r\n结尾- 在Windows平台下，以“文本”方式打开文件，不加b：  - 当读取文件的时候，系统会将所有的 &quot;\r\n&quot; 转换成 &quot;\n&quot;  - 当写入文件的时候，系统会将 &quot;\n&quot; 转换成 &quot;\r\n&quot; 写入   - 以&quot;二进制&quot;方式打开文件，则读\写都不会进行这样的转换- 在Unix/Linux平台下，“文本”与“二进制”模式没有区别，&quot;\r\n&quot; 作为两个字符原样输入输出**9.2 文本结尾**在C语言中，EOF表示文件结束符(end of file)。在while循环中以EOF作为文件结束标志，这种以EOF作为文件结束标志的文件，必须是文本文件。在文本文件中，数据都是以字符的ASCII代码值的形式存放。我们知道，ASCII代码值的范围是0~127，不可能出现-1，因此可以用EOF作为文件结束标志。`#define EOF (-1)`当把数据以二进制形式存放到文件中时，就会有-1值的出现，因此不能采用EOF作为二进制文件的结束标志。为解决这一个问题，ANSI C提供一个feof函数，用来判断文件是否结束。feof函数既可用以判断二进制文件又可用以判断文本文件。```c#include &lt;stdio.h&gt;int feof(FILE * stream);功能：检测是否读取到了文件结尾。判断的是最后一次“读操作的内容”，不是当前位置内容(上一个内容)。参数：    stream：文件指针返回值：    非0值：已经到文件结尾    0：没有到文件结尾</code></pre><h2 id="10、void"><a href="#10、void" class="headerlink" title="10、void"></a>10、void</h2><p><strong>void的作用</strong></p><ul><li>对函数参数的限定：当不需要传入参数时，即 <code>function (void);</code></li><li>对函数返回值的限定：当函数没有返回值时，即 <code>void function(void);</code></li></ul><p><strong>void指针的作用</strong></p><p>（1）void指针可以指向任意的数据类型，即任意类型的指针可以赋值给void指针</p><pre class="line-numbers language-c"><code class="language-c"><span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token operator">*</span>p<span class="token punctuation">;</span>p<span class="token operator">=</span>a<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>如果void指针赋值给其他类型，则需要强制转换；<code>a=（int *）p;</code></p><p>（2）在ANSI C标准中不允许对void指针进行算术运算，因为没有特定的数据类型，即在内存中不知道移动多少个字节；而在GNU标准中，认为void指针和char指针等同。</p><p><strong>应用</strong></p><p>（1）void指针一般用于应用的底层，比如malloc函数的返回类型是void指针，需要再强制转换； </p><p>（2）文件句柄HANDLE也是void指针类型，这也是句柄和指针的区别； </p><p>（3）内存操作函数的原型也需要void指针限定传入参数：</p><pre class="line-numbers language-c"><code class="language-c"><span class="token keyword">void</span> <span class="token operator">*</span> <span class="token function">memcpy</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>dest<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">void</span> <span class="token operator">*</span>src<span class="token punctuation">,</span> size_t len<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token operator">*</span> <span class="token function">memset</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>buffer<span class="token punctuation">,</span> <span class="token keyword">int</span> c<span class="token punctuation">,</span> size_t num <span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（4）面向对象函数中底层对基类的抽象。</p><h2 id="11、数据类型的本质"><a href="#11、数据类型的本质" class="headerlink" title="11、数据类型的本质"></a>11、数据类型的本质</h2><ul><li><p>数据类型可理解为创建变量的模具：是固定内存大小的别名。</p></li><li><p>数据类型的作用：编译器预算对象（变量）分配的内存空间大小。</p></li></ul><h2 id="12、变量的本质"><a href="#12、变量的本质" class="headerlink" title="12、变量的本质"></a>12、变量的本质</h2><p>变量的本质：一段连续内存空间的别名。</p><pre><code>1）程序通过变量来申请和命名内存空间 int a = 02）通过变量名访问内存空间3）不是向变量读写数据，而是向变量所代表的内存空间中读写数据</code></pre><h2 id="13、数组与指针的关系"><a href="#13、数组与指针的关系" class="headerlink" title="13、数组与指针的关系"></a>13、数组与指针的关系</h2><p>数组不是指针，数组名也只有在表达式中才会被当成一个指针常量。数组名在表达式中使用的时候，编译器才会产生一个指针常量。</p><ul><li><p><code>p[i]</code>这种写法只不过是<code>*(p + i)</code>的简便写法。实际上，至少对于编译器来说，[]这样的运算符完全可以不存在。[]运算符是为了方便人们读写而引入的，是一种语法糖。</p></li><li><p>当数组名作为sizeof操作符的操作数的时候，此时sizeof返回的是整个数组的长度，而不是指针数组指针的长度。</p></li><li><p>当数组名作为 &amp; 操作符的操作数的时候，此时返回的是一个指向数组的指针，而不是指向某个数组元素的指针常量。</p></li><li><p>二级指针是指向指针的指针，而指针数组则是元素类型为指针的数组。虽然它们是不一样的，但是在表达式中，它们是等效的。</p></li></ul><h2 id="14、字节序（大端、小端）"><a href="#14、字节序（大端、小端）" class="headerlink" title="14、字节序（大端、小端）"></a>14、字节序（大端、小端）</h2><p><strong>14.1 大端和小端</strong></p><p>计算机的内存最小单位是字节。字节序是指多字节（大于1字节）数据的存储顺序，在设计计算机系统的时候，有两种处理内存中数据的方法：大端格式、小端格式。</p><ul><li><p>小端格式（Little-Endian）：将低位字节数据存储在低地址。X86和ARM都是小端对齐。</p></li><li><p>大端格式（Big-Endian）：将高位字节数据存储在低地址。很多Unix服务器的CPU是大端对齐的、网络上数据是以大端对齐。</p></li></ul><p><img src="03.png" alt></p><p>对于整形 0x12345678，它在大端格式和小端格式的系统中，分别如下图所示的方式存放：</p><p><img src="04.png" alt></p><p><strong>14.2 网络字节序和主机字节序</strong></p><p>网络字节顺序NBO(Network Byte Order)</p><pre><code>在网络上使用统一的大端模式，低字节存储在高地址，高字节存储在低地址。</code></pre><p>主机字节序顺序HBO(Host Byte Order)</p><pre><code>不同的机器HBO不相同，与CPU设计相关，数据的顺序是由CPU决定的，而与操作系统无关。</code></pre><p>处理器 |操作系统  |字节排序|</p><p>Alpha    全部    Little endian<br>HP-PA    NT    Little endian<br>HP-PA    UNIX    Big endian<br>Intelx86    全部    Little endian &lt;—–x86系统是小端字节序系统<br>Motorola680x()    全部    Big endian<br>MIPS    NT    Little endian<br>MIPS    UNIX    Big endian<br>PowerPC    NT    Little endian<br>PowerPC    非NT    Big endian  &lt;—–PPC系统是大端字节序系统<br>RS/6000    UNIX    Big endian<br>SPARC    UNIX    Big endian<br>IXP1200 ARM核心    全部    Little endian </p><pre class="line-numbers language-c"><code class="language-c">相关函数：htons 把<span class="token keyword">unsigned</span> <span class="token keyword">short</span>类型从主机序转换到网络序htonl 把<span class="token keyword">unsigned</span> <span class="token keyword">long</span>类型从主机序转换到网络序ntohs 把<span class="token keyword">unsigned</span> <span class="token keyword">short</span>类型从网络序转换到主机序ntohl 把<span class="token keyword">unsigned</span> <span class="token keyword">long</span>类型从网络序转换到主机序头文件：#include <span class="token operator">&lt;</span>netinet<span class="token operator">/</span>in<span class="token punctuation">.</span>h<span class="token operator">></span>定义函数：<span class="token keyword">unsigned</span> <span class="token keyword">short</span> <span class="token function">ntohs</span><span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">short</span> netshort<span class="token punctuation">)</span><span class="token punctuation">;</span>函数说明：<span class="token function">ntohs</span><span class="token punctuation">(</span><span class="token punctuation">)</span>用来将参数指定的<span class="token number">16</span> 位netshort 转换成主机字符顺序<span class="token punctuation">.</span>返回值：返回对应的主机顺序<span class="token punctuation">.</span>范例：参考<span class="token function">getservent</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;winsock.h></span> </span><span class="token comment" spellcheck="true">// windows使用winsock.h</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//左边是高位，右边是低位，高位放高地址，低位放低地址</span>    <span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">0x11223344</span><span class="token punctuation">;</span>    <span class="token keyword">unsigned</span> <span class="token keyword">char</span> <span class="token operator">*</span>p <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">char</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>a<span class="token punctuation">;</span>    <span class="token keyword">int</span> i<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">4</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%x\n"</span><span class="token punctuation">,</span> p<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    u_long b <span class="token operator">=</span> <span class="token function">htonl</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">unsigned</span> <span class="token keyword">char</span> <span class="token operator">*</span>q <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">char</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>b<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span><span class="token number">4</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%x\n"</span><span class="token punctuation">,</span> q<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">//  gcc hello.c -lwsock32 -o hello</span><span class="token comment" spellcheck="true">// 编译时添加-lwsock32，不然会报错undefined reference to `htonl@4'</span><span class="token comment" spellcheck="true">// 在编译socket程序的时候，一定要加上-l wsock32选项，因为mingw默认没有包含windows库</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="15、数组指针"><a href="#15、数组指针" class="headerlink" title="15、数组指针"></a>15、数组指针</h2><pre class="line-numbers language-c"><code class="language-c"><span class="token comment" spellcheck="true">// 1) 先定义数组类型，再根据类型定义指针变量</span><span class="token keyword">typedef</span> <span class="token keyword">int</span> A<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span>A <span class="token operator">*</span>p <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 2) 先定义数组指针类型，根据类型定义指针变量</span><span class="token keyword">typedef</span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token operator">*</span>P<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//第一个()代表指针，第二个[]代表数组</span>P q<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//数据组指针变量</span><span class="token comment" spellcheck="true">// 3) 直接定义数组指针变量</span><span class="token keyword">int</span> <span class="token punctuation">(</span><span class="token operator">*</span>q<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="16、深拷贝和浅拷贝"><a href="#16、深拷贝和浅拷贝" class="headerlink" title="16、深拷贝和浅拷贝"></a>16、深拷贝和浅拷贝</h2><p><strong>结构体</strong>:</p><p>浅拷贝  不同结构体成员指针变量指向同一块内存</p><p>深拷贝  不同结构体成员指针变量指向不同的内存</p><p><strong>类</strong>:</p><p>浅拷贝 类中有动态分配的空间的指针指向相同内存空间</p><p>深拷贝 类中有动态分配的空间的指针指向不同的内存空间</p><h2 id="17、-include-lt-gt-与-include-“”的区别"><a href="#17、-include-lt-gt-与-include-“”的区别" class="headerlink" title="17、#include&lt; &gt; 与 #include “”的区别"></a>17、#include&lt; &gt; 与 #include “”的区别</h2><ul><li><p>“” 表示系统先在file1.c所在的当前目录找file1.h，如果找不到，再按系统指定的目录检索。</p></li><li><p>&lt; &gt; 表示系统直接按系统指定的目录检索。</p></li></ul><p>注意：</p><ul><li>#include &lt;&gt;常用于包含库函数的头文件</li><li>#include “”常用于包含自定义的头文件</li><li>理论上#include可以包含任意格式的文件(.c .h等) ，但我们一般用于头文件的包含。</li></ul><h2 id="18、静态库和动态库"><a href="#18、静态库和动态库" class="headerlink" title="18、静态库和动态库"></a>18、静态库和动态库</h2><p><strong>18.1、静态库优缺点</strong></p><ul><li><p>静态库在程序的链接阶段被复制到了程序中，和程序运行的时候没有关系；</p></li><li><p>程序在运行时与函数库再无瓜葛，移植方便；</p></li><li><p>浪费空间和资源，所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。</p></li></ul><p><strong>18.2、动态库</strong></p><p>要解决空间浪费和更新困难这两个问题，最简单的办法就是把程序的模块相互分割开来，形成独立的文件，而不是将他们静态的链接在一起。</p><p>简单地讲，就是不对哪些组成程序的目标程序进行链接，等程序运行的时候才进行链接。也就是说，把整个链接过程推迟到了运行时再进行，这就是动态链接的基本思想。</p><p><strong>18.3、动态库的lib文件和静态库的lib文件的区别</strong></p><p>在使用动态库的时候，往往提供两个文件：一个引入库（.lib）文件（也称“导入库文件”）和一个DLL（.dll）文件。 </p><p>虽然引入库的后缀名也是“lib”，但是，动态库的引入库文件和静态库文件有着本质的区别，对一个DLL文件来说，其引入库文件（.lib）包含该DLL导出的函数和变量的符号名，而.dll文件包含该DLL实际的函数和数据。</p><p>在使用动态库的情况下，在编译链接可执行文件时，只需要链接该DLL的引入库文件，该DLL中的函数代码和数据并不复制到可执行文件，直到可执行程序运行时，才去加载所需的DLL，将该DLL映射到进程的地址空间中，然后访问DLL中导出的函数。</p><h1 id="二、C-篇"><a href="#二、C-篇" class="headerlink" title="二、C++篇"></a>二、C++篇</h1><h2 id="1、c语言和c-语言的关系"><a href="#1、c语言和c-语言的关系" class="headerlink" title="1、c语言和c++语言的关系"></a>1、c语言和c++语言的关系</h2><p>“c++”中的++来自于c语言中的递增运算符++，该运算符将变量加1。c++起初也叫”c with clsss”。通过名称表明，c++是对C的扩展，因此c++是c语言的超集，这意味着任何有效的c程序都是有效的c++程序。c++程序可以使用已有的c程序库。</p><p>c++语言在c语言的基础上添加了<strong>面向对象编程</strong>和<strong>泛型编程</strong>的支持。c++继承了c语言高效，简洁，快速和可移植的传统。</p><p>c++融合了3种不同的编程方式:</p><ul><li><p>c语言代表的过程性语言.</p></li><li><p>c++在c语言基础上添加的类代表的面向对象语言.</p></li><li><p>c++模板支持的泛型编程。</p></li></ul><h2 id="2、左值和右值"><a href="#2、左值和右值" class="headerlink" title="2、左值和右值"></a>2、左值和右值</h2><p>判断是否是左值，有一个简单的方法，就是看看能否取它的地址，能取地址就是左值，否则就是右值。</p><p>当一个对象成为右值时，使用的是它的值（内容）, 而成为左值时，使用的是它的身份（在内存中的位置）。</p><p>平常所说的引用，实际上指的就是左值引用<code>lvalue reference</code>, 常用单个&amp;来表示。左值引用只能接收左值，不能接收右值。<strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></p><p><strong>为了支持移动操作，在c++11版本，增加了右值引用。</strong>右值引用一般用于绑定到一个即将销毁的对象，所以右值引用又通常出现在移动构造函数中。</p><p>看完下面的例子，左值和右值基本就清楚了，左值具有持久的状态，有独立的内存空间，右值要么是字面常量，要么就是表达式求值过程中创建的临时对象</p><pre class="line-numbers language-c++"><code class="language-c++">int i = 66;int &r = i ; //r 是一个左引用，绑定左值 iint &&rr = i ; //rr是一个右引用，绑定到左值i , 错误！int &r2 = i*42 ; //  r2 是一个左引用， 而i*42是一个表达式，计算出来的结果是一个右值。 错误！const int &r3 = i*42; // const修饰的左值引用 正确int &&rr2 = i*42 ; // 右引用，绑定右值 正确<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、c-对c的扩展"><a href="#3、c-对c的扩展" class="headerlink" title="3、c++对c的扩展"></a>3、c++对c的扩展</h2><p><strong>3.1 三目运算符</strong></p><p>c语言三目运算表达式返回值为数据值，为右值，不能赋值。</p><p>c++语言三目运算表达式返回值为变量本身（引用），为左值，可以赋值。</p><pre class="line-numbers language-c++"><code class="language-c++">int a = 10;int b = 20;printf("ret:%d\n", a > b ? a : b);//思考一个问题，(a > b ? a : b) 三目运算表达式返回的是什么？cout << "b:" << b << endl;//返回的是左值，变量的引用(a > b ? a : b) = 100;//返回的是左值，变量的引用cout << "b:" << b << endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3.2 bool</strong></p><p>c++中新增bool类型关键字</p><ul><li>bool类型只有两个值，true（1）， false（0）</li><li>bool类型占1个字节</li><li>给bool类型赋值时, 非0值会自动转换为true（1）, 0值会自动转换为false（0）</li></ul><p>C语言中也有bool类型，在c99标准之前是没有bool关键字，c99标准已经有bool类型，包含头文件<code>stdbool.h</code>,就可以使用和c++一样的bool类型。</p><p><strong>3.3 struct类型增强</strong></p><ul><li><p>c中定义结构体变量需要加上struct关键字，c++不需要。</p></li><li><p>c中的结构体只能定义成员变量，不能定义成员函数。c++即可以定义成员变量，也可以定义成员函数。</p></li></ul><p><strong>3.4 更严格的类型转换</strong></p><p>在C++中，不同类型的变量一般是不能直接赋值的，需要相应的强转。</p><p>在C++中，所有的变量和函数都必须有类型</p><p><strong>3.5 全局变量检测增强</strong></p><pre class="line-numbers language-c++"><code class="language-c++">int a = 10; //赋值，当做定义int a; //没有赋值，当做声明int main(){    printf("a:%d\n",a);    return EXIT_SUCCESS;}// 上面的代码在c++下编译失败，在c下编译通过。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、内部连接和外部连接"><a href="#4、内部连接和外部连接" class="headerlink" title="4、内部连接和外部连接"></a>4、内部连接和外部连接</h2><p>内部连接：如果一个名称对编译单元（.cpp）来说是局部的，在链接的时候其他的编译单元无法链接到它且不会与其它编译单元（.cpp）中的同样的名称相冲突。例如static函数，inline函数等（注 : 用static修饰的函数，本限定在本源码文件中，不能被本源码文件以外的代码文件调用。而普通的函数，默认是extern的，也就是说，可以被其它代码文件调用该函数。）</p><p>外部连接：如果一个名称对编译单元（.cpp）来说不是局部的，而在链接的时候其他的编译单元可以访问它，也就是说它可以和别的编译单元交互。 例如全局变量就是外部链接 。</p><h2 id="5、C-C-中const的区别"><a href="#5、C-C-中const的区别" class="headerlink" title="5、C/C++中const的区别"></a>5、C/C++中const的区别</h2><p>1、const全局变量</p><p>c语言全局const会被存储到只读数据区。c++中全局const当声明extern或者对变量取地址时，编译器会分配存储地址，变量存储在只读数据段。两个都受到了只读数据区的保护，不可修改。</p><pre class="line-numbers language-c++"><code class="language-c++">const int constA = 10;int main(){    int* p = (int*)&constA;    *p = 200;}// 以上代码在c/c++中编译通过，在运行期，修改constA的值时，发生写入错误。原因是修改只读数据段的数据。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2、const 局部变量</p><p>c语言中局部const存储在堆栈区，只是不能通过变量直接修改const只读变量的值，但是可以跳过编译器的检查，通过指针间接修改const值。</p><pre class="line-numbers language-c++"><code class="language-c++">const int constA = 10;int* p = (int*)&constA;*p = 300;printf("constA:%d\n",constA);printf("*p:%d\n", *p);// constA:300// *p:300<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>c++中对于局部的const变量要区别对待：</p><ul><li>对于基础数据类型，也就是const int a = 10这种，编译器会把它放到符号表中，不分配内存，当对其取地址时，会分配内存。</li></ul><pre class="line-numbers language-c++"><code class="language-c++">const int constA = 10;int* p = (int*)&constA;*p = 300;cout << "constA:" << constA << endl;cout << "*p:" << *p << endl;// constA:10// *p:300// constA在符号表中，当我们对constA取地址，这个时候为constA分配了新的空间，*p操作的是分配的空间，而constA是从符号表获得的值。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​    </p><ul><li>对于基础数据类型，如果用一个变量初始化const变量，如果const int a = b,那么也是会给a分配内存。</li></ul><pre class="line-numbers language-c++"><code class="language-c++">int b = 10;const int constA = b;int* p = (int*)&constA;*p = 300;cout << "constA:" << constA << endl;cout << "*p:" << *p << endl;// constA:300// *p:300 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>对于自定数据类型，比如类对象，那么也会分配内存。</li></ul><pre class="line-numbers language-c++"><code class="language-c++">const Person person; //未初始化age//person.age = 50; //不可修改Person* pPerson = (Person*)&person;//指针间接修改pPerson->age = 100;cout << "pPerson->age:" << pPerson->age << endl;pPerson->age = 200;cout << "pPerson->age:" << pPerson->age << endl;// pPerson->age:100// pPerson->age:200//为person分配了内存，所以我们可以通过指针的间接赋值修改person对象。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3、链接方式</p><p>c中const默认为外部连接，c++中const默认为内部连接.当c语言两个文件中都有const int a的时候，编译器会报重定义的错误。而在c++中，则不会，因为c++中的const默认是内部连接的。如果想让c++中的const具有外部连接，必须显示声明为: extern const int a = 10;</p><h2 id="6、const与-define的区别"><a href="#6、const与-define的区别" class="headerlink" title="6、const与#define的区别"></a>6、const与#define的区别</h2><ul><li><p>const有数据类型，可进行编译器类型安全检查。#define无类型，不可以进行类型检查</p></li><li><p>const有作用域，而#define不重视作用域，默认定义处到文件结尾。如果想定义在指定作用域下有效的常量，那么#define就不能用。</p></li></ul><h2 id="7、引用"><a href="#7、引用" class="headerlink" title="7、引用"></a>7、引用</h2><ol><li><p>&amp;在此不是求地址运算，而是起标识作用。</p></li><li><p>类型标识符是指目标变量的类型</p></li><li><p>必须在声明引用变量时进行初始化。</p></li><li><p><strong>引用初始化之后不能改变。</strong></p></li><li><p>不能有NULL引用。必须确保引用是和一块合法的存储单元关联。</p></li><li><p><strong>可以建立对数组的引用。</strong></p></li><li><p>函数不能返回局部变量的引用</p></li><li><p>函数当左值，必须返回引用</p></li></ol><p><strong>引用的本质</strong></p><p>引用的本质是在c++内部实现一个指针常量</p><p><code>Type&amp; ref = val;  // Type* const ref = val;</code></p><p>c++编译器在编译过程中使用常指针作为引用的内部实现，因此引用所占用的空间大小与指针相同，只是这个过程是编译器内部实现，用户不可见。</p><pre class="line-numbers language-c++"><code class="language-c++">//发现是引用，转换为 int* const ref = &a;void testFunc(int& ref){    ref = 100; // ref是引用，转换为*ref = 100}int main(){    int a = 10;    int& aRef = a; //自动转换为 int* const aRef = &a;这也能说明引用为什么必须初始化    aRef = 20; //内部发现aRef是引用，自动帮我们转换为: *aRef = 20;    cout << "a:" << a << endl;    cout << "aRef:" << aRef << endl;    testFunc(a);    return EXIT_SUCCESS;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="8、面向对象的三大特性"><a href="#8、面向对象的三大特性" class="headerlink" title="8、面向对象的三大特性"></a>8、面向对象的三大特性</h2><p><strong>封装</strong></p><ol><li>把变量（属性）和函数（操作）合成一个整体，封装在一个类中</li><li>对变量和函数进行访问控制</li></ol><p><strong>继承</strong></p><pre><code>c++最重要的特征是代码重用，通过继承机制可以利用已有的数据类型来定义新的数据类型，新的类不仅拥有旧类的成员，还拥有新定义的成员。派生类继承基类，派生类拥有基类中全部成员变量和成员方法（除了构造和析构之外的成员方法），但是在派生类中，继承的成员并不一定能直接访问，不同的继承方式会导致不同的访问权限。任何时候重新定义基类中的一个重载函数，在新类中所有的其他版本将被自动隐藏.operator=也不能被继承，因为它完成类似构造函数的行为。</code></pre><p><strong>多态</strong></p><pre><code>c++支持编译时多态(静态多态)和运行时多态(动态多态)，运算符重载和函数重载就是编译时多态，而派生类和虚函数实现运行时多态。静态多态和动态多态的区别就是函数地址是早绑定(静态联编)还是晚绑定(动态联编)。如果在编译阶段就可以确定函数的调用地址，并产生代码，就是静态多态(编译时多态)，就是说地址是早绑定的。而如果函数的调用地址不能编译不能在编译期间确定，而需要在运行时才能决定，这这就属于晚绑定(动态多态,运行时多态)。多态性改善了代码的可读性和组织性，同时也使创建的程序具有可扩展性。</code></pre><p>​    C++的函数调用默认不使用动态绑定。要触发动态绑定，必须满足两个条件：<br>​        1、只有指定为虚函数的成员函数才能进行动态绑定<br>​        2、必须通过基类类型的引用或指针进行函数调用</p><h2 id="9、C-编译器优化技术：RVO-NRVO和复制省略"><a href="#9、C-编译器优化技术：RVO-NRVO和复制省略" class="headerlink" title="9、C++编译器优化技术：RVO/NRVO和复制省略"></a>9、C++编译器优化技术：RVO/NRVO和复制省略</h2><p>现代编译器缺省会使用RVO（return value optimization，返回值优化）、NRVO（named return value optimization、命名返回值优化）和复制省略（Copy elision）技术，来减少拷贝次数来提升代码的运行效率</p><p>注1：vc6、vs没有提供编译选项来关闭该优化，无论是debug还是release都会进行RVO和复制省略优化</p><p>注2：vc6、vs2005以下及vs2005+ Debug上不支持NRVO优化，vs2005+ Release支持NRVO优化</p><p>注3：g++支持这三种优化，并且可通过编译选项：-fno-elide-constructors来关闭优化</p><p><strong>RVO</strong></p><pre class="line-numbers language-c++"><code class="language-c++">#include <stdio.h>class A{public:    A()    {        printf("%p construct\n", this);    }    A(const A& cp)    {        printf("%p copy construct\n", this);    }    ~A()     {        printf("%p destruct\n", this);    }};A GetA(){    return A();}int main(){    {        A a = GetA();    }    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p><pre><code>0x7ffe9d1edd0f construct0x7ffe9d1edd0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下：</p><pre><code>0x7ffc46947d4f construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量temp0x7ffc46947d7f copy construct // 函数GetA return语句处，把临时变量temp做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来0x7ffc46947d4f destruct // 函数GetA执行完return语句后，临时变量temp生命周期结束，调用其析构函数~A()0x7ffc46947d7e copy construct // 函数GetA调用结束，返回上层main函数后，把返回值变量ret做为参数传入并调用拷贝构造函数A(const A&amp; cp)将变量A a构造出来0x7ffc46947d7f destruct // A a = GetA()语句结束后，返回值ret生命周期结束，调用其析构函数~A()0x7ffc46947d7e destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：临时变量temp、返回值ret均为匿名变量</p><p><strong>NRVO</strong></p><p>g++编译器、vs2005+ Release（开启/O2及以上优化开关）</p><p>修改上述代码，将GetA的实现修改成：</p><pre class="line-numbers language-c++"><code class="language-c++">A GetA(){    A o;    return o;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在g++、vs2005+ Release中，上述代码也仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p><pre><code>0x7ffe9d1edd0f construct0x7ffe9d1edd0f destruct</code></pre><p>g++加上-fno-elide-constructors选项关闭优化后，和上述结果一样</p><pre><code>0x7ffc46947d4f construct0x7ffc46947d7f copy construct0x7ffc46947d4f destruct0x7ffc46947d7e copy construct0x7ffc46947d7f destruct0x7ffc46947d7e destruct</code></pre><p>但在vc6、vs2005以下、vs2005+ Debug中，没有进行NRVO优化，输出结果为：</p><pre><code>18fec4 construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量o18ff44 copy construct  // 函数GetA return语句处，把临时变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来18fec4 destruct  // 函数GetA执行完return语句后，临时变量o生命周期结束，调用其析构函数~A()18ff44 destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：<strong>与g++、vs2005+ Release相比，vc6、vs2005以下、vs2005+ Debug只优化掉了返回值到变量a的拷贝，命名局部变量o没有被优化掉，所以最后一共有2次构造和析构的调用</strong></p><p><strong>复制省略</strong></p><p>典型情况是：调用构造函数进行值类型传参</p><pre><code>void Func(A a) {}int main(){    {        Func(A());    }    return 0;}</code></pre><p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p><pre><code>0x7ffeb5148d0f construct0x7ffeb5148d0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下： </p><pre><code>0x7ffc53c141ef construct   // 在main函数中，调用无参构造函数构造实参变量o0x7ffc53c141ee copy construct // 调用Func函数后，将实参变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将形参变量a构造出来0x7ffc53c141ee destruct // 函数Func执行完后，形参变量a生命周期结束，调用其析构函数~A()0x7ffc53c141ef destruct // 返回main函数后，实参变量o要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p><strong>优化失效的情况</strong></p><ol><li>根据不同的条件分支，返回不同变量</li><li>返回参数变量</li><li>返回全局变量</li><li>返回复合函数类型中的成员变量</li><li>返回值赋值给已构造好的变量（此时会调用operator==赋值运算符）</li></ol><p><a href="https://www.cnblogs.com/kekec/p/11303391.html" target="_blank" rel="noopener">https://www.cnblogs.com/kekec/p/11303391.html</a></p><h2 id="10、explicit关键字"><a href="#10、explicit关键字" class="headerlink" title="10、explicit关键字"></a>10、explicit关键字</h2><ul><li><p>explicit用于修饰构造函数,防止隐式转化。</p></li><li><p>是针对单参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造)而言。</p></li></ul><pre class="line-numbers language-c++"><code class="language-c++">class MyString{public:    explicit MyString(int n){        cout << "MyString(int n)!" << endl;    }    MyString(const char* str){        cout << "MyString(const char* str)" << endl;    }};int main(){    //给字符串赋值？还是初始化？    //MyString str1 = 1;     MyString str2(10);    //寓意非常明确，给字符串赋值    MyString str3 = "abcd";    MyString str4("abcd");    return EXIT_SUCCESS;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="11、new-delete-malloc-free"><a href="#11、new-delete-malloc-free" class="headerlink" title="11、new/delete/malloc/free"></a>11、new/delete/malloc/free</h2><p><strong>11.1、new</strong></p><ol><li>内存申请成功后，会返回一个指向该内存的地址。</li><li>若内存申请失败，则抛出异常，</li><li>申请成功后，如果是程序员定义的类型，会执行相应的构造函数</li></ol><p><strong>11.2、delete</strong></p><ol><li>如果指针的值是0 ，delete不会执行任何操作，有检测机制</li><li>delete只是释放内存，不会修改指针，指针仍然会指向原来的地址</li><li>重复delete，有可能出现异常</li><li>如果是自定义类型，会执行析构函数</li></ol><p><strong>11.3、malloc</strong></p><ol><li>malloc 申请成功之后，返回的是void类型的指针。需要将void*指针转换成我们需要的类型。1.</li><li>malloc 要求制定申请的内存大小 ， 而new由编译器自行计算。</li><li>申请失败，返回的是NULL ， 比如： 内存不足。</li><li>不会执行自定义类型的构造函数</li></ol><p><strong>11.4、free</strong></p><ol><li>如果是空指针，多次释放没有问题，非空指针，重复释放有问题</li><li>不会执行对应的析构</li><li>delete的底层执行的是free</li></ol><h2 id="12、const"><a href="#12、const" class="headerlink" title="12、const"></a>12、const</h2><ol><li>const修饰静态成员变量时可以在类内部初始化</li><li>const 修饰成员函数时，修饰的是this指针，所以成员函数内不可以修改任何普通成员变量，当成员变量类型符前用mutable修饰时例外</li><li>常对象(cons修饰的对象)只能调用const修饰的成员函数</li><li>常对象可以访问成员属性，但是不能修改</li><li><strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></li></ol><h2 id="13、虚继承"><a href="#13、虚继承" class="headerlink" title="13、虚继承"></a>13、虚继承</h2><p>虚继承是解决C++多重继承问题的一种手段，从不同途径继承来的同一基类，会在子类中存在多份拷贝。这将存在两个问题：其一，浪费存储空间；第二，存在二义性问题，多重继承可能存在一个基类的多份拷贝，这就出现了二义性。</p><pre class="line-numbers language-c++"><code class="language-c++">class BigBase{public:    BigBase(){ mParam = 0; }    void func(){ cout << "BigBase::func" << endl; }public:    int mParam;};class Base1 : public BigBase{};class Base2 : public BigBase{};class Derived : public Base1, public Base2{};int main(){    Derived derived;    //1. 对“func”的访问不明确    //derived.func();    //cout << derived.mParam << endl;    cout << "derived.Base1::mParam:" << derived.Base1::mParam << endl;    cout << "derived.Base2::mParam:" << derived.Base2::mParam << endl;    //2. 重复继承    cout << "Derived size:" << sizeof(Derived) << endl; //8    return EXIT_SUCCESS;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虚继承可以解决多种继承前面提到的两个问题：</p><p>虚继承底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，64位8字节/windows 4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类指针依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，<strong>虚基类指针也会被继承。</strong></p><p>实际上，<code>vbptr</code>指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。</p><p><img src="05.png" alt></p><pre class="line-numbers language-c++"><code class="language-c++">#include <iostream>using namespace std;class A  //大小为4{public:    int a;};class B:virtual public A{ // vbptr 8, int b 4, int a 4 = 12public:    int b;};class C:virtual public A{// vbptr 8, int c 4, int a 4 = 12public:    int c;};class D:public B, public C{    // int a, b, c, d=16    // class B  vbptr 4    // class C  vbptr 4  = 24public:    int d;};int main(){    A a;    B b;    C c;    D d;    cout << sizeof(a) << endl; // 4     cout << sizeof(b) << endl; // 16    cout << sizeof(c) << endl; // 16    cout << sizeof(d) << endl; // 24    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>链接:<a href="https://blog.csdn.net/bxw1992/article/details/77726390" target="_blank" rel="noopener">https://blog.csdn.net/bxw1992/article/details/77726390</a></p><h2 id="14、虚函数"><a href="#14、虚函数" class="headerlink" title="14、虚函数"></a>14、虚函数</h2><p><strong>问题</strong></p><pre><code>父类引用或指针可以指向子类对象，通过父类指针或引用来操作子类对象。但是由于编译阶段编译器根据对象的指针或者引用选择函数调用，所以会调用父类的函数。解决问题的方法是迟绑定(动态绑定)，在运行时根据对象的实际类型决定。</code></pre><p><strong>解决</strong></p><pre><code>C++动态多态性是通过虚函数来实现的，虚函数允许子类（派生类）重新定义父类（基类）成员函数，而子类（派生类）重新定义父类（基类）虚函数的做法称为覆盖(override)，或者称为重写。对于特定的函数进行动态绑定，c++要求在基类中声明这个函数的时候使用virtual关键字,动态绑定也就对virtual函数起作用.</code></pre><ul><li><p>为创建一个需要动态绑定的虚成员函数，可以简单在这个函数声明前面加上virtual关键字，定义时候不需要.</p></li><li><p>如果一个函数在基类中被声明为virtual，那么在所有派生类中它都是virtual的.</p></li><li><p>在派生类中virtual函数的重定义称为重写(override).</p></li><li><p>Virtual关键字只能修饰成员函数.</p></li><li><p>构造函数不能为虚函数</p></li></ul><p><strong>虚函数原理</strong></p><p>首先，我们看看编译器如何处理虚函数。当编译器发现我们的类中有虚函数的时候，编译器会创建一张虚函数表（virtual function table ），表中存储着类对象的虚函数地址，并且给类增加一个指针，这个指针就是<code>vpointer</code>(缩写<code>vptr</code>)，这个指针是指向虚函数表。</p><p>父类对象包含的指针，指向父类的虚函数表地址，子类对象包含的指针，指向子类的虚函数表地址。</p><p>如果子类重新定义了父类的函数，那么函数表中存放的是新的地址，如果子类没有重新定义，那么表中存放的是父类的函数地址。如果子类有自己的虚函数，则只需要添加到表中即可。</p><pre class="line-numbers language-c++"><code class="language-c++">class A{public:    virtual void func1(){}    virtual void func2(){}};//B类为空，那么大小应该是1字节，实际情况是这样吗？class B : public A{};void test(){    cout << "A size:" << sizeof(A) << endl; // win指针字节为4, linux 64 字节为8    cout << "B size:" << sizeof(B) << endl; // 4}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>多态的成立条件：</strong></p><ul><li><p>有继承</p></li><li><p>子类重写父类虚函数函数</p><pre><code>a) 返回值，函数名字，函数参数，必须和父类完全一致(析构函数除外) b) 子类中virtual关键字可写可不写，建议写</code></pre></li><li><p>类型兼容，父类指针，父类引用指向子类对象</p></li></ul><p><strong>抽象类和纯虚函数</strong></p><p>当基类中有至少一个纯虚函数则为抽象类</p><ul><li><p>纯虚函数使用关键字virtual，并在其后面加上=0。如果试图去实例化一个抽象类，编译器则会阻止这种操作。</p></li><li><p>当继承一个抽象类的时候，必须实现所有的纯虚函数，否则由抽象类派生的类也是一个抽象类。</p></li><li><p>Virtual void fun() = 0;告诉编译器在vtable中为函数保留一个位置，但在这个特定位置不放地址。</p></li></ul><p><strong>接口类</strong></p><p>接口类中只有函数原型定义，没有任何数据定义。多重继承接口不会带来二义性和复杂性问题。接口类只是一个功能声明，并不是功能实现，子类需要根据功能说明定义功能实现。</p><p>注意:除了析构函数外，其他声明都是纯虚函数。</p><p><strong>虚析构函数</strong></p><p>虚析构函数是为了解决[基类]的[指针]指向派生类对象，并用基类的指针删除派生类对象。</p><p><strong>虚函数和虚继承的异同</strong></p><p>在这里我们可以对比虚函数的实现原理：他们有相似之处，都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间）。</p><p>虚基类指针依旧存在继承类中，只占用存储空间；基类虚函数指针不存在于子类中，不占用存储空间。</p><p>虚基类表存储的是虚基类相对直接继承类的偏移；而虚函数表存储的是虚函数地址。</p><h2 id="15、函数模板的机制"><a href="#15、函数模板的机制" class="headerlink" title="15、函数模板的机制"></a>15、函数模板的机制</h2><p><strong>函数模板机制结论：</strong></p><ul><li><p>编译器并不是把函数模板处理成能够处理任何类型的函数</p></li><li><p>函数模板通过具体类型产生不同的函数</p></li><li><p>编译器会对函数模板进行两次编译，在声明的地方对模板代码本身进行编译，在调用的地方对参数替换后的代码进行编译。</p></li></ul><p><strong>局限性</strong>:</p><p>编写的模板函数很可能无法处理某些类型，另一方面，有时候通用化是有意义的，但C++语法不允许这样做。为了解决这种问题，可以提供<strong>模板的重载</strong>，为这些特定的类型提供具体化的模板。</p><pre class="line-numbers language-c++"><code class="language-c++">class Person{public:    Person(string name, int age)    {        this->mName = name;        this->mAge = age;    }    string mName;    int mAge;};//普通交换函数template <class T>void mySwap(T &a,T &b){    T temp = a;    a = b;    b = temp;}//第三代具体化，显示具体化的原型和定意思以template<>开头，并通过名称来指出类型//具体化优先于常规模板template<>void mySwap<Person>(Person &p1, Person &p2){    string nameTemp;    int ageTemp;    nameTemp = p1.mName;    p1.mName = p2.mName;    p2.mName = nameTemp;    ageTemp = p1.mAge;    p1.mAge = p2.mAge;    p2.mAge = ageTemp;}void test(){    Person P1("Tom", 10);    Person P2("Jerry", 20);    cout << "P1 Name = " << P1.mName << " P1 Age = " << P1.mAge << endl;    cout << "P2 Name = " << P2.mName << " P2 Age = " << P2.mAge << endl;    mySwap(P1, P2);    cout << "P1 Name = " << P1.mName << " P1 Age = " << P1.mAge << endl;    cout << "P2 Name = " << P2.mName << " P2 Age = " << P2.mAge << endl;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="16、C-类型转换"><a href="#16、C-类型转换" class="headerlink" title="16、C++类型转换"></a>16、C++类型转换</h2><p><strong>静态类型转换static_cast</strong></p><ul><li><p>用于<a href="http://baike.baidu.com/view/2405425.htm" target="_blank" rel="noopener">类层次结构</a>中基类（父类）和<a href="http://baike.baidu.com/view/535532.htm" target="_blank" rel="noopener">派生类</a>（子类）之间指针或引用的转换。</p><ul><li><p>进行上行转换（把派生类的指针或引用转换成基类表示）是安全的；</p></li><li><p>进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。</p></li></ul></li><li><p>用于基本数据类型之间的转换，如把int转换成char，把char转换成int。这种转换的安全性也要开发人员来保证。</p></li></ul><p><strong>动态类型转换(dynamic_cast)</strong></p><ul><li><p>dynamic_cast主要用于类层次间的上行转换和下行转换；</p></li><li><p>在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；</p></li><li><p>在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全；</p></li></ul><p><strong>常量转换(const_cast)</strong></p><p>该运算符用来修改类型的const属性。。</p><ul><li><p>常量指针被转化成非常量指针，并且仍然指向原来的对象；</p></li><li><p>常量引用被转换成非常量引用，并且仍然指向原来的对象；</p></li></ul><p><strong>重新解释转换(reinterpret_cast)</strong></p><p>这是最不安全的一种转换机制，最有可能出问题。</p><p>主要用于将一种数据类型从一种类型转换为另一种类型。它可以将一个指针转换成一个整数，也可以将一个整数转换成一个指针.</p><h2 id="17、C-成员变量初始化顺序"><a href="#17、C-成员变量初始化顺序" class="headerlink" title="17、C++成员变量初始化顺序"></a>17、C++成员变量初始化顺序</h2><ul><li>成员变量在使用初始化列表初始化时，与构造函数中初始化成员列表的顺序无关，只与定义成员变量的顺序有关。因为成员变量的初始化次序是根据变量在内存中次序有关，而内存中的排列顺序早在编译期就根据变量的<strong>定义次序</strong>决定了。</li><li>类中const成员常量必须在构造函数初始化列表中初始化。</li><li>类中static成员变量，必须在类外初始化。</li><li>变量的初始化顺序就应该是：<ol><li>基类的静态变量或全局变量</li><li>派生类的静态变量或全局变量</li><li>基类的成员变量</li><li>派生类的成员变量</li></ol></li></ul><h2 id="18、重载、重定义和重写"><a href="#18、重载、重定义和重写" class="headerlink" title="18、重载、重定义和重写"></a>18、重载、重定义和重写</h2><p><strong>重载，同一作用域的同名函数</strong></p><ol><li><p>同一个作用域</p></li><li><p>参数个数，参数顺序，参数类型不同</p></li><li><p>和函数返回值，没有关系</p></li><li><p>const也可以作为重载条件  //do(const Teacher&amp; t){}  do(Teacher&amp; t)</p></li></ol><p><strong>重定义（隐藏）</strong></p><ol><li><p>有继承</p></li><li><p>子类（派生类）重新定义父类（基类）的同名成员（非virtual函数）</p></li></ol><p><strong>重写（覆盖）</strong></p><ol><li><p>有继承</p></li><li><p>子类（派生类）重写父类（基类）的virtual函数</p></li><li><p>函数返回值，函数名字，函数参数，必须和基类中的虚函数一致</p></li></ol><h1 id="三、STL"><a href="#三、STL" class="headerlink" title="三、STL"></a>三、STL</h1><h2 id="1、STL六大组件简介"><a href="#1、STL六大组件简介" class="headerlink" title="1、STL六大组件简介"></a>1、STL六大组件简介</h2><p>STL提供了六大组件，彼此之间可以组合套用，这六大组件分别是:容器、算法、迭代器、仿函数、适配器（配接器）、空间配置器。</p><p><strong>容器：</strong>各种数据结构，如vector、list、deque、set、map等, 用来存放数据，从实现角度来看，STL容器是一种class template。</p><p><strong>算法：</strong>各种常用的算法，如sort、find、copy、for_each。从实现的角度来看，STL算法是一种function tempalte.</p><p><strong>迭代器：</strong>扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，<strong>迭代器是一种将operator* , operator-&gt; , operator++,operator–等指针相关操作予以重载的class template.</strong> 所有STL容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。原生指针(native pointer)也是一种迭代器。</p><p><strong>仿函数：</strong>行为类似函数，可作为算法的某种策略。从实现角度来看，仿函数是一种重载了operator()的class 或者class template</p><p><strong>适配器：</strong>一种用来修饰容器或者仿函数或迭代器接口的东西。</p><p><strong>空间配置器：</strong>负责空间的配置与管理。从实现角度看，配置器是一个实现了动态空间配置、空间管理、空间释放的class tempalte.</p><h2 id="2、string容器"><a href="#2、string容器" class="headerlink" title="2、string容器"></a>2、string容器</h2><p>C风格字符串(以空字符结尾的字符数组)太过复杂难于掌握，不适合大程序的开发，所以C++标准库定义了一种string类，定义在头文件<string>。</string></p><p>String和c风格字符串对比：</p><ul><li><p>char*是一个指针，String是一个类</p><p>string封装了char*，管理这个字符串，是一个char*型的容器。</p></li><li><p>String封装了很多实用的成员方法</p><p>查找find，拷贝copy，删除delete 替换replace，插入insert</p></li><li><p>不用考虑内存释放和越界</p><p> string管理char*所分配的内存。每一次string的复制，取值都由string类负责维护，不用担心复制越界和取值越界等。</p></li></ul><h2 id="3、vector容器"><a href="#3、vector容器" class="headerlink" title="3、vector容器"></a>3、vector容器</h2><p>vector的数据安排以及操作方式，与array非常相似，两者的唯一差别在于空间的运用的灵活性。Array是静态空间，一旦配置了就不能改变，要换大一点或者小一点的空间，可以，一切琐碎得由自己来，首先配置一块新的空间，然后将旧空间的数据搬往新空间，再释放原来的空间。Vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新元素。因此vector的运用对于内存的合理利用与运用的灵活性有很大的帮助，我们再也不必害怕空间不足而一开始就要求一个大块头的array了。</p><p>所谓动态增加大小，并不是在原空间之后续接新空间(因为无法保证原空间之后尚有可配置的空间)，而是一块更大的内存空间，然后将原数据拷贝新空间，并释放原空间。因此，对vector的任何操作，一旦引起空间的重新配置，指向原vector的所有迭代器就都失效了。这是程序员容易犯的一个错误，务必小心。</p><p>为了降低空间配置时的速度成本，vector实际配置的大小可能比客户端需求大一些，以备将来可能的扩充，这边是<strong>容量</strong>的概念。换句话说，<strong>一个vector的容量永远大于或等于其大小，一旦容量等于大小，便是满载，下次再有新增元素，整个vector容器就得另觅居所。</strong></p><h2 id="4、deque容器"><a href="#4、deque容器" class="headerlink" title="4、deque容器"></a>4、deque容器</h2><p>Vector容器是单向开口的连续内存空间，deque则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector容器也可以在头尾两端插入元素，但是在其头部操作效率奇差，无法被接受。</p><p>Deque容器和vector容器最大的差异，一在于deque允许使用常数项时间对头端进行元素的插入和删除操作。二在于deque没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，换句话说，像vector那样，”旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque身上是不会发生的。也因此，deque没有必须要提供所谓的空间保留(reserve)功能.</p><p>虽然deque容器也提供了Random Access Iterator,但是它的迭代器并不是普通的指针，其复杂度和vector不是一个量级，这当然影响各个运算的层面。因此，除非有必要，我们应该尽可能的使用vector，而不是deque。对deque进行的排序操作，为了最高效率，可将deque先完整的复制到一个vector中，对vector容器进行排序，再复制回deque.</p><p>既然deque是分段连续内存空间，那么就必须有中央控制，维持整体连续的假象，数据结构的设计及迭代器的前进后退操作颇为繁琐。Deque代码的实现远比vector或list都多得多。</p><p>Deque采取一块所谓的map(注意，不是STL的map容器)作为主控，这里所谓的map是一小块连续的内存空间，其中每一个元素(此处成为一个结点)都是一个指针，指向另一段连续性内存空间，称作缓冲区。缓冲区才是deque的存储空间的主体。</p><p><img src="06.png" alt></p><h2 id="5、stack容器"><a href="#5、stack容器" class="headerlink" title="5、stack容器"></a>5、stack容器</h2><p>stack是一种先进后出(First In Last Out,FILO)的数据结构，它只有一个出口，形式如图所示。stack容器允许新增元素，移除元素，取得栈顶元素，但是除了最顶端外，没有任何其他方法可以存取stack的其他元素。换言之，stack不允许有遍历行为。</p><p>有元素推入栈的操作称为:push,将元素推出stack的操作称为pop.</p><p>Stack所有元素的进出都必须符合”先进后出”的条件，只有stack顶端的元素，才有机会被外界取用。Stack不提供遍历功能，也不提供迭代器。</p><h2 id="6、queue容器"><a href="#6、queue容器" class="headerlink" title="6、queue容器"></a>6、queue容器</h2><p>Queue是一种先进先出(First In First Out,FIFO)的数据结构，它有两个出口，queue容器允许从一端新增元素，从另一端移除元素。</p><p>Queue所有元素的进出都必须符合”先进先出”的条件，只有queue的顶端元素，才有机会被外界取用。Queue不提供遍历功能，也不提供迭代器。</p><h2 id="7、list容器"><a href="#7、list容器" class="headerlink" title="7、list容器"></a>7、list容器</h2><p>链表是一种物理<a href="http://baike.baidu.com/view/1223079.htm" target="_blank" rel="noopener">存储单元</a>上非连续、非顺序的<a href="http://baike.baidu.com/view/2820182.htm" target="_blank" rel="noopener">存储结构</a>，<a href="http://baike.baidu.com/view/38785.htm" target="_blank" rel="noopener">数据元素</a>的逻辑顺序是通过链表中的<a href="http://baike.baidu.com/view/159417.htm" target="_blank" rel="noopener">指针</a>链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储<a href="http://baike.baidu.com/view/38785.htm" target="_blank" rel="noopener">数据元素</a>的数据域，另一个是存储下一个结点地址的<a href="http://baike.baidu.com/view/159417.htm" target="_blank" rel="noopener">指针</a>域。</p><p>相较于vector的连续线性空间，list就显得负责许多，它的好处是每次插入或者删除一个元素，就是配置或者释放一个元素的空间。因此，list对于空间的运用有绝对的精准，一点也不浪费。而且，对于任何位置的元素插入或元素的移除，list永远是常数时间。</p><p>List和vector是两个最常被使用的容器。</p><p>List容器是一个双向链表。</p><ul><li><p>采用动态存储分配，不会造成内存浪费和溢出</p></li><li><p>链表执行插入和删除操作十分方便，修改指针即可，不需要移动大量元素</p></li><li><p>链表灵活，但是空间和时间额外耗费较大</p></li></ul><p>List有一个重要的性质，插入操作和删除操作都不会造成原有list迭代器的失效。这在vector是不成立的，因为vector的插入操作可能造成记忆体重新配置，导致原有的迭代器全部失效，甚至List元素的删除，也只有被删除的那个元素的迭代器失效，其他迭代器不受任何影响。</p><h2 id="8、set-multiset容器"><a href="#8、set-multiset容器" class="headerlink" title="8、set/multiset容器"></a>8、set/multiset容器</h2><p>Set的特性是。所有元素都会根据元素的键值自动被排序。Set的元素不像map那样可以同时拥有实值和键值，set的元素即是键值又是实值。Set不允许两个元素有相同的键值。</p><p>我们可以通过set的迭代器改变set元素的值吗？不行，因为set元素值就是其键值，关系到set元素的排序规则。如果任意改变set元素值，会严重破坏set组织。换句话说，set的iterator是一种const_iterator.</p><p>set拥有和list某些相同的性质，当对容器中的元素进行插入操作或者删除操作的时候，操作之前所有的迭代器，在操作完成之后依然有效，被删除的那个元素的迭代器必然是一个例外。</p><h2 id="9、map-multimap容器"><a href="#9、map-multimap容器" class="headerlink" title="9、map/multimap容器"></a>9、map/multimap容器</h2><p>Map的特性是，所有元素都会根据元素的键值自动排序。Map所有的元素都是pair,同时拥有实值和键值，pair的第一元素被视为键值，第二元素被视为实值，map不允许两个元素有相同的键值。</p><p>我们可以通过map的迭代器改变map的键值吗？答案是不行，因为map的键值关系到map元素的排列规则，任意改变map键值将会严重破坏map组织。如果想要修改元素的实值，那么是可以的。</p><p>Map和list拥有相同的某些性质，当对它的容器元素进行新增操作或者删除操作时，操作之前的所有迭代器，在操作完成之后依然有效，当然被删除的那个元素的迭代器必然是个例外。</p><p>Multimap和map的操作类似，唯一区别multimap键值可重复。</p><p>Map和multimap都是以红黑树为底层实现机制。</p><h2 id="10、STL容器使用时机"><a href="#10、STL容器使用时机" class="headerlink" title="10、STL容器使用时机"></a>10、STL容器使用时机</h2><table><thead><tr><th></th><th>vector</th><th>deque</th><th>list</th><th>set</th><th>multiset</th><th>map</th><th>multimap</th></tr></thead><tbody><tr><td>典型内存结构</td><td>单端数组</td><td>双端数组</td><td>双向链表</td><td>二叉树</td><td>二叉树</td><td>二叉树</td><td>二叉树</td></tr><tr><td>可随机存取</td><td>是</td><td>是</td><td>否</td><td>否</td><td>否</td><td>对key而言：不是</td><td>否</td></tr><tr><td>元素搜寻速度</td><td>慢</td><td>慢</td><td>非常慢</td><td>快</td><td>快</td><td>对key而言：快</td><td>对key而言：快</td></tr><tr><td>元素安插移除</td><td>尾端</td><td>头尾两端</td><td>任何位置</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><ul><li><p>vector的使用场景：比如软件历史操作记录的存储，我们经常要查看历史记录，比如上一次的记录，上上次的记录，但却不会去删除记录，因为记录是事实的描述。</p></li><li><p>deque的使用场景：比如排队购票系统，对排队者的存储可以采用deque，支持头端的快速移除，尾端的快速添加。如果采用vector，则头端移除时，会移动大量的数据，速度慢。</p><p>​     vector与deque的比较：</p><p>​      一：vector.at()比deque.at()效率高，比如vector.at(0)是固定的，deque的开始位置却是不固定的。</p><p>​    二：如果有大量释放操作的话，vector花的时间更少，这跟二者的内部实现有关。</p><p>​    三：deque支持头部的快速插入与快速移除，这是deque的优点。</p></li><li><p>list的使用场景：比如公交车乘客的存储，随时可能有乘客下车，支持频繁的不确实位置元素的移除插入。</p></li><li><p>set的使用场景：比如对手机游戏的个人得分记录的存储，存储要求从高分到低分的顺序排列。 </p></li><li><p>map的使用场景：比如按ID号存储十万个用户，想要快速要通过ID查找对应的用户。二叉树的查找效率，这时就体现出来了。如果是vector容器，最坏的情况下可能要遍历完整个容器才能找到该用户。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> C </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之CNN模型演化</title>
      <link href="/2019/12/30/shen-du-xue-xi-zhi-cnn-mo-xing-yan-hua/"/>
      <url>/2019/12/30/shen-du-xue-xi-zhi-cnn-mo-xing-yan-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h1><h1 id="一、LeNet"><a href="#一、LeNet" class="headerlink" title="一、LeNet"></a>一、LeNet</h1><p>1998年LeCun发布了<a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" target="_blank" rel="noopener">LeNet</a>网络架构，从而揭开了深度学习的神秘面纱。</p><p>​    <img src="c1.png" alt></p><p>和“现在的CNN”相比， LeNet有几个不同点。</p><ul><li><p>第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。</p></li><li><p>第二个不同点在于池化层。原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。</p></li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> keras<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> np_utils<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Flatten<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token comment" spellcheck="true">#load the MNIST dataset from keras datasets</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Process data</span>X_train <span class="token operator">=</span> X_train<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Expend dimension for 1 cahnnel image</span>X_test <span class="token operator">=</span> X_test<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Expend dimension for 1 cahnnel image</span>X_train <span class="token operator">=</span> X_train <span class="token operator">/</span> <span class="token number">255</span> <span class="token comment" spellcheck="true"># Normalize</span>X_test <span class="token operator">=</span> X_test <span class="token operator">/</span> <span class="token number">255</span> <span class="token comment" spellcheck="true"># Normalize</span><span class="token comment" spellcheck="true">#One hot encoding</span>y_train <span class="token operator">=</span> np_utils<span class="token punctuation">.</span>to_categorical<span class="token punctuation">(</span>y_train<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>y_test <span class="token operator">=</span> np_utils<span class="token punctuation">.</span>to_categorical<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Build LetNet model with Keras</span><span class="token keyword">def</span> <span class="token function">LetNet</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> classes<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># initialize the model</span>    model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># first layer, convolution and pooling</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">)</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># second layer, convolution and pooling</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">)</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> filters<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Fully connection layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span>activation <span class="token operator">=</span> <span class="token string">'tanh'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span>activation <span class="token operator">=</span> <span class="token string">'tanh'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># softmax classifier</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Activation<span class="token punctuation">(</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> modelLetNet_model <span class="token operator">=</span> LetNet<span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>LetNet_model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>LetNet_model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss <span class="token operator">=</span> <span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Strat training</span>History <span class="token operator">=</span> LetNet_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>validation_data<span class="token operator">=</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Plot Loss and accuracy</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h1><p>2012年，Alex Krizhevsky发表了AlexNet，相对比LeNet它的网络层次更加深，从LeNet的5层到AlexNet的8层，更重要的是AlexNet还赢得了2012年的ImageNet竞赛的第一。AlexNet不仅比LeNet的神经网络层数更多更深，并且可以学习更复杂的图像高维特征。</p><p>​    <img src="c2.png" alt></p><p>AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然结构上AlexNet和LeNet没有大的不同，但有以下几点差异。</p><ul><li>激活函数使用ReLU。</li><li>使用进行局部正规化的LRN（Local Response Normalization）层</li><li>使用Dropout</li><li>引入max pooling</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> keras<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> np_utils<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Flatten<span class="token punctuation">,</span>Dropout<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token comment" spellcheck="true">#Load oxflower17 dataset</span><span class="token keyword">import</span> tflearn<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>oxflower17 <span class="token keyword">as</span> oxflower17<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitx<span class="token punctuation">,</span> y <span class="token operator">=</span> oxflower17<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>one_hot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Split train and test data</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Data augumentation with Keras tools</span><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>image <span class="token keyword">import</span> ImageDataGeneratorimg_gen <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>    rescale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">,</span>    shear_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    zoom_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    horizontal_flip<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Build AlexNet model</span><span class="token keyword">def</span> <span class="token function">AlexNet</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> classes<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#First Convolution and Pooling layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span>height<span class="token punctuation">,</span>depth<span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Second Convolution and Pooling layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Three Convolution layer and Pooling Layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Fully connection layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Classfication layer</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> modelAlexNet_model <span class="token operator">=</span> AlexNet<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">17</span><span class="token punctuation">)</span>AlexNet_model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>AlexNet_model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.00001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss <span class="token operator">=</span> <span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Start training using dataaugumentation generator</span>History <span class="token operator">=</span> AlexNet_model<span class="token punctuation">.</span>fit_generator<span class="token punctuation">(</span>img_gen<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>X_train<span class="token operator">*</span><span class="token number">255</span><span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                      steps_per_epoch <span class="token operator">=</span> len<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">16</span><span class="token punctuation">,</span> validation_data <span class="token operator">=</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">30</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Plot Loss and Accuracy</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="三、Network-in-network"><a href="#三、Network-in-network" class="headerlink" title="三、Network-in-network"></a>三、Network-in-network</h1><p>2013年年尾，Min Lin提出了在卷积后面再跟一个1x1的卷积核对图像进行卷积，这就是<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network-in-network</a>的核心思想了。NiN在每次卷积完之后使用，目的是为了在进入下一层的时候合并更多的卷积特征，减少网络参数、同样的内存可以存储更大的网络。</p><p><strong>1x1卷积核的作用</strong></p><ul><li><p>缩放通道的大小</p><p>通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。</p></li><li><p>增加非线性</p><p>1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。</p></li><li><p>减少参数</p><p>在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。</p></li></ul><h1 id="四、VGG"><a href="#四、VGG" class="headerlink" title="四、VGG"></a>四、VGG</h1><p>VGG 在 2014 年的ILSVRC比赛中最终获得了第 2 名的成绩.</p><p>​    <img src="c3.png" alt></p><p><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>的创新是使用3x3的小型卷积核连续卷积。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。</p><p>​    <img src="c4.png" alt></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> keras<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> np_utils<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Flatten<span class="token punctuation">,</span>Dropout<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token comment" spellcheck="true">#Load oxflower17 dataset</span><span class="token keyword">import</span> tflearn<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>oxflower17 <span class="token keyword">as</span> oxflower17<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitx<span class="token punctuation">,</span> y <span class="token operator">=</span> oxflower17<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>one_hot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Split train and test data</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Data augumentation with Keras tools</span><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>image <span class="token keyword">import</span> ImageDataGeneratorimg_gen <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>    rescale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">,</span>    shear_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    zoom_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    horizontal_flip<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Build VGG16Net model</span><span class="token keyword">def</span> <span class="token function">VGG16Net</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> classes<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">17</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> modelVGG16_model <span class="token operator">=</span> VGG16Net<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">17</span><span class="token punctuation">)</span>VGG16_model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>VGG16_model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.00001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss <span class="token operator">=</span> <span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Start training using dataaugumentation generator</span>History <span class="token operator">=</span> VGG16_model<span class="token punctuation">.</span>fit_generator<span class="token punctuation">(</span>img_gen<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>X_train<span class="token operator">*</span><span class="token number">255</span><span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                      steps_per_epoch <span class="token operator">=</span> len<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">16</span><span class="token punctuation">,</span> validation_data <span class="token operator">=</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">30</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Plot Loss and Accuracy</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="五、GoogLeNet"><a href="#五、GoogLeNet" class="headerlink" title="五、GoogLeNet"></a>五、GoogLeNet</h1><p>2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842" target="_blank" rel="noopener">GoogleNet</a>了（也叫做Inception V1）。在 2014 年的ILSVRC比赛中最终获得了第 1名的成绩.</p><p>​    <img src="c6.png" alt></p><p>​    <img src="c5.png" alt></p><p>GoogLeNet的特征:</p><ul><li>Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果</li><li>最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> keras<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> np_utils<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Flatten<span class="token punctuation">,</span>Dropout<span class="token punctuation">,</span>BatchNormalization<span class="token punctuation">,</span>AveragePooling2D<span class="token punctuation">,</span>concatenate<span class="token punctuation">,</span>Input<span class="token punctuation">,</span> concatenate<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Model<span class="token punctuation">,</span>load_model<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token comment" spellcheck="true">#Load oxflower17 dataset</span><span class="token keyword">import</span> tflearn<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>oxflower17 <span class="token keyword">as</span> oxflower17<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitx<span class="token punctuation">,</span> y <span class="token operator">=</span> oxflower17<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>one_hot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Split train and test data</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Data augumentation with Keras tools</span><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>image <span class="token keyword">import</span> ImageDataGeneratorimg_gen <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>    rescale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">,</span>    shear_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    zoom_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    horizontal_flip<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Define convolution with batchnromalization</span><span class="token keyword">def</span> <span class="token function">Conv2d_BN</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> nb_filter<span class="token punctuation">,</span>kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> name <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>        bn_name <span class="token operator">=</span> name <span class="token operator">+</span> <span class="token string">'_bn'</span>        conv_name <span class="token operator">=</span> name <span class="token operator">+</span> <span class="token string">'_conv'</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        bn_name <span class="token operator">=</span> None        conv_name <span class="token operator">=</span> None    x <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>nb_filter<span class="token punctuation">,</span>kernel_size<span class="token punctuation">,</span>padding<span class="token operator">=</span>padding<span class="token punctuation">,</span>strides<span class="token operator">=</span>strides<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">,</span>name<span class="token operator">=</span>conv_name<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>name<span class="token operator">=</span>bn_name<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span class="token comment" spellcheck="true">#Define Inception structure</span><span class="token keyword">def</span> <span class="token function">Inception</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter_para<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">(</span>branch1<span class="token punctuation">,</span>branch2<span class="token punctuation">,</span>branch3<span class="token punctuation">,</span>branch4<span class="token punctuation">)</span><span class="token operator">=</span> nb_filter_para    branch1x1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    branch3x3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    branch3x3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>branch3x3<span class="token punctuation">)</span>    branch5x5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    branch5x5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch3<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>branch5x5<span class="token punctuation">)</span>    branchpool <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    branchpool <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>branch4<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">(</span>branchpool<span class="token punctuation">)</span>    x <span class="token operator">=</span> concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>branch1x1<span class="token punctuation">,</span>branch3x3<span class="token punctuation">,</span>branch5x5<span class="token punctuation">,</span>branchpool<span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span class="token comment" spellcheck="true">#Build InceptionV1 model</span><span class="token keyword">def</span> <span class="token function">InceptionV1</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> classes<span class="token punctuation">)</span><span class="token punctuation">:</span>    inpt <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span>height<span class="token punctuation">,</span>depth<span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>inpt<span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">192</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 3a 28x28x256</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">192</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">96</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 3b 28x28x480</span>    x <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#14x14x480</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">208</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">48</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 4a 14x14x512</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">112</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 4a 14x14x512</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 4a 14x14x512</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">112</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">144</span><span class="token punctuation">,</span><span class="token number">288</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 4a 14x14x528</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span><span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 4a 14x14x832</span>    x <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#7x7x832</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span><span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 5a 7x7x832</span>    x <span class="token operator">=</span> Inception<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span><span class="token number">384</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">48</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#Inception 5b 7x7x1024</span>    <span class="token comment" spellcheck="true">#Using AveragePooling replace flatten</span>    x <span class="token operator">=</span> AveragePooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model<span class="token operator">=</span>Model<span class="token punctuation">(</span>input<span class="token operator">=</span>inpt<span class="token punctuation">,</span>output<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> modelInceptionV1_model <span class="token operator">=</span> InceptionV1<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">17</span><span class="token punctuation">)</span>InceptionV1_model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>InceptionV1_model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.00001</span><span class="token punctuation">,</span> beta_1<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> beta_2<span class="token operator">=</span><span class="token number">0.999</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss <span class="token operator">=</span> <span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>History <span class="token operator">=</span> InceptionV1_model<span class="token punctuation">.</span>fit_generator<span class="token punctuation">(</span>img_gen<span class="token punctuation">.</span>flow<span class="token punctuation">(</span>X_train<span class="token operator">*</span><span class="token number">255</span><span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>steps_per_epoch <span class="token operator">=</span> len<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">16</span><span class="token punctuation">,</span> validation_data <span class="token operator">=</span> <span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">30</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Plot Loss and accuracy</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="六、Inception-V3"><a href="#六、Inception-V3" class="headerlink" title="六、Inception V3"></a>六、Inception V3</h1><p>Christian 和他的团队都是非常高产的研究人员。2015 年 2 月，<strong>Batch-normalized Inception</strong> 被引入作为<strong>Inception V2</strong>。</p><p>2015年12月，他们发布了一个新版本的GoogLeNet(<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Inception V3</a>)模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：</p><ul><li>通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps</li><li>当网络层数深度增加时，特征的数量或层的宽度也相对应地增加</li><li>在每一层使用宽度增加以增加下一层之前的特征的组合</li><li><strong>只使用3x3卷积</strong></li></ul><p>因此最后的模型就变成这样了：</p><p>​    <img src="c7.png" alt></p><h1 id="七、ResNet"><a href="#七、ResNet" class="headerlink" title="七、ResNet"></a>七、ResNet</h1><p>2015年12月<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">ResNet</a>发表了，时间上大概与Inception v3网络一起发表的。</p><p>我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，会出现梯度消失、梯度爆炸、网络退化，导致最终性能不佳。 ResNet中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。 </p><p><img src="09.png" alt></p><p>图，在连续2层的卷积层中，将输入x跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出$F(x)$变成了$F(x) + x$。通过引入这种快捷结构，即使加深层，也能高效地学习。 </p><p>因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。</p><p><img src="c8.png" alt></p><p>ResNet通过以2个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到150层以上，识别精度也会持续提高。并且，在ILSVRC大赛中， ResNet的错误识别率为3.5%（前5类中包含正确解这一精度下的错误识别率），令人称奇。 </p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> keras<span class="token keyword">from</span> keras<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist<span class="token keyword">from</span> keras<span class="token punctuation">.</span>utils <span class="token keyword">import</span> np_utils<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Activation<span class="token punctuation">,</span> Conv2D<span class="token punctuation">,</span> MaxPooling2D<span class="token punctuation">,</span> Flatten<span class="token punctuation">,</span>Dropout<span class="token punctuation">,</span>BatchNormalization<span class="token punctuation">,</span>AveragePooling2D<span class="token punctuation">,</span>concatenate<span class="token punctuation">,</span>Input<span class="token punctuation">,</span> concatenate<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Model<span class="token punctuation">,</span>load_model<span class="token keyword">from</span> keras<span class="token punctuation">.</span>optimizers <span class="token keyword">import</span> Adam<span class="token comment" spellcheck="true">#Load oxflower17 dataset</span><span class="token keyword">import</span> tflearn<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>oxflower17 <span class="token keyword">as</span> oxflower17<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitx<span class="token punctuation">,</span> y <span class="token operator">=</span> oxflower17<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>one_hot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Split train and test data</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Data augumentation with Keras tools</span><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>image <span class="token keyword">import</span> ImageDataGeneratorimg_gen <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>    rescale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">,</span>    shear_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    zoom_range<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>    horizontal_flip<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Define convolution with batchnromalization</span><span class="token keyword">def</span> <span class="token function">Conv2d_BN</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> nb_filter<span class="token punctuation">,</span>kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> name <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>        bn_name <span class="token operator">=</span> name <span class="token operator">+</span> <span class="token string">'_bn'</span>        conv_name <span class="token operator">=</span> name <span class="token operator">+</span> <span class="token string">'_conv'</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        bn_name <span class="token operator">=</span> None        conv_name <span class="token operator">=</span> None    x <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>nb_filter<span class="token punctuation">,</span>kernel_size<span class="token punctuation">,</span>padding<span class="token operator">=</span>padding<span class="token punctuation">,</span>strides<span class="token operator">=</span>strides<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">,</span>name<span class="token operator">=</span>conv_name<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>name<span class="token operator">=</span>bn_name<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span class="token comment" spellcheck="true">#Define Residual Block for ResNet34(2 convolution layers)</span><span class="token keyword">def</span> <span class="token function">Residual_Block</span><span class="token punctuation">(</span>input_model<span class="token punctuation">,</span>nb_filter<span class="token punctuation">,</span>kernel_size<span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> with_conv_shortcut <span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>input_model<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span>strides<span class="token operator">=</span>strides<span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>x<span class="token punctuation">,</span> nb_filter<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#need convolution on shortcut for add different channel</span>    <span class="token keyword">if</span> with_conv_shortcut<span class="token punctuation">:</span>        shortcut <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>input_model<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span>nb_filter<span class="token punctuation">,</span>strides<span class="token operator">=</span>strides<span class="token punctuation">,</span>kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">)</span>        x <span class="token operator">=</span> add<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span>shortcut<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">else</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> add<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span>input_model<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment" spellcheck="true">#Built ResNet34</span><span class="token keyword">def</span> <span class="token function">ResNet34</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> classes<span class="token punctuation">)</span><span class="token punctuation">:</span>    Img <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>width<span class="token punctuation">,</span>height<span class="token punctuation">,</span>depth<span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Conv2d_BN<span class="token punctuation">(</span>Img<span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">#Residual conv2_x ouput 56x56x64 </span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Residual conv3_x ouput 28x28x128 </span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>with_conv_shortcut<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># need do convolution to add different channel</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Residual conv4_x ouput 14x14x256</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>with_conv_shortcut<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># need do convolution to add different channel</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Residual conv5_x ouput 7x7x512</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>with_conv_shortcut<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> Residual_Block<span class="token punctuation">(</span>x<span class="token punctuation">,</span>nb_filter<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#Using AveragePooling replace flatten</span>    x <span class="token operator">=</span> GlobalAveragePooling2D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> Dense<span class="token punctuation">(</span>classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>    model<span class="token operator">=</span>Model<span class="token punctuation">(</span>input<span class="token operator">=</span>Img<span class="token punctuation">,</span>output<span class="token operator">=</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> model  <span class="token comment" spellcheck="true">#Plot Loss and accuracy</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'accuracy'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>History<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'model loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="八、Inception-v4-和-Inception-ResNet"><a href="#八、Inception-v4-和-Inception-ResNet" class="headerlink" title="八、Inception v4 和 Inception-ResNet"></a>八、Inception v4 和 Inception-ResNet</h1><p>2016年2月</p><p>Inception v4 和 Inception -ResNet 在同一篇论文<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</a>.首先说明一下Inception v4<strong>没有</strong>使用残差学习的思想, 而出自同一篇论文的Inception-Resnet-v1和Inception-Resnet-v2才是Inception module与残差学习的结合产物。Inception-ResNet和Inception v4网络结构都是基于Inception v3的改进。</p><p><strong>Inception v4中的三个基本模块</strong>：</p><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v1.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v2.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v3.png" width="180" height="240"></div><br><br><br><br><br><br><br><br><br><br><br><br><ol><li><p>左图是基本的Inception v2/v3模块，使用两个3x3卷积代替5x5卷积，并且使用average pooling，该模块主要处理尺寸为35x35的feature map；</p></li><li><p>中图模块使用1xn和nx1卷积代替nxn卷积，同样使用average pooling，该模块主要处理尺寸为17x17的feature map；</p></li><li><p>右图在原始的8x8处理模块上将3x3卷积用1x3卷积和3x1卷积。 </p></li></ol><p>总的来说，Inception v4中基本的Inception module还是沿袭了Inception v2/v3的结构，只是结构看起来更加简洁统一，并且使用更多的Inception module，实验效果也更好。</p><p>下图左图为Inception v4的网络结构，右图为Inception v4的Stem模块：</p><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v10.png" width="250" height="350"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v11.png" width="250" height="350"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><p><strong>Inception-Resnet-v1基本模块</strong>：</p><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v4.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v5.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v6.png" width="180" height="240"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><ol><li>Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息；</li><li>Inception module每个分支都没有使用pooling；</li><li>每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。</li></ol><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v12.png" width="250" height="350"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v13.png" width="250" height="350"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><p><strong>Inception-Resnet-v2基本模块：</strong>：</p><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v7.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v8.png" width="180" height="240"></div><div style="float:left;border:solid 1px 000;margin:2px;"><img src="v9.png" width="180" height="240"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>Inception-Resnet-v2网络结构同Inception-Resnet-v1，Stem模块同Inception v4<h1 id="九、Xception"><a href="#九、Xception" class="headerlink" title="九、Xception"></a>九、Xception</h1><p>2016年８月</p><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception</a>是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p><p><strong>结构的变形过程如下</strong>：</p><ul><li><p>在 Inception 中，特征可以通过 1×1卷积，3×3卷积，5×5 卷积，pooling 等进行提取，Inception 结构将特征类型的选择留给网络自己训练，也就是将一个输入同时输给几种提取特征方式，然后做 concat 。Inception-v3的结构图如下:</p><p><img src="x1.png" alt></p></li><li><p>对 Inception-v3 进行简化，去除 Inception-v3 中的 avg pool 后，输入的下一步操作就都是 1×1卷积：</p><p><img src="x2.png" alt></p></li><li><p>提取 1×1卷积的公共部分：</p><p><img src="x3.png" alt></p></li><li><p>Xception（<strong>极致的 Inception</strong>）：先进行普通卷积操作，再对 1×1卷积后的每个channel分别进行 3×3卷积操作，最后将结果 concat：</p><p><img src="x4.png" alt></p></li></ul><p><strong>深度可分离卷积 Depthwise Separable Convolution</strong></p><p>传统卷积的实现过程：</p><p><img src="x5.png" alt></p><p>Depthwise Separable Convolution 的实现过程：</p><p><img src="x6.png" alt></p><p><strong>Depthwise Separable Convolution 与 极致的 Inception 区别：</strong></p><p>极致的 Inception：</p><p>​    第一步：普通 1×1卷积。</p><p>​    第二步：对 1×1卷积结果的每个 channel，分别进行 3×3卷积操作，并将结果 concat。</p><p>Depthwise Separable Convolution：</p><p>​    第一步：Depthwise 卷积，对输入的每个channel，分别进行 3×3 卷积操作，并将结果 concat。</p><p>​    第二步：Pointwise 卷积，对 Depthwise 卷积中的 concat 结果，进行 1×1卷积操作。</p><p>两种操作的循序不一致：Inception 先进行 1×1卷积，再进行 3×3卷积；Depthwise Separable Convolution 先进行 3×3卷积，再进行1×1 卷积。</p><h1 id="十-、DenseNet"><a href="#十-、DenseNet" class="headerlink" title="十 、DenseNet"></a>十 、DenseNet</h1><p>2016年8yue</p><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">DenseNe</a>的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。</p><p><img src="d1.png" alt></p><p><strong>设计理念</strong></p><p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是<strong>每个层都会接受其前面所有层作为其额外的输入</strong>。ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过<strong>元素级</strong>相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 $L$层的网络，DenseNet共包含 $\frac{L(L+1)}{2}$个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p><p><strong>网络结构</strong></p><p>CNN网络一般要经过Pooling或者stride&gt;1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。</p><p><img src="d2.jpg" alt></p><p>在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数 $H$采用的是<strong>BN+ReLU+3x3 Conv</strong>的结构。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出个<strong>K</strong>特征图，即得到的特征图的channel数为$k$，或者说采用$k$个卷积核。$k$ 在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 $k$（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 $k_0$，那么$l$层输入的channel数为$k_0 + k(l-1)$，因此随着层数增加，尽管<strong>K</strong>设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有 <strong>K</strong> 个特征是自己独有的。</p><p><img src="d3.jpg" alt></p><p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加1x1 Conv，如图7所示，即<strong>BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv</strong>，称为DenseNet-B结构。其中1x1 Conv得到 $4k$个特征图它起到的作用是降低特征数量，从而提升计算效率。</p><p>对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为<strong>BN+ReLU+1x1 Conv+2x2 AvgPooling</strong>。另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为$m$，Transition层可以产生 个$[\theta m]$特征（通过卷积层），其中 $\theta \in (0, 1]$是压缩系数（compression rate）。当 $\theta =1$ 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C 。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>​    <a href="https://www.cnblogs.com/CZiFan/p/9490565.html" target="_blank" rel="noopener">https://www.cnblogs.com/CZiFan/p/9490565.html</a></p><p>​    <a href="https://www.zhihu.com/question/53727257/answer/136261195" target="_blank" rel="noopener">https://www.zhihu.com/question/53727257/answer/136261195</a></p><p>​    <a href="https://blog.csdn.net/lk3030/article/details/84847879" target="_blank" rel="noopener">https://blog.csdn.net/lk3030/article/details/84847879</a></p><p>​    <a href="https://blog.csdn.net/zzc15806/article/details/83504130" target="_blank" rel="noopener">https://blog.csdn.net/zzc15806/article/details/83504130</a></p><p>​    <a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37189203</a></p><p><a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-cnn演化史-alexnet-vgg-inception-resnet-keras-coding-668f74879306" target="_blank" rel="noopener">https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之超参数</title>
      <link href="/2019/12/30/shen-du-xue-xi-zhi-chao-can-shu/"/>
      <url>/2019/12/30/shen-du-xue-xi-zhi-chao-can-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="一、什么是超参数"><a href="#一、什么是超参数" class="headerlink" title="一、什么是超参数"></a>一、什么是超参数</h1><p>​    超参数是我们在将学习算法应用于数据集之前需要设置的变量。超参数的一个挑战在于,它不存在适用于所有地方的万能数字,每个任务和数据集的最佳数字各不相同。<br>​    一般来讲, 我们可以将超参数分为两类, 第一类是优化器超参数,它们是与优化和训练过程相关的变量,而非模型本身。这些包括学习率、minibatch大小以及训练迭代或 epoch 次数。 第二类是模型超参数。</p><h1 id="二、学习率"><a href="#二、学习率" class="headerlink" title="二、学习率"></a>二、学习率</h1><p>学习率是最重要的一个超参数, 即使你将他人构建的模型应用于 自己的数据集 你也会发现你可能需要尝试多个不同的学习率值才能使模型正确训练。</p><p>如果你归一化模型的输入，一个好的起始点通常是 0.01。这些是学习率的通常假设0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001。</p><p>学习率用于控制梯度下降的幅度，根据选取的学习率的不同，梯度下降的误差会呈现出不同的情况:</p><ul><li><p>如果我们选择的学习率小于理想的学习率,没关系,我们的模型将继续学习 直到找到权重的最佳值。但是,如果学习率太小,那么我们的训练误差就会降低的非常慢。很明显，在这种情况下我们需要做的是提高学习率 。</p></li><li><p>另一种情况是，如果我们选择一个大于理想学习率的学习率，更新的值将会越过理想的权重值。下一次更新时它会在反方向越过最佳值，也就是误差来回震荡,  但它会越来越靠近最佳值，可能最终会收敛到一个合理的值。</p></li><li><p>但是如果我们选择的学习率比理想学习率大很多，比如两倍以上，这就会产生问题。在这种情况下 我们会看到权重采取较大的步长 它不仅越过理想权值 而且实际上离我们每步获得的最佳误差越来越远。所以如果我们的训练误差在增加,不妨试试降低学习率，看看会发生什么 </p></li></ul><p>我们实际上无法保证误差曲线会是整洁的 U 形。事实上，它们会成为更复杂的形状。而且学习算法可能会错误地将局部最小值当做最佳值进行收敛 </p><p>下面我们来看一个在调整学习率时经常会遇到的一个具体情形，假设我们选择了一个合理的学习率，它可以降低误差但只能到某一个点，在那之后就无法下降了，尽管它还没到达底部，它会一直在两个值之间震荡，她们优于刚开始训练时的误差但却不是此模型的最佳值。在这种情况下，让我们的训练算法降低整个训练过程的学习率会比较有用，此技术叫作学习率衰减。这么做的直观方式是线性降低学习率，假设每5个epoch减半，也可以按指数方式降低学习率，例如 每8个epoch对学习率乘以0.1, 除了之间降低学习率外还有一些聪明的学习算法如自适应学习率，不仅在需要时降低学习率，还在学习率太低时升高它。</p><h1 id="三、mini-batch"><a href="#三、mini-batch" class="headerlink" title="三、mini-batch"></a>三、mini-batch</h1><p>​     一直以来人们都在争论哪种做法更好, 一种是在线随机训练, 在数据集中随机选择一条样本,然后仅用这一个样本进行前向传递,计算误差,然后反向传播并设置所有参数的调整数值，然后重复执行这个过程 。另一种是将整个数据集作为输入，使用数据集中所有示例的误差来计算整个数据集的梯度，这叫做批量训练 。</p><p>​    如今普遍使用的抽象是设置一个 mini-batch 大小，那么在线训练的mini-batch 大小就为 1，而批量训练的 mini-batch大小与训练集中的示例数量相同 。我们可以将 mini-batch 大小设置为1到数据集数量之间的任意值，32通常是一个不错的选择 。</p><p><img src="01.png" alt></p><p>​    较大的mini-batch可以更好的代表数据集整体的方向，会提高矩阵乘法的计算速度，但这也会占用更多的内存。较小的 mini-batch大小会使误差计算中有更多的噪声,但是此噪声通常有助于防止误差陷入局部最小值。</p><p><img src="02.png" alt></p><p>从上图中可以看出：</p><p>随着mini-batch的增大，训练一个epoch的时间越来越少</p><p>随着mini-batch的增大，达到同一准确率所花费的时间越来越多</p><p>一篇名为 “Systematic evaluation of CNN advances on the ImageNet” 的文章显示，在学习率相同的情况下，mini-batch越大，模型的准确度越低。这不仅在于 minibatch 大小的影响，而当我们改变批量大小时还需要改变学习率 。如果我们在增加批量大小的同时调整学习率，可以看到准确度会随批量大小增加而下降，不过只是轻微的下降 。<br>所以总结来说， 32 至 256是不错的初始值选择</p><h1 id="四、隐藏单元和层的数量"><a href="#四、隐藏单元和层的数量" class="headerlink" title="四、隐藏单元和层的数量"></a>四、隐藏单元和层的数量</h1><p>隐藏单元的数据量越多越好，但如果过多往往会导致过拟合。所以如果你的模型无法训练就向它添加更多隐藏层并跟踪验证误差，直到验证误差开始变大。</p><p> Andrej Karpathy 告诉我们在实践中三层神经网络的性能往往优于两层网络的性能，但继续增加层却作用不大 。不过，卷积神经网络除外，它们往往是越深性能越好。</p><h1 id="五、超参数的验证"><a href="#五、超参数的验证" class="headerlink" title="五、超参数的验证"></a>五、超参数的验证</h1><p>为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样话，可能就会得到不能拟合其他数据、泛化能力低的模型。 </p><p>因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。 </p><p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。</p><p><strong>超参数的最优化</strong></p><p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。  </p><p><strong>最优化的步骤</strong></p><p>步骤0, 设定超参数的范围。<br>步骤1, 从设定的超参数范围中随机采样。<br>步骤2, 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精<br>度（但是要将epoch设置得很小）。<br>步骤3, 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。 </p><p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。 </p><p>在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine Learning Algorithms” 等。 </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之过拟合</title>
      <link href="/2019/12/29/shen-du-xue-xi-zhi-guo-ni-he/"/>
      <url>/2019/12/29/shen-du-xue-xi-zhi-guo-ni-he/</url>
      
        <content type="html"><![CDATA[<h1 id="一、过拟合"><a href="#一、过拟合" class="headerlink" title="一、过拟合"></a>一、过拟合</h1><p>机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。</p><p>发生过拟合的原因，主要有以下两个：</p><ul><li>模型拥有大量参数、表现力强（模型太复杂）</li><li>训练数据少</li></ul><h1 id="二、如何减少过拟合"><a href="#二、如何减少过拟合" class="headerlink" title="二、如何减少过拟合"></a>二、如何减少过拟合</h1><h2 id="1、获取更多数据"><a href="#1、获取更多数据" class="headerlink" title="1、获取更多数据"></a>1、获取更多数据</h2><p>​    获取更多的数据，从源头上解决问题，比如数据增强</p><h2 id="2、正则化"><a href="#2、正则化" class="headerlink" title="2、正则化"></a>2、正则化</h2><p>​    该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p><p><strong>L1正则化</strong><br>$$<br>L_1(\theta) = loss(\theta) + \lambda\sum_{i=1}^n|w_i|<br>$$<br><strong>L2正则化（权值衰减）</strong><br>$$<br>L_2(\theta) = loss(\theta) + \lambda\sum_{i=1}^nw_i^2<br>$$</p><ul><li>L1减少的是一个常量，L2减少的是权重的固定比例</li><li>L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。</li><li>实践中L2正则化通常优于L1正则化</li></ul><h2 id="3、权重初始化"><a href="#3、权重初始化" class="headerlink" title="3、权重初始化"></a>3、权重初始化</h2><p>上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生。 </p><p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 </p><p>为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 </p><p>各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 </p><p>Xavier：如果前一层的节点数为n，则初始值使用标准差为$\frac{1}{\sqrt n}$的分布 </p><p>Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\frac{2}{\sqrt n}$的高斯分布。当Xavier初始值是$\frac{1}{\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 </p><h2 id="4、Batch-Normalization"><a href="#4、Batch-Normalization" class="headerlink" title="4、Batch Normalization"></a>4、Batch Normalization</h2><p>如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 </p><p>为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。</p><ul><li>可以使学习快速进行（可以增大学习率）。</li><li>不那么依赖初始值（对于初始值不用那么神经质）。</li><li>抑制过拟合（降低Dropout等的必要性）。 </li></ul><p><img src="02.png" alt></p><p>Batch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 </p><p>接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用<br>数学式可以如下表示。<br>$$<br>y_i \leftarrow \gamma x_i + \beta<br>$$<br>这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合<br>适的值。 </p><h2 id="5、Dropout"><a href="#5、Dropout" class="headerlink" title="5、Dropout"></a>5、Dropout</h2><p>​    如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。</p><p><img src="01.png" alt></p><p>​    Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机<br>选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递</p><h2 id="6、早期停止法"><a href="#6、早期停止法" class="headerlink" title="6、早期停止法"></a>6、早期停止法</h2><p>我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/skyfsm/p/8456968.html" target="_blank" rel="noopener">https://www.cnblogs.com/skyfsm/p/8456968.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之优化算法</title>
      <link href="/2019/11/28/shen-du-xue-xi-zhi-you-hua-suan-fa/"/>
      <url>/2019/11/28/shen-du-xue-xi-zhi-you-hua-suan-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="一、最优化"><a href="#一、最优化" class="headerlink" title="一、最优化"></a>一、最优化</h1><p>神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。常见的最优化方法有梯度下降法、牛顿法、拟牛顿法和共轭梯度法等。</p><h1 id="二、梯度下降法"><a href="#二、梯度下降法" class="headerlink" title="二、梯度下降法"></a>二、梯度下降法</h1><p>梯度下降法是使用最广泛的最优化方法，在目标函数是凸函数的时候可以得到全局解。</p><p>使用参数的梯度，沿梯度反方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为梯度下降法，也被称为“最速下降法”。</p><p>最速下降法越接近目标值的时候，需要步长越小，前进越慢，否则就会越过最优点。通常在机器学习优化任务中，有两种常用的梯度下降方法，分别是随机梯度下降法和批量梯度下降法。</p><p>所谓批量梯度下降（Batch gradient descent），就是使用所有的训练样本计算梯度，梯度计算稳定，可以求得全局最优解，但问题是计算非常慢，往往因为资源问题不可能实现。</p><p>所谓随机梯度下降（Stochastic gradient descent），就是每次只取一个样本进行梯度的计算，它的问题是梯度计算相对不稳定，容易震荡，不过整体上还是趋近于全局最优解的，所以最终的结果往往是在全局最优解附近。</p><p>通常我们训练的时候会进行折中，即从训练集中随机取一部分样本进行迭代，这就是常说的mini-batch训练了。</p><h1 id="三、牛顿法、拟牛顿法和共轭梯度法"><a href="#三、牛顿法、拟牛顿法和共轭梯度法" class="headerlink" title="三、牛顿法、拟牛顿法和共轭梯度法"></a>三、牛顿法、拟牛顿法和共轭梯度法</h1><p>梯度下降法是基于一阶的梯度进行优化的方法，牛顿法则是基于二阶梯度的方法，通常有更快的收敛速度。该算法利用局部一阶和二阶的偏导信息，推测整个函数的形状，进而求得近似函数的全局最小值，然后将当前最小值设定成近似函数的最小值。</p><p>不过牛顿法作为一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。当Hessian矩阵不可逆时无法计算，矩阵的逆计算复杂度为$n^2$，当问题规模比较大时，计算量很大。</p><p>拟牛顿法通过用正定矩阵来近似Hessian矩阵的逆，不需要二阶导数的信息，简化了运算的复杂度。</p><p>共轭梯度法是一种通过迭代下降的共轭方向来避免Hessian矩阵求逆计算的方法，介于最速下降法与牛顿法之间。</p><h1 id="四、SGD"><a href="#四、SGD" class="headerlink" title="四、SGD"></a>四、SGD</h1><p>深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.</p><p>数学公式<br>$$<br>W \leftarrow W - \eta \frac {\partial L}{\partial W}<br>$$</p><p><strong>代码实现</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SGD</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""随机梯度下降法（Stochastic Gradient Descent）"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        更新权重        :param params: 权重, 字典，params['W1'], ..        :param grads: 梯度, 字典, grads['w1']        :return:        """</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>SGD的缺点</strong></p><ol><li>选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.</li><li>SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.</li></ol><h1 id="四、Momentum"><a href="#四、Momentum" class="headerlink" title="四、Momentum"></a>四、Momentum</h1><p>为解决随机梯度下降的问题，在1964年，Polyak提出了动量项（Momentum）方法。动量算法积累了之前梯度的<strong>指数加权平均</strong>，并且继续沿该方向移动，将前几次的梯度计算量加进来一起进行运算。<br>为了表示动量，首先引入一个新的变量v（velocity），v是之前梯度计算量的累加，但是每回合都有一定的衰减。动量法的思想就是将历史步长更新向量的一个分量$\alpha$，增加到当前的更新向量中，其具体实现为在每次迭代过程中，计算梯度和误差，更新速度v和参数$W$:<br>$$<br>\upsilon_{t+1} \leftarrow \alpha \upsilon_{t} - \eta \frac {\partial L}{\partial W_{t}}<br>$$</p><p>$$<br>W_{t+1} \leftarrow W_{t} + \upsilon_{t+1}<br>$$</p><p>原论文:<a href="http://www.cs.toronto.edu/~hinton/absps/momentum.pdf" target="_blank" rel="noopener">http://www.cs.toronto.edu/~hinton/absps/momentum.pdf</a></p><p>如果前一刻的梯度与当前的梯度方向几乎相反，则因为受到前一时刻的影响，当前时刻梯度幅度会减小，反之则会增强。</p><p>形象地说，动量法就像我们从山上推下一个球，在滚落过程中累积动量，速度越来越快，直到达到终极速度。在梯度更新过程中，对在梯度点处具有相同方向的维度，增大其动量项，对于在梯度点处改变方向的维度，减小其动量项。</p><p><strong>代码实现</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Momentum</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Momentum SGD"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr        self<span class="token punctuation">.</span>momentum <span class="token operator">=</span> momentum        self<span class="token punctuation">.</span>v <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>v <span class="token keyword">is</span> None<span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 初始化v</span>            self<span class="token punctuation">.</span>v <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            <span class="token keyword">for</span> key<span class="token punctuation">,</span> val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                                                self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>momentum<span class="token operator">*</span>self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>lr<span class="token operator">*</span>grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>             params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="sgd.png" alt><img src="mom.png" alt></p><p>​    和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。</p><p>总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。</p><h1 id="五、Nesterov"><a href="#五、Nesterov" class="headerlink" title="五、Nesterov"></a>五、Nesterov</h1><p>Nesterov加速梯度下降法（Nesterov Accelerated Gradient，NAG）是动量算法的一个变种，同样是一阶优化算法，但在梯度评估方面有所不同。NAG能给动量项一个预知的能力，并且收敛速度更快。其更新算法如下：<br>$$<br>\upsilon_{t+1} \leftarrow \alpha \upsilon_{t} - \eta \frac {\partial L(W_t + \alpha \upsilon_t)}{\partial W_{t}}<br>$$</p><p>$$<br>W_{t+1} \leftarrow W_{t} + \upsilon_{t+1}<br>$$</p><p>我们利用动量项$\alpha v_t$更新参数$w_t$，通过计算$(W_t + \alpha v_t)$得到参数未来位置的一个近似值，计算关于参数未来的近似位置的梯度，而不是关于当前参数$W_t$的梯度，这样NAG算法可以高效地求解。<br>我们可以将其抽象为球滚落的时候，一般是盲目地沿着某个斜率方向，结果并不一定能令人满意。于是我们希望有一个较为“智能”的球，能够自己判断下落的方向，这样在途中遇到斜率上升的时候能够知道减速，该思想对于RNN性能的提升有重要的意义。</p><h1 id="六、Adagrad"><a href="#六、Adagrad" class="headerlink" title="六、Adagrad"></a>六、Adagrad</h1><p>在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。</p><p>在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。</p><p>和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。</p><p><strong>数学公式</strong><br>$$<br>h \leftarrow h + \frac {\partial L}{\partial W} \cdot \frac {\partial L}{\partial W}<br>$$</p><p>$$<br>W \leftarrow W -  \frac {\eta}{\sqrt h}\cdot\frac {\partial L}{\partial W}<br>$$</p><p>在更新参数时，通过乘以$\frac {1}{\sqrt h}$ ，就可以调整学习的尺度</p><p>前期$g_t$较小的时候，正则化项$\frac{1}{\sqrt h}$较大，能够放大梯度；后期$g_t$较大的时候，正则化项较小，能够约束梯度，适合处理稀疏梯度。</p><p>但Adagrad算法同样依赖于人工设置一个全局学习率η，设置过大的话，会使正则化项过于敏感，对梯度的调节过大。</p><p><strong>代码实现</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AdaGrad</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""AdaGrad"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr        self<span class="token punctuation">.</span>h <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>h <span class="token keyword">is</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            <span class="token keyword">for</span> key<span class="token punctuation">,</span> val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​    <img src="adagrad.png" alt></p><p>​                                <strong>基于Adagrad的最优化的更新路径</strong></p><p>​    由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。</p><p><strong>特点</strong></p><ol><li>前期放大梯度，加速学习，后期约束梯度</li><li>适合处理稀疏梯度</li></ol><p><strong>缺点</strong></p><p>​    中后期，分母上梯度的平方的积累将会越来越大，使gradient–&gt;0, 使得训练提前结束。</p><h1 id="七、Adadelta"><a href="#七、Adadelta" class="headerlink" title="七、Adadelta"></a>七、Adadelta</h1><p>Adadelta算法的出现可较好地解决全局学习率问题，其本质是对Adagrad算法的扩展，同样是对学习率进行自适应约束，但是计算上进行了简化。Adagrad算法会累加之前所有的梯度平方，而Adadelta算法只累加固定大小的项，并且仅存储这些项近似计算对应的平均值。其公式如下：<br>$$<br>h_t \leftarrow \alpha h_{t-1} + {(1-\alpha)}\frac {\partial L}{\partial W_t} \cdot \frac {\partial L}{\partial W_t}<br>$$</p><p>$$<br>\Delta x_t = \frac {\sqrt{\sum_{k=1}^{t-1}\Delta x_k}}{\sqrt h_t}\frac {\partial L}{\partial W}<br>$$</p><p>$$<br>W \leftarrow W -  \Delta x_t<br>$$</p><p>Adadelta不依赖于全局学习率，训练初、中期，加速效果理想化，训练后期，反复在局部最小值附近抖动。</p><p>原论文: <a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">https://arxiv.org/abs/1212.5701</a></p><h1 id="八、RMSProp"><a href="#八、RMSProp" class="headerlink" title="八、RMSProp"></a>八、RMSProp</h1><p>RMSProp可以算作Adadelta的一个特例，依然依赖于全局学习率。RMSProp效果趋于Adagrad和Adadelta之间，适合处理非平稳目标，适用于RNN的优化。</p><p><strong>数学公式</strong><br>$$<br>h \leftarrow \alpha h + (1-\alpha)\frac {\partial L}{\partial W} \cdot \frac {\partial L}{\partial W}<br>$$</p><p>$$<br>W \leftarrow W - \eta \frac {1}{\sqrt h}\frac {\partial L}{\partial W}<br>$$</p><p><strong>代码实现</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RMSprop</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""RMSprop"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> decay_rate <span class="token operator">=</span> <span class="token number">0.99</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr        self<span class="token punctuation">.</span>decay_rate <span class="token operator">=</span> decay_rate        self<span class="token punctuation">.</span>h <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>h <span class="token keyword">is</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            <span class="token keyword">for</span> key<span class="token punctuation">,</span> val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">*=</span> self<span class="token punctuation">.</span>decay_rate            self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>decay_rate<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="九、Adam"><a href="#九、Adam" class="headerlink" title="九、Adam"></a>九、Adam</h1><p>​    Adam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。其公式如下：<br>$$<br>m_t \leftarrow \beta_{1} m_{t-1} + (1-\beta_1)\frac {\partial L}{\partial W}<br>$$</p><p>$$<br>v_t \leftarrow \beta_{2} v_{t-1} + (1-\beta_2)\frac {\partial L}{\partial W} \cdot \frac {\partial L}{\partial W}<br>$$</p><p>$$<br>\hat m_t = \frac {m_t}{(1-\beta_1)}<br>$$</p><p>$$<br>\hat v_t = \frac {v_t}{(1-\beta_2)}<br>$$</p><p>$$<br>W_t = W_{t-1} - \alpha \frac{\hat m_t}{\sqrt{\hat v_t}}<br>$$</p><p>​    Adam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\beta_1$和二次momentum系数$\beta_2$。根据论文，标准的设定值是$\beta_1$为0.9， $\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。</p><p>其中，$m_t$和$v_t$分别是对梯度的一阶和二阶矩估计，即对期望$E|g_t|$和$E|g_t^2|$的估计, $\hat m_t$、$\hat v_t$是对$m_t$和$v_t$的校正，近似为对期望的无偏估计。由公式可得，Adam算法可以根据梯度进行动态调整，对学习率有一个动态约束。<br>Adam算法的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定的范围，使参数比较平稳；对内存需求较小，适用于大多数非凸优化问题，尤其适合处理大数据集和高维空间问题。</p><p>原文： <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6980</a></p><h1 id="十、Nadam"><a href="#十、Nadam" class="headerlink" title="十、Nadam"></a>十、Nadam</h1><p>Nadam类似于带有Nesterov动量项的Adam。对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，使用带动量的RMSprop或者Adam的情况下，大多可以使用Nadam取得更好的效果。</p><h1 id="十一、不同算法比较"><a href="#十一、不同算法比较" class="headerlink" title="十一、不同算法比较"></a>十一、不同算法比较</h1><p>​    <img src="v1.webp" alt></p><p><img src="v2.webp" alt></p><ol><li>如果数据是稀疏的，就用自适应算法, 即Adagrad, Adadelta, RMSProp, Adam</li><li>RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。</li><li>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</li><li>SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</li></ol><h1 id="十二、参考"><a href="#十二、参考" class="headerlink" title="十二、参考"></a>十二、参考</h1><p>《深度学习入门: 基于Python的理论与实现》</p><p>《深度学习之图像识别：核心技术与案例实战》</p><p><a href="https://blog.csdn.net/qq_28031525/article/details/79535942" target="_blank" rel="noopener">https://blog.csdn.net/qq_28031525/article/details/79535942</a></p><p><a href="https://www.cnblogs.com/guoyaohua/p/8542554.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8542554.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python中with的用法</title>
      <link href="/2019/11/28/python-zhong-with-de-yong-fa/"/>
      <url>/2019/11/28/python-zhong-with-de-yong-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="一、什么是with语句"><a href="#一、什么是with语句" class="headerlink" title="一、什么是with语句"></a>一、什么是with语句</h1><p>​    对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 操作文件的方式</span><span class="token comment" spellcheck="true"># 普通方式操作文件</span>f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 普通方式操作文件的问题</span><span class="token comment" spellcheck="true"># 1、忘记关闭文件</span><span class="token comment" spellcheck="true"># 2、程序执行的过程中发生了异常导致关闭文件的代码没有被执行</span><span class="token comment" spellcheck="true"># 可以使用try...finally的方式解决上述问题</span><span class="token keyword">try</span><span class="token punctuation">:</span>    f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test'</span><span class="token punctuation">)</span><span class="token keyword">finally</span><span class="token punctuation">:</span>    f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Python提供了一种更简单的解决方案:with语句</span><span class="token comment" spellcheck="true"># 不论执行过程中是否发生异常，文件都会关闭</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>with 语句的语法格式</strong>:</p><pre class="line-numbers language-shell"><code class="language-shell">with 表达式 [as目标]:        代码块        <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​                                </p><h1 id="二、with语句的原理"><a href="#二、with语句的原理" class="headerlink" title="二、with语句的原理"></a>二、with语句的原理</h1><ol><li><p>上下文管理器</p><p>上下文管理器是一个实现了上下文协议的对象，即在对象中定义了<code>__enter__</code>和<code>__exit__</code>方法。上下文管理器定义运行时需要建立的上下文，处理程序的进入和退出。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># open函数返回的就是一个上下文管理器对象</span>f<span class="token punctuation">.</span>__enter__   <span class="token comment" spellcheck="true"># &lt;function TextIOWrapper.__enter__></span>f<span class="token punctuation">.</span>__exit__    <span class="token comment" spellcheck="true"># &lt;function TextIOWrapper.__exit__></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><code>__enter__(self)</code>:该方法只接收一个self参数。当对象返回时该方法立即执行，并返回当前对象或者与运行时上下文相关的其他对象。如果有as变量（as子句是可选项），返回值将被赋给as后面的变量上。</p><p><code>__exit__(self, exception_type, exception_value, traceback)</code>:退出运行时上下文，并返回一个布尔值标示是否有需要处理的异常。如果在执行with语句体时发生异常，那退出时参数会包括异常类型、异常值、异常追踪信息，否则，3个参数都是None。返回True异常被处理，返回其他任何值，异常都会抛出。</p></li></ol><ol start="2"><li><p>自定义上下文管理器</p><p>任何实现了上下文管理器协议的对象都可以称为一个上下文管理器。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 实现一个简单的open()上下文管理</span><span class="token keyword">class</span> <span class="token class-name">myopen</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>file <span class="token operator">=</span> open<span class="token punctuation">(</span>name<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>              <span class="token keyword">def</span> <span class="token function">__enter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>file    <span class="token keyword">def</span> <span class="token function">__exit__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> exception_type<span class="token punctuation">,</span> exception_value<span class="token punctuation">,</span> traceback<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">with</span> myopen<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># test</span>f<span class="token punctuation">.</span>closed  <span class="token comment" spellcheck="true"># True</span><span class="token comment" spellcheck="true"># 同时打开多个文件</span><span class="token keyword">with</span> myopen<span class="token punctuation">(</span><span class="token string">'test1.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f1<span class="token punctuation">,</span> myopen<span class="token punctuation">(</span><span class="token string">'test2.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f2<span class="token punctuation">,</span> myopen<span class="token punctuation">(</span><span class="token string">'test3.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f3<span class="token punctuation">:</span>    f1<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test1'</span><span class="token punctuation">)</span>    f2<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test2'</span><span class="token punctuation">)</span>    f3<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'test3'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h1 id="三、contextlib"><a href="#三、contextlib" class="headerlink" title="三、contextlib"></a>三、contextlib</h1><p>​    为了更好的辅助上下文管理，python提供了<code>contextlib</code>模块，该模块通过<code>Generator</code>实现，其中的<code>contextmanager</code>作为装饰器来提供了一种针对函数级别的上下文管理机制，可以直接使用与函数/对象而不用关心<code>__enter__</code>和<code>__exit__</code>方法的具体实现。</p><ol><li><p><code>contextmanager</code></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> contextlib <span class="token keyword">import</span> contextmanager@contextmanager<span class="token keyword">def</span> <span class="token function">myopen</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        f <span class="token operator">=</span> open<span class="token punctuation">(</span>name<span class="token punctuation">,</span> model<span class="token punctuation">)</span>        <span class="token keyword">yield</span> f    <span class="token keyword">finally</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> myopen<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># test</span>f<span class="token punctuation">.</span>closed  <span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><ol start="2"><li><p><code>closing</code></p><p>文件类是支持上下文管理协议的，可以直接用with语句，还有一些对象并不支持该协议，但使用的时候又要确保正常退出，这时就可以使用closing创建上下文管理器。</p></li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> urllib <span class="token keyword">import</span> request<span class="token keyword">from</span> contextlib <span class="token keyword">import</span> closing<span class="token punctuation">,</span> contextmanager<span class="token keyword">with</span> closing<span class="token punctuation">(</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">'https://www.baidu.com/'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Status:'</span><span class="token punctuation">,</span> f<span class="token punctuation">.</span>status<span class="token punctuation">,</span> f<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Data:'</span><span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 等价于:</span>@contextmanager<span class="token keyword">def</span> <span class="token function">closing</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> f    <span class="token keyword">finally</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python包管理</title>
      <link href="/2019/11/28/python-bao-guan-li/"/>
      <url>/2019/11/28/python-bao-guan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="一、什么是包？"><a href="#一、什么是包？" class="headerlink" title="一、什么是包？"></a>一、什么是包？</h1><p>简单来说包即是目录，但和普通目录不同，它除了包含python文件以外，还包含一个<code>__init__.py</code>文件，同时它允许嵌套。</p><p>包结构如下：</p><pre class="line-numbers language-python"><code class="language-python">package<span class="token operator">/</span>__init__<span class="token punctuation">.</span>py        module1<span class="token punctuation">.</span>py            <span class="token keyword">class</span> <span class="token class-name">C1</span><span class="token punctuation">:</span><span class="token keyword">pass</span>        module2<span class="token punctuation">.</span>py            <span class="token keyword">class</span> <span class="token class-name">C2</span><span class="token punctuation">:</span><span class="token keyword">pass</span>        subpackage<span class="token operator">/</span>__init__<span class="token punctuation">.</span>py                   module1<span class="token punctuation">.</span>py                                          module2<span class="token punctuation">.</span>py                                     module3<span class="token punctuation">.</span>pymain<span class="token punctuation">.</span>py<span class="token keyword">import</span> package<span class="token keyword">import</span> package<span class="token punctuation">.</span>module1<span class="token keyword">import</span> package<span class="token punctuation">.</span>subpackage<span class="token keyword">import</span> package<span class="token punctuation">.</span>subpackage<span class="token punctuation">.</span>module1<span class="token keyword">from</span> package <span class="token keyword">import</span> module1<span class="token keyword">from</span> package <span class="token keyword">import</span> subpackage<span class="token keyword">from</span> package<span class="token punctuation">.</span>subpackage <span class="token keyword">import</span> module1<span class="token comment" spellcheck="true"># from package import module3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="二、-init-py的作用"><a href="#二、-init-py的作用" class="headerlink" title="二、__init__.py的作用"></a>二、<code>__init__.py</code>的作用</h1><ol><li><p>区别包和普通目录</p></li><li><p>可以使模块中的对象变成包可见</p><p>例如：要导入package包下module1中的类Test, 当<code>__init__.py</code>文件为空的时候需要使用完整的导入路径:<code>from package.module import Test</code>, 但如果在<code>__init__.py</code>中添加<code>from module1 import Test</code>语句，就可以直接使用<code>from package import Test</code>来导入类Test。</p></li><li><p>通过在该文件中定义<code>__all__</code>变量，控制需要导入的子包或模块。</p></li></ol><h1 id="三、-all-的作用"><a href="#三、-all-的作用" class="headerlink" title="三、__all__的作用"></a>三、<code>__all__</code>的作用</h1><p>​    <code>__all__</code>只控制<code>from xxx import *</code>的行为, 不影响<code>import</code> 和 <code>from xxx import xxxx</code>的行为</p><ol><li><p>在<code>__init__.py</code>文件中添加：</p><p>​                    <code>__all__ = [&#39;module1&#39;, &#39;subpackage&#39;]</code></p><p><code>__init__.py</code>不使用<code>__all__</code>属性，不能通过<code>from package import *</code>导入</p><p><code>__init__.py</code>使用<code>__all__</code>属性，<code>from package import *</code>只能导入<code>__all__</code>列表中的成员，但可以通过</p><p><code>import package.module2</code>和<code>from package import module2</code>导入</p></li><li><p>在普通<code>*.py</code>文件中添加：<code>__all__</code></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># from xxx import * 这种方式只能导入公有的属性，方法或类【无法导入以单下划线开头（protected）或以双下划线开头(private)的属性，方法或类】</span><span class="token comment" spellcheck="true"># from xxx import aa, bb 可以导入public,protected,private</span><span class="token comment" spellcheck="true"># import xxx   xxx.__func  可以访问public,protected,private</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>模块中不使用<code>__all__</code>属性，可以导入模块内的所有公有属性，函数和类 。</p><p>模块中使用<code>__all__</code>属性，只能导入<code>__all__</code>中定义的属性，函数和类(包括私有属性和保护属性)。</p></li></ol><h1 id="四、from-import-的问题"><a href="#四、from-import-的问题" class="headerlink" title="四、from ... import ...的问题"></a>四、<code>from ... import ...</code>的问题</h1><ol><li><p>命名空间的冲突</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># module1.py</span><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"add in module1"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># module1.py</span><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"add in module2"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># main.py</span><span class="token keyword">from</span> package<span class="token punctuation">.</span>module1 <span class="token keyword">import</span> add<span class="token keyword">from</span> package<span class="token punctuation">.</span>module2 <span class="token keyword">import</span> add<span class="token comment" spellcheck="true"># 最近导入的add,会覆盖先导入的add</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><ol start="2"><li><p>循环嵌套导入的问题</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># module1.py</span><span class="token keyword">from</span> module2 <span class="token keyword">import</span> g<span class="token keyword">def</span> <span class="token function">x</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">pass</span><span class="token comment" spellcheck="true"># module2.py</span><span class="token keyword">from</span> module1 <span class="token keyword">import</span> x<span class="token keyword">def</span> <span class="token function">g</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">pass</span><span class="token comment" spellcheck="true"># 会抛出一个ImportError: cannot import name 'g'异常，解决方法直接使用import 语句</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyhton高数计算库</title>
      <link href="/2019/11/25/pyhton-gao-shu-ji-suan-ku/"/>
      <url>/2019/11/25/pyhton-gao-shu-ji-suan-ku/</url>
      
        <content type="html"><![CDATA[<h1 id="math数学库"><a href="#math数学库" class="headerlink" title="math数学库"></a>math数学库</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入math库</span><span class="token keyword">import</span> math<span class="token comment" spellcheck="true"># 常用数学常量</span>math<span class="token punctuation">.</span>pi        <span class="token comment" spellcheck="true"># π</span>math<span class="token punctuation">.</span>emath<span class="token punctuation">.</span>inf    <span class="token comment" spellcheck="true"># ∞</span>math<span class="token punctuation">.</span>nan    <span class="token comment" spellcheck="true"># not a num</span><span class="token comment" spellcheck="true"># 指数/对数/开平方</span>math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># math.e**a</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 自然底数 math.e</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 以b为底，b**x = a</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>a<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 开平方</span><span class="token comment" spellcheck="true"># 近似值</span>math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token number">4.1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># roud up to 5</span>math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token number">4.9</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># roud up to 4</span><span class="token comment" spellcheck="true"># 阶乘</span>math<span class="token punctuation">.</span>factorial<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># a!</span><span class="token comment" spellcheck="true"># 最大公约数</span>math<span class="token punctuation">.</span>gcd<span class="token punctuation">(</span><span class="token number">35</span><span class="token punctuation">,</span> <span class="token number">49</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 7</span><span class="token comment" spellcheck="true"># 三角函数</span>math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>math<span class="token punctuation">.</span>pi<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 1.0</span>math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span>math<span class="token punctuation">.</span>tan<span class="token punctuation">(</span><span class="token punctuation">)</span>math<span class="token punctuation">.</span>asin<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 1.5707963267948966</span>math<span class="token punctuation">.</span>acos<span class="token punctuation">(</span><span class="token punctuation">)</span>math<span class="token punctuation">.</span>atan<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 弧度角度转换</span>math<span class="token punctuation">.</span>degrees<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 弧度转角度</span>math<span class="token punctuation">.</span>radians<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 角度转弧度</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="sympy代数运算库"><a href="#sympy代数运算库" class="headerlink" title="sympy代数运算库"></a>sympy代数运算库</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入库</span><span class="token keyword">from</span> sympy <span class="token keyword">import</span> <span class="token operator">*</span><span class="token comment" spellcheck="true"># 有理数</span>Rational<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 1/3</span><span class="token comment" spellcheck="true"># 特殊无理数</span>pi    <span class="token comment" spellcheck="true"># math.pi</span>E    <span class="token comment" spellcheck="true"># math.e</span>oo    <span class="token comment" spellcheck="true"># math.inf</span><span class="token comment" spellcheck="true"># jupyter pretty print</span>init_printing<span class="token punctuation">(</span>pretty_print<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Pretty printing mode</span>N<span class="token punctuation">(</span>pi<span class="token punctuation">)</span> <span class="token operator">=</span> pi<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 3.15..默认取前15位</span><span class="token comment" spellcheck="true"># .n() and N() are equivalent to .evalf();</span><span class="token comment" spellcheck="true"># 代数运算 用符号代替数进行运算</span>x <span class="token operator">=</span> Symbol<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 声明一个代数符号</span>x<span class="token punctuation">,</span>y <span class="token operator">=</span> symbols<span class="token punctuation">(</span><span class="token string">'x y'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 一次声明的多个代数符号</span><span class="token punctuation">(</span>x<span class="token operator">+</span>y<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>  <span class="token comment" spellcheck="true"># (𝑥+𝑦)2</span><span class="token comment" spellcheck="true"># 展开和分解</span><span class="token comment" spellcheck="true"># 展开多项式</span>expand<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">+</span>y<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 𝑥2+2𝑥𝑦+𝑦2</span><span class="token comment" spellcheck="true"># 展开三角函数</span>expand<span class="token punctuation">(</span>cos<span class="token punctuation">(</span>x<span class="token operator">+</span>y<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> trig<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># sin2(𝑥)sin2(𝑦)−2sin(𝑥)sin(𝑦)cos(𝑥)cos(𝑦)+cos2(𝑥)cos2(𝑦)</span><span class="token comment" spellcheck="true"># 化简</span>simplify<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">+</span>x<span class="token operator">*</span>y<span class="token punctuation">)</span><span class="token operator">/</span>x<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 1+y</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="累加运算"><a href="#累加运算" class="headerlink" title="累加运算"></a>累加运算</h2><p>$$<br>\sum_{x=1}^{10} {\frac {1}{x^2 + 2x}}<br>$$</p><pre class="line-numbers language-python"><code class="language-python">expr <span class="token operator">=</span> Sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>expr <span class="token comment" spellcheck="true"># 上面公式</span>expr<span class="token punctuation">.</span>evalf<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 求值 0.662878787878788</span>expr<span class="token punctuation">.</span>doit<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 175/264</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="累积运算"><a href="#累积运算" class="headerlink" title="累积运算"></a>累积运算</h2><p>$$<br>\prod_{x=1}^{10} {\frac {1}{x^2 + 2x}}<br>$$</p><pre class="line-numbers language-python"><code class="language-python">expr <span class="token operator">=</span> Product<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>exprexpr<span class="token punctuation">.</span>doit<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 1/869100503040000</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h2><p>$$<br>\lim_{n \to +\infty} \frac{1}{n(n+1)} \quad<br>$$</p><pre class="line-numbers language-python"><code class="language-python">n <span class="token operator">=</span> Symbol<span class="token punctuation">(</span><span class="token string">'n'</span><span class="token punctuation">)</span>expr <span class="token operator">=</span> limit<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span>n<span class="token operator">*</span><span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> oo<span class="token punctuation">)</span>expr    <span class="token comment" spellcheck="true"># 0</span><span class="token comment" spellcheck="true"># 左极限和有极限</span>limit<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> dir<span class="token operator">=</span><span class="token string">'+'</span><span class="token punctuation">)</span>limit<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> dir<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><pre class="line-numbers language-python"><code class="language-python">diff<span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 2x</span>diff<span class="token punctuation">(</span>sin<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 2cos(2𝑥)</span>diff<span class="token punctuation">(</span>sin<span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># diff(E**x*(x + sin(x)), x)</span><span class="token comment" spellcheck="true"># 高阶导数</span><span class="token comment" spellcheck="true"># 二阶导数</span>diff<span class="token punctuation">(</span>sin<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># −4sin(2𝑥)</span><span class="token comment" spellcheck="true"># 三阶导数</span>diff<span class="token punctuation">(</span>sin<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># −8cos(2𝑥)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="积分"><a href="#积分" class="headerlink" title="积分"></a>积分</h2><p>不指定区间<br>$$<br>\int_{-\infty}^\infty {x^2} \,{\rm dx}<br>$$</p><pre class="line-numbers language-python"><code class="language-python">integrate<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 𝑥2</span>integrate<span class="token punctuation">(</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># −cos(𝑥)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>指定区间[a, b]<br>$$<br>\int_a^b {x^2} \,{\rm dx}<br>$$</p><pre class="line-numbers language-python"><code class="language-python">integrate<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 1</span>integrate<span class="token punctuation">(</span>cos<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">-</span>pi<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> pi<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="解方程"><a href="#解方程" class="headerlink" title="解方程"></a>解方程</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 解一元方程</span>solve<span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token operator">-</span><span class="token number">3</span><span class="token operator">*</span>x<span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># [1, 2]</span><span class="token comment" spellcheck="true"># 解二元方程</span>solve<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token operator">+</span><span class="token number">5</span><span class="token operator">*</span>y<span class="token number">-2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token operator">*</span>x<span class="token operator">+</span><span class="token number">6</span><span class="token operator">*</span>y<span class="token number">-15</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>x<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#{x:-3, y:1}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="代数运算"><a href="#代数运算" class="headerlink" title="代数运算"></a>代数运算</h2><pre class="line-numbers language-python"><code class="language-python">expr <span class="token operator">=</span> x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>x <span class="token operator">+</span> <span class="token number">1</span><span class="token comment" spellcheck="true"># 令x = 2</span>expr<span class="token punctuation">.</span>subs<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 9b</span><span class="token comment" spellcheck="true"># 令x=y+1</span>expr<span class="token punctuation">.</span>subs<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 2𝑦+(𝑦+1)2+3</span><span class="token comment" spellcheck="true"># 多元函数代数</span>expr <span class="token operator">=</span> x<span class="token operator">**</span><span class="token number">3</span> <span class="token operator">+</span> <span class="token number">4</span><span class="token operator">*</span>x<span class="token operator">*</span>y <span class="token operator">-</span>zexpr<span class="token punctuation">.</span>subs<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>z<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 5</span><span class="token comment" spellcheck="true"># 使用字符串</span>expr <span class="token operator">=</span> sympify<span class="token punctuation">(</span><span class="token string">"x*2 + 4*x*y"</span><span class="token punctuation">)</span>expr<span class="token punctuation">.</span>subs<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 6</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sympy <span class="token keyword">import</span> stats<span class="token comment" spellcheck="true">#创建一个6个面的筛子</span>X <span class="token operator">=</span> stats<span class="token punctuation">.</span>Die<span class="token punctuation">(</span><span class="token string">'X'</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 查看某个面出现的概率</span>stats<span class="token punctuation">.</span>density<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>dict    <span class="token comment" spellcheck="true"># {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}</span><span class="token comment" spellcheck="true"># 随机丢一次筛子</span>stats<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 4</span><span class="token comment" spellcheck="true">#     硬币</span>C <span class="token operator">=</span> stats<span class="token punctuation">.</span>Coin<span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">)</span>    stats<span class="token punctuation">.</span>density<span class="token punctuation">(</span>C<span class="token punctuation">)</span><span class="token punctuation">.</span>dict    <span class="token comment" spellcheck="true"># {H: 1/2, T: 1/2}</span><span class="token comment" spellcheck="true"># 正态分布</span>Z <span class="token operator">=</span> stats<span class="token punctuation">.</span>Normal<span class="token punctuation">(</span><span class="token string">'Z'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Z>1的概率</span>stats<span class="token punctuation">.</span>P<span class="token punctuation">(</span>Z <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>evalf<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 0.158655253931457</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> math </tag>
            
            <tag> sympy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络简介</title>
      <link href="/2019/11/25/shen-jing-wang-luo-jian-jie/"/>
      <url>/2019/11/25/shen-jing-wang-luo-jian-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="一、感知器"><a href="#一、感知器" class="headerlink" title="一、感知器"></a>一、感知器</h1><p>​    感知器(perceptron)是由美国学者FrankRoseblatt在1957年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。</p><h2 id="1-什么是感知器"><a href="#1-什么是感知器" class="headerlink" title="1.什么是感知器"></a>1.什么是感知器</h2><p>​    感知机接收多个输入，生成一个输出，输出只有两种1和0。</p><p>​    <img src="p1.png" alt></p><p>​                                    <strong>图1.1 有两个输入的感知机</strong></p><p>图1-1是一个接收两个输入的感知机. $x_1$、$x_2$是输入，$y$是输出，$w_1$、$w_2$是权重。图中的○称为神经元或者节点。输入被送往神经元时，会被分别乘以固定的权重$(w_1x_1,  w_2x_2)$。神经元会计算传送过来的输入的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。<br>$$<br>y =\begin{cases}1, &amp; (w_1x_1 + w_2x_2) &gt; \theta \\0, &amp; (w_1x_1 + w_2x_2) \leq \theta\end{cases}<br>$$<br>​        <strong>权重越大，对应该权重的信号的重要性就越高。</strong></p><h2 id="2-逻辑运算"><a href="#2-逻辑运算" class="headerlink" title="2.逻辑运算"></a>2.逻辑运算</h2><p>使用感知器可以解决简单的逻辑运算，与门（AND）, 与非门（NOT AND）, 或门（OR）.</p><p>​    <img src="p2.png" alt></p><p>​                                        <strong>1-2 与门真值表</strong></p><p>满足图2-2的条件的参数的选择方法有无数多个。比如，当$(w_1, w_2, θ)$ = $(0.5, 0.5, 0.7)$ ​时，可以满足图 2-1的条件。</p><p>我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。</p><h2 id="3-偏置和权重"><a href="#3-偏置和权重" class="headerlink" title="3.偏置和权重"></a>3.偏置和权重</h2><p>$$<br>y =\begin{cases}1, &amp; (b + w_1x_1 + w_2x_2) &gt; 0 \\0, &amp; (b + w_1x_1 + w_2x_2) \leq 0\end{cases}<br>$$</p><p>令$b = -\theta$， $b$称为偏置，$w_1$和$w_2$称为权重, 但是请注意，偏置和权重$w_1$、$w_2$的作用是不一样的。具体地说， $w_1$和$w_2$是控制输入的重要性的参数，而偏置是调整神经元被激活的容易程度（输出为1的程度）的参数。有时也会将$b$、$w_1$、$w_2$这些参数统称为权重。</p><h2 id="4-单层感知机的局限性"><a href="#4-单层感知机的局限性" class="headerlink" title="4.单层感知机的局限性"></a>4.单层感知机的局限性</h2><p>单层感知机的局限性就在于它只能表示由一条直线分割的空间,无法表示用曲线分割的空间。弯曲的曲线无法用感知机表示</p><p>​    <img src="p3.png" alt></p><p>​        <strong>图1-3　○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？</strong></p><p>​    <img src="p4.png" alt></p><p>​                    <strong>图1-4　使用曲线可以分开○和△</strong></p><h2 id="5-多层感知机"><a href="#5-多层感知机" class="headerlink" title="5.多层感知机"></a>5.多层感知机</h2><p>​    单层感知机虽然不能表示异或，但多层感知机的叠加却可以。</p><p>​    <img src="p5.png" alt></p><p>​                        <strong>图1-5　通过组合与门、与非门、或门实现异或门</strong></p><p>​    <img src="p6.png" alt></p><p>​                    <strong>图2-6　用感知机表示异或门</strong>            </p><p>叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。异或可以通过多层感知机实现。</p><p>图2-6中的感知机总共由3层构成，但是因为拥有权重的层实质上只有2层（第0层和第1层之间，第1层和第2层之间），所以称为“2层感知机”。不过，有的文献认为图2-6的感知机是由3层构成的，因而将其称为“3层感知机”。</p><h2 id="6-感知机与计算机"><a href="#6-感知机与计算机" class="headerlink" title="6.感知机与计算机"></a>6.感知机与计算机</h2><p>​    多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？</p><p>​    <strong>理论上</strong>可以说2层感知机就能构建计算机。这是因为，已有研究证明，2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数。但是，使用2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。</p><p>​    实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU）, 然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。</p><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h2><p>​    感知机从算法的角度来说就是单位阶跃函数+线性回归算法。</p><h1 id="二、神经网路"><a href="#二、神经网路" class="headerlink" title="二、神经网路"></a>二、神经网路</h1><h2 id="1-1-从感知器到神经网络"><a href="#1-1-从感知器到神经网络" class="headerlink" title="1.1 从感知器到神经网络"></a>1.1 从感知器到神经网络</h2><p>​    一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用<code>sigmoid</code> 函数等平滑的激活函数的多层网络。</p><p>​    <img src="p7.png" alt></p><p>​    如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层.</p><p>​    图中的网络一共由 3 层神经元构成, 但实质上只有 2层神经元有权重，因此将其称为“2层网络”。</p><h2 id="1-2-激活函数"><a href="#1-2-激活函数" class="headerlink" title="1.2 激活函数"></a>1.2 激活函数</h2><p>​    神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比”阶跃函数”更佳平滑.</p><p>​    阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。</p><p>​    激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。</p><p>​    输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。</p><p>​    常见的激活函数:</p><ul><li>阶跃函数:  </li></ul><p>$$<br>h(x) =\begin{cases}1, &amp; x &gt; 0 \\0, &amp; x \leq 0\end{cases}<br>$$</p><p>​    <img src="n2.png" alt></p><ul><li>sigmoid函数(S函数)<br>$$<br>h(x) = \dfrac {1}{1+e^{-x}}<br>$$</li></ul><p>  <img src="n3.png" alt></p><ul><li>Relu函数<br>$$<br>h(x) =\begin{cases}x, &amp; x &gt; 0 \\0, &amp; x \leq 0\end{cases}<br>$$</li></ul><p>  <img src="n4.png" alt></p><ul><li>softmax函数</li></ul><p>$$<br>\sigma(x) =  \dfrac {e^{a_k}}{ \sum_{i=1}^n e^{a^i}   }<br>$$</p><p><strong>注意</strong>: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。</p><p><strong>改进</strong>: 先进行归一化，再求值</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1010</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">990</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#  array([nan, nan, nan])  没有计算正确的值</span>mi <span class="token operator">=</span> np<span class="token punctuation">.</span>min<span class="token punctuation">(</span>a<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 990                                 </span>ma <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span>                                 nor <span class="token operator">=</span> <span class="token punctuation">(</span>a<span class="token operator">-</span>mi<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>ma<span class="token operator">-</span>mi<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 归一化 array([1. , 0.5, 0. ])   </span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>nor<span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>nor<span class="token punctuation">)</span><span class="token punctuation">)</span>                 <span class="token comment" spellcheck="true"># array([0.50648039, 0.30719589, 0.18632372])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​    一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此<strong>输出层的softmax函数一般会被省略</strong></p><p>​    求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系</p><h2 id="1-3-3层神经网络"><a href="#1-3-3层神经网络" class="headerlink" title="1.3 3层神经网络"></a>1.3 3层神经网络</h2><p>​    <img src="n7.png" alt></p><p>3层神经网络：输入层（第0层）有2个神经元，第1个隐藏层（第1层）有3个神经元，第2个隐藏层（第2层）有2个神经元，输出层（第3层）有2个神经元</p><p><img src="n6.png" alt></p><p>​                                图２－６权重的符号</p><p>请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个，索引表示的是后一层神经元的索引。</p><p>数学公式表示$a_1^{(1)}$<br>$$<br>a_1^{(1)} = a_{11}^{(1)}x1 + w_{12}^{(1)}x2 + b_1^{(1)}<br>$$<br>矩阵$W^{(1)}$表示第１层的权重：<br>$$<br>W^{(1)} = \begin{pmatrix}w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)}\\\\w_{12}^{(1)} &amp;w_{22}^{(1)} &amp;w_{32}^{(1)}\end{pmatrix}<br>$$<br>向量$B^{(1)}$表示第一层的偏置:<br>$$<br>\begin{pmatrix} b_1^{(1)} &amp; b_2^{(1)} &amp; b_3^{(1)}\\ \end{pmatrix} \quad<br>$$</p><p>$$<br>A^{(1)} = \begin{pmatrix} a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)}\\ \end{pmatrix} \quad<br>$$</p><p>$$<br>X^{(1)} = \begin{pmatrix} x_1 &amp; x_2\\ \end{pmatrix} \quad<br>$$</p><p>第１层的加权和表示:<br>$$<br>A^{(1)} = XW^{(1)} + B^{(1)}<br>$$</p><h1 id="三、神经网络的学习"><a href="#三、神经网络的学习" class="headerlink" title="三、神经网络的学习"></a>三、神经网络的学习</h1><p>​    神经网络的学习就是从训练数据学习权重参数，然后使用刚才学习到的参数对输入数据进行预测。</p><p>​    神经网络学习的策略是首先对输入数据进行前向传播（forward propagation）过程得到输出，然后计算输出与真实值之间的差别，最后通过反向传播跟新权重参数，重复这一过程直到权重参数没有更新，此时损失函数达到最小。</p><p>​    计算输出与真实值之间的差别通过损失函数计算。反向传播需要用到梯度下降算法实现。</p><h2 id="1-损失函数"><a href="#1-损失函数" class="headerlink" title="1.损失函数"></a>1.损失函数</h2><p>​    损失函数是表示神经网络性能的指标。神经网络通过减小损失函数，寻找最优权重参数。常用的误差函数有均方误差和交叉熵误差等。</p><ul><li>均方误差(mean squared error MSE)</li></ul><p>$$<br>E = \frac {1}{2} \sum_{k}({y_k - t_k})^2<br>$$</p><p>​    $y_k$表示神经网络的输出，$t_k$表示目标数据，k表示输出数据的维度，比如手写数字识别的输出数据的维度是10 (0-9)。</p><ul><li>交叉熵误差(cross entropy error)<br>$$<br>E = - \sum_{k}t_k\log y_k<br>$$<br>上式是一条数据的误差，如果批量计算多条数据则函数为:<br>$$<br>E = - \frac{1}{N}\sum_{N}\sum_{k}t_{nk}\log y_{nk}<br>$$<br>假设数据有N个， $t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神<br>经网络的输出， $t_{nk}$是目标数据）, 不过最后还要除以N, 通过这样的<br>平均化，可以获得和训练数据的数量无关的统一指标。</li></ul><h2 id="2-梯度下降"><a href="#2-梯度下降" class="headerlink" title="2.梯度下降"></a>2.梯度下降</h2><p>​    梯度是导数对多元函数的推广，它是多元函数对各个变量偏导形成的向量。多元函数的梯度定义为:<br>$$<br>\triangledown(x) = \begin{pmatrix} \frac{\partial f}{\partial x_1}, &amp; …, &amp; \frac{\partial f}{\partial x_n}\\  \end{pmatrix}^{T} \quad<br>$$<br>　其中$\triangledown$称为梯度算子，它作用与一个多元函数，得到一个向量。梯度是一个向量，即有大小又有方向，大小为该点的变化率，方向是该点增加最快的方向，-$\triangledown$(x)就为减小最快的方向。所以只要沿着梯度的反方向就可能达到函数的驻点，可能是局部极小值，全局极小值或鞍点。</p><p>​    在梯度下降法中，函数的取值从当前位置沿着梯度反方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度反方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度反方向前进，逐渐减小函数值的过程就是梯度下降法（gradient descent method）。</p><p>​    梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题。</p><p>数学公式来表示梯度下降方法:<br>$$<br>w_0 = w_0 - \eta \frac {\partial f}{\partial w_0}<br>$$</p><p>$$<br>w_1 = w_1 - \eta \frac {\partial f}{\partial w_1}<br>$$</p><p>$\eta$是学习率，表示根据梯度值变化的幅度，w在神经网络中代表权重.</p><p>​    控制梯度下降的幅度</p><p>​    0.1    0.01    0.001    ….</p><p>​    如果学习率太大，误差会逐步增大</p><p>​    如果学习率较大，误差会来回震荡</p><p>​    如果学习率太小， 误差下降缓慢</p><p>​    <img src="n8.png" alt></p><p>​                    图　$f(x_0, x_1) = x_0^2 + x_1^2$的梯度下降更新过程:虚线是函数的等高线</p><p>神经网络的学习分成下面4个步骤:<br><strong>步骤1（ mini-batch）</strong><br>    从训练数据中选出一部分数据，这部分数据称为mini-batch。我们的目标是减小损失函数的值。<br><strong>步骤2（计算梯度）</strong><br>    为了减小损失函数的值，需要求出各个权重参数的梯度。<br><strong>步骤3（更新参数）</strong><br>    将权重参数沿梯度方向进行微小更新。        </p><p><strong>步骤4（重复）</strong><br>    重复步骤1、步骤2、步骤3, 直到梯度为０或接近０．</p><p>​        </p><h2 id="3-误差反向传播"><a href="#3-误差反向传播" class="headerlink" title="3.误差反向传播"></a>3.误差反向传播</h2><ol><li><p><strong>计算图</strong></p><p>小明在超市买了2个苹果、 3个橘子。其中，苹果每个100日元，<br>橘子每个150日元。消费税是10%，请计算支付金额。</p><p><img src="b1.png" alt></p><p>​                        <strong>计算图的求解过程</strong></p><p>​    在计算图上，从左向右进行计算是一种正方向的传播, 简称为<strong>正向传播</strong>（forward propagation）。同理，反方向的计算，就是<strong>反向传播</strong>（backward propagation）。</p><p>​    计算图的特征是可以通过传递“局部计算”获得最终结果。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。</p></li></ol><p>   <img src="b2.png" alt></p><p>   ​    计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值. 从上图可以看到，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生200倍大小的影响，苹果的价格将产生2.2倍大小的影响。不过，因为这个例子中消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的1是100%，苹果的价格的1是1元）。</p><p>   反向传播的计算顺序是，将信号E乘以节点的局部导数，然后将结果传递给下一个节点 </p><ol start="2"><li><p><strong>链式法则</strong></p><p>反向传播将局部导数向正方向的反方向（从右到左）传递，传递这个局部导数的原理，是基于<strong>链式法则</strong>的。</p><p><img src="b3.png" alt></p><p><strong>图b3 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数</strong></p><p>如图所示，反向传播的计算顺序是，将信号$E$乘以节点的局部导数$(\frac {\partial y}{\partial x})$，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中y = f(x)的导数，也就是y关于x的导数$(\frac {\partial y}{\partial x})$。比如，假设$y = f(x) = x^2$，则局部导数为 $(\frac {\partial y}{\partial x})$= $2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。</p></li></ol><p>   <img src="b4.png" alt></p><p>   根据链式法则，$\frac {\partial z}{\partial z} \frac {\partial z}{\partial t} \frac {\partial t}{\partial x} = \frac {\partial z}{\partial t} \frac {\partial t}{\partial x} = \frac {\partial z}{\partial x}$成立，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的</p><ol start="4"><li><p><strong>激活函数的反向传播</strong></p><ul><li><p>Relu激活函数<br>$$<br>h^{‘}(x) =\begin{cases}1, &amp; x &gt; 0 \\0, &amp; x \leq 0\end{cases}<br>$$<br>如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。</p><p>Relu 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。</p></li><li><p>Sigmoid激活函数<br>$$<br>h^{‘}(y) = y(1-y)<br>$$</p></li></ul></li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Sigmoid</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> None     <span class="token comment" spellcheck="true"># 保存当前层的输出</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        前向传播        :param x:         :return:         """</span>        out <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> out        <span class="token keyword">return</span> out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        反向传播        :param dout:         :return:         """</span>        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>out<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>out        <span class="token keyword">return</span> dx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li><p>矩阵运算<br>$$<br>X \cdot W = Y<br>$$</p><p>$$<br>\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T<br>$$</p><p>$$<br>\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}<br>$$</p></li></ol><p>   <img src="b6.png" alt></p><p>   需要注意各个变量的形状</p><ol start="5"><li><p>Softmax-with-Loss层</p><p><img src="b7.png" alt></p><p>Softmax层将输入（a1, a2, a3）正规化，输出（y1,y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和目标值（t1,t2, t3），从这些数据中输出损失L。</p><p>Softmax反向传播的梯度为（y1 - t1,  y2 - t2, y3 - t3）.</p></li></ol><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><p>通过使用计算图，可以直观地把握计算过程。<br>• 计算图的节点是由局部计算构成的。局部计算构成全局计算。<br>• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以<br>计算各个节点的导数。<br>• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传<br>播法）。<br>• 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传<br>播法的实现是否正确（梯度确认）</p><h1 id="四、参考"><a href="#四、参考" class="headerlink" title="四、参考"></a>四、参考</h1><p>《深度学习入门: 基于Python的理论与实现》</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
            <tag> ANN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2019/11/24/ji-qi-xue-xi-gai-shu/"/>
      <url>/2019/11/24/ji-qi-xue-xi-gai-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="一、人工智能"><a href="#一、人工智能" class="headerlink" title="一、人工智能"></a>一、人工智能</h1><ol><li><p>什么是人工智能</p><p>​        <img src="3.jpg" alt="3"></p><p>​        人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。<br>​        人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。</p></li><li><p>强人工智能和弱人工智能</p><p>​        早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。<br>​        人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。<br>​        我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类；或者人脸识别。</p></li><li><p>人工智能，机器学习和深度学习的关系</p><p><img src="1.png" alt></p></li></ol><p>   机器学习是人工智能的一种实现方式，也是最重要的实现方式。目前机器学习的方法被大量的应用解决人工智能的问题。</p><p>   深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。</p><p>   总的来说，深度学习是机器学习的一个子集，机器学习是人工智能的一个子集。</p><h1 id="二、机器学习"><a href="#二、机器学习" class="headerlink" title="二、机器学习"></a>二、机器学习</h1><h2 id="机器学习的概念"><a href="#机器学习的概念" class="headerlink" title="机器学习的概念"></a>机器学习的概念</h2><ol><li><p>什么是机器学习</p><p>机器学习就是机器像人类一样学习，人能从过去的经验中学习，对于机器来说过去的经验就是记录的数据。机器理解大量的数据然后归纳出模型来对数据进行预测和分析。</p><p><img src="11.png" alt></p></li></ol><ol start="2"><li><p>机器学习的对象</p><p>机器学习的对象是数据(data), 它从数据出发，提取数据的特征，抽取数据的模型，发现数据的知识，又回到对数据的分析与预测中去。</p><p>作为机器学习的对象，数据包括各种数字、文字、图像、音频、视频数据以及它们的组合。</p></li></ol><ol start="3"><li><p>机器学习的目的</p><p>机器学习用于对数据进行预测和分析，特别是对未知新数据进行预测和分析。对数据的预测可以让计算机更加智能化；对数据的分析可以让人们获取新的知识。</p><p>对数据的预测和分析是通过构建模型实现的。机器学习总的目标就是考虑学习什么样的模型和如何学习模型，已使模型对数据进行准确的预测和分析，同时也要尽可能地提升学习的效率。</p></li></ol><ol start="4"><li><p>机器学习的分类</p><p>机器学习由监督学习、非监督学习、半监督学习和强化学习等组成。</p><p><img src="6.jpg" alt></p><p>监督学习，非监督学习，半监督学习的区别是训练数据是否有标记。</p></li></ol><ol start="5"><li><p>机器学习的应用场景</p><p><img src="5.jpg" alt></p></li></ol><h2 id="机器学习三要素"><a href="#机器学习三要素" class="headerlink" title="机器学习三要素"></a>机器学习三要素</h2><p>​        机器学习的方法由模型，策略，算法构成。</p><ol><li><p>模型</p><p>假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，这个函数的集合就称为假设空间。</p><p>模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的概率分布或决策函数。</p><p>例如：线性回归算法，它的模型就是一个线性函数，即</p><p>$$<br>f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b<br>$$<br>一般用向量形式写成   $f(x) = w^Tx +b$, 其中$w = (w_1, w_2, …, w_3)$.</p><p>$w$和$d$确定之后，模型就确定了。</p><p>根据$w$和$b$的所有取值所组成的集合就是线性回归算法的假设空间。</p></li><li><p>策略</p><p>应用某个评估指标, 从假设空间中选择一个最优的模型。</p><p>对于给定的输入$X$, 模型$f(X)$给出相应的输出$Y$, 这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。记作$L(Y, f(X))$</p><p>机器学习中常用的损失函数有以下几种:</p><p>(1) 0-1损失函数(0-1 loss function)<br>$$<br>L(Y,f(X)) =<br>\begin{cases}<br>0, &amp; \text{Y = f(X)}  \<br>1, &amp; \text{Y $\neq$ f(X)}<br>\end{cases}<br>$$</p><p>(2) 平方损失函数(quadratic loss function)<br>$$<br>L(Y, f(X)) = (Y - f(X))^2<br>$$<br>(3) 绝对损失函数<br>$$<br>L(Y, f(X)) = |Y - f(X)|<br>$$<br>(4) Huber损失—-平滑绝对误差<br>$$<br>L_\delta(Y,f(X)) =<br>\begin{cases}<br>{\frac 12}(y-f(x))^2, &amp; for|y-f(x)|\le \delta  \<br>\delta|y-f(x)| - {\frac 12}{\delta}^2, &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>(5) 对数损失函数(logarithmic loss function)<br>$$<br>L(Y, P(Y|X)) = -logP(Y|X)<br>$$<br>损失函数值越小，模型就越好。<strong>损失函数值最小的模型就是最优模型</strong>。</p></li></ol><p>   举例: 线性回归, $$f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b$$ , 均方误差为<br>   $$<br>   E(w, b) = {\frac 1n}\sum_{i=1}^n (f(x_i) - Y)^2<br>   $$<br>   均方误差的几何意义就是欧几里得距离。</p><p>   <img src="12.png" alt></p><p>   <img src="14.png" alt></p><ol start="3"><li><p>算法</p><p>算法是指学习模型的具体计算方法。</p><p>机器学习常用优化算法:</p><p>(1) 梯度下降</p><p>​    随机梯度下降(<code>Stochastic Gradient Descent, SGD</code>)</p><p>​    批量梯度下降(<code>Batch Gradient Descent, BGD</code>)</p><p>​    小批量梯度下降(<code>Mini-batch Gradient Descent, MBGD</code>)</p><p>(2) 梯度下降的变体</p><p>​    <code>Momentum</code>、<code>Adagrad</code>、<code>Adadelta</code>、<code>RMSprop</code>、<code>Adam</code></p><p>(3) 牛顿法和拟牛顿法</p></li></ol><p>​        举例：线性回归的优化算法可以使用梯度下降或最小二乘法</p><p>​        在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之后最小</p><p>​        求解$w$和$b$使$E(w, b) = \sum_{i=1}^n(y_i - wx_i -b)$最小化，可以将$E(w, b)$分别对$w$和$b$求导，等于0，可以得到$w$和$b$的值。</p><h2 id="机器学习的训练步骤"><a href="#机器学习的训练步骤" class="headerlink" title="机器学习的训练步骤"></a>机器学习的训练步骤</h2><h3 id="1-明确问题和目标"><a href="#1-明确问题和目标" class="headerlink" title="1. 明确问题和目标"></a>1. 明确问题和目标</h3><p>​    需要解决什么问题，达到什么目标</p><h3 id="2-确定输入，收集数据"><a href="#2-确定输入，收集数据" class="headerlink" title="2.确定输入，收集数据"></a>2.确定输入，收集数据</h3><p>​    通过多种途径得到一个有限的训练数据的集合</p><p>​    <a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Kaggle数据集</a></p><p>​    <a href="https://registry.opendata.aws" target="_blank" rel="noopener">亚马逊数据集</a></p><p>​    <a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">UCI机器学习库</a></p><p>​    <a href="https://toolbox.google.com/datasetsearch" target="_blank" rel="noopener">谷歌的数据集搜素引擎</a></p><p>​    <a href="https://msropendata.com/" target="_blank" rel="noopener">微软数据集</a></p><p>​    <a href="https://github.com/awesomedata/awesome-public-datasets" target="_blank" rel="noopener">Awesome公共数据集</a></p><p>​    <a href="https://www.visualdata.io/" target="_blank" rel="noopener">计算机视觉数据集</a></p><p>​    <a href="http://image-net.org/" target="_blank" rel="noopener">ImageNet</a></p><p>​    <a href="http://cocodataset.org/" target="_blank" rel="noopener">MS COCO</a></p><h3 id="3-确定输出，选择算法"><a href="#3-确定输出，选择算法" class="headerlink" title="3.确定输出，选择算法"></a>3.确定输出，选择算法</h3><p>​    根据输入输出数据的类型决定使用的算法类型，分类还是回归？</p><p>​    输入变量与输出变量均为连续变量的预测问题称为回归问题；</p><p>​    输出变量为有限个离散变量的预测问题称为分类问题；</p><p>​    在对应的算法类型中选择一个或多个算法。</p><h3 id="4-特征工程"><a href="#4-特征工程" class="headerlink" title="4.特征工程"></a>4.特征工程</h3><ul><li><p>数据预处理</p><ul><li>缺失数据—&gt; 删除 和 填充 (平均数，众数)</li><li>处理特征数据—&gt; 正规化 (归一化，正则化，白化)</li><li>处理类别数据—&gt;独热编码</li><li>数据集划分—&gt;训练、验证、测试</li></ul></li><li><p>数据降维</p><ul><li><p>特征选择</p><ul><li>使用L1正则化进行数据稀疏化</li><li>序列特征选择算法  SBS</li><li>通过随机森林判定特征的重要性</li></ul></li><li><p>特征提取 (将特征压缩到一个低维空间，而不是像特征选择那样完全剔除不相关的特征)</p><ul><li>PCA  主成分分析</li><li>线性判别    </li></ul></li></ul></li></ul><h3 id="5-建立模型"><a href="#5-建立模型" class="headerlink" title="5. 建立模型"></a>5. 建立模型</h3><ul><li><p>模型空间</p></li><li><p>损失函数</p></li><li><p>优化算法</p></li><li><p>评估标准</p></li></ul><ol><li><p>模型空间</p><p>确定了算法也就确定了模型空间，模型空间包含了算法的所有可能</p></li><li><p>损失函数</p><p>根据具体的算法和输出决定损坏函数</p></li><li><p>优化算法</p><p>选择优化算法</p></li><li><p>评估指标:</p></li></ol><p><img src="7.png" alt></p><ul><li><p>分类算法</p><ul><li>准确率</li><li>精确率和召回率(查准率和查全率)</li><li>ROC和AUC</li><li>$F_1$和$F_{\beta}$</li></ul></li><li><p>回归算法</p><ul><li><p>平均绝对误差</p></li><li><p>均方误差</p></li><li><p>R2分数</p></li></ul></li></ul><h3 id="6-确定最优模型"><a href="#6-确定最优模型" class="headerlink" title="6.确定最优模型"></a>6.确定最优模型</h3><p>​    不断重复<strong>训练模型/评估模型/选择模型</strong>的步骤指导选择最优的模型</p><ul><li><p>模型选择数据集划分:</p><p>​        训练集训练模型</p><p>​        验证集评估模型</p><p>​        测试集测试模型</p><p>​        <img src="13.png" alt></p></li><li><p>使用k折交叉验证评估模型性能</p><ul><li>holdout方法</li><li>k折交叉验证</li></ul><p>通常情况下，我们将k折交叉验证用于模型的调优，也就是找到使得模型泛化性能最优的超参值。一旦找到了满意的超参值，我们就可<br>以在全部的训练数据上重新训练模型，并使用独立的测试数据集对模型性能做出最终评价。 </p></li><li><p>通过学习及验证曲线来调试算法</p><ul><li>使用学习曲线判定偏差和方差问题</li><li>使用验证曲线判定过拟合与欠拟合</li></ul></li><li><p>使用网格搜索调优机器学习模型</p><ul><li>使用网格搜索调优超参数</li><li>通过嵌套交叉验证选择模型</li><li>网格搜索（grid search），它通过寻找最优的超参值的组合以进一步提高模型的性能。 </li></ul></li></ul><h3 id="7-应用实际问题"><a href="#7-应用实际问题" class="headerlink" title="7.应用实际问题"></a>7.应用实际问题</h3><p>​    利用学习的最优模型对新数据进行预测或分析</p><h1 id="三、参考"><a href="#三、参考" class="headerlink" title="三、参考"></a>三、参考</h1><p>​    《统计学习方法》李航</p><p>​     《Python机器学习》[美] [塞巴斯蒂安·拉施卡]著  高明 徐莹 陶虎成译</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> AI </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法之树</title>
      <link href="/2019/11/23/shu-ju-jie-gou-yu-suan-fa-zhi-shu/"/>
      <url>/2019/11/23/shu-ju-jie-gou-yu-suan-fa-zhi-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="一、树"><a href="#一、树" class="headerlink" title="一、树"></a>一、树</h1><h2 id="树的定义"><a href="#树的定义" class="headerlink" title="树的定义"></a>树的定义</h2><p>树(Tree)是n(n&gt;=0)个结点的有限集。当n=0时成为空树，在任意一颗非空树中：</p><ul><li>有且仅有一个特定的称为根(Root)的结点;</li><li>当n&gt;1时，其余节点可分为m(m&gt;0)个<strong>互不相交</strong>的有限集T1、T2、。。。、Tm, 其中每一个集合本身又是一棵树，并且称为根的子树(SubTree)。</li></ul><h2 id="结点的分类"><a href="#结点的分类" class="headerlink" title="结点的分类"></a>结点的分类</h2><p>结点拥有的子树称为结点的度(Degree), 树的度取树内各结点的度的最大值。</p><ul><li>度为0的结点称为叶结点(Leaf)或终端结点；</li><li>度不为0的点称为分支结点或非终端结点，除根结点外，分支结点也称为内部结点。</li></ul><h2 id="结点间的关系"><a href="#结点间的关系" class="headerlink" title="结点间的关系"></a>结点间的关系</h2><ul><li>结点的子树的根称为结点的孩子(Child), 相应的，该结点称为孩子的双亲(Parent), 同一双亲的孩子之间互称为兄弟(Sibling)。</li><li>结点的祖先是从根到该结点所经过分支上的所有结点。</li></ul><h2 id="结点的层次"><a href="#结点的层次" class="headerlink" title="结点的层次"></a>结点的层次</h2><ul><li>结点的层次(Level)从根开始，根为第一层，根的孩子为第二层。</li><li>其双亲在同一层的结点互为堂兄弟。</li><li>树中结点最大层称为树的深度(Depth)或者高度。</li></ul><h2 id="有序树和森林"><a href="#有序树和森林" class="headerlink" title="有序树和森林"></a>有序树和森林</h2><ul><li>如果将树中结点的各个子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。</li><li>森林(Forest)是m(m&gt;=0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。</li></ul><h2 id="树的存储结构"><a href="#树的存储结构" class="headerlink" title="树的存储结构"></a>树的存储结构</h2><h3 id="1-双亲表示法"><a href="#1-双亲表示法" class="headerlink" title="1.双亲表示法"></a>1.双亲表示法</h3><ul><li><p>双亲表示法，言外之意就是以双亲作为索引的关键词的一种存储方式。</p></li><li><p>我们假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示双亲结点在数组中位置的元素。</p></li><li><p>也就是说，每个结点除了知道自己是谁，还知道它的双亲在哪里。</p></li><li><p>那么我们可以做如下定义:</p><pre class="line-numbers language-c++"><code class="language-c++">// 树的双亲表示法结构定义#define MAX_TREE_SIZE 100typedef int ElemType;typedef struct PTNode{    ElemType data; // 结点数据    int parent;     // 双亲位置} PTNode;typedef struct{    PTNode nodes[MAX_TREE_SIZE];    int r;        // 根的位置    int n;        // 结点数目} PTree;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="parents.png" alt></p></li><li><p>这样的存储结构，我们可以根据某结点的parent指针找到它的双亲结点，所用的时间复杂度是O(1), 索引到parent的值为-1时，表示找到了树结点的根。</p></li><li><p>可是，如果我们要知道某结点的孩子是什么？那么不好意思，请遍历整个树结构。</p></li><li><p>改进一些也很简单，只需要在每个结点中添加孩子的索引</p></li></ul><h3 id="2-孩子表示法"><a href="#2-孩子表示法" class="headerlink" title="2.孩子表示法"></a>2.孩子表示法</h3><ul><li><p>方案一：根据树的度，声明足够空间存放子树的结点。缺点十分明显，就是造成了浪费！</p><p><img src="childs01.png" alt></p></li><li><p>方案二：根据每个结点的度申请空间存放子树结点。</p><p><img src="childs02.png" alt></p></li><li><p>方案三:  数组和链表结合</p><p><img src="childs03.png" alt></p></li></ul><h3 id="3-双亲孩子表示法"><a href="#3-双亲孩子表示法" class="headerlink" title="3.双亲孩子表示法"></a>3.双亲孩子表示法</h3><p>​    前两种方案结合</p><p>​    <img src="parchild.png" alt></p><pre class="line-numbers language-c++"><code class="language-c++">#define MAX_TREE_SIZE 100// 孩子节点typedef struct CTNode{    int child;  // 孩子结点下标    struct CTNode *next; // 指向下一个孩子的指针} *ChildPtr;// 表头结构typedef struct{    ElemType data;  // 存放在树中的结点的数据    int parent;        // 存放双亲的下标    ChildPtr firstchild; // 指向第一个孩子的指针} CTBox;// 树结构typedef struct{    CTBox nodes[MAX_TREE_SIZE]; // 结点数组    int r;        // 根的位置    int n;        // 结点数目}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="二、二叉树"><a href="#二、二叉树" class="headerlink" title="二、二叉树"></a>二、二叉树</h1><h2 id="二叉树的定义"><a href="#二叉树的定义" class="headerlink" title="二叉树的定义"></a>二叉树的定义</h2><ul><li>二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）</li><li>左子树和右子树是有顺序的，次序不能颠倒。</li><li>即是树中某结点只有一颗子树，也要区分它是左子树还是右子树。</li></ul><h2 id="二叉树的五种基本形态"><a href="#二叉树的五种基本形态" class="headerlink" title="二叉树的五种基本形态"></a>二叉树的五种基本形态</h2><ul><li><p>空二叉树</p></li><li><p>只有一个根结点</p></li><li><p>根结点只有左子树</p></li><li><p>根结点只有右子树</p></li><li><p>根节点即有左子树又有右子树</p><p><img src="binarytree.png" alt></p></li></ul><h2 id="特殊二叉树"><a href="#特殊二叉树" class="headerlink" title="特殊二叉树"></a>特殊二叉树</h2><ul><li><p>斜树</p></li><li><p>满二叉树</p><ul><li><p>叶子只能出现在最下一层</p></li><li><p>非叶子结点的度都是2</p></li><li><p>在同样深度的二叉树中，满二叉树的结点个数一定是最多的，同时叶子也是最多的。</p><p><img src="%E6%BB%A1%E4%BA%8C%E5%8F%89%E6%A0%91.png" alt></p></li></ul></li><li><p>完全二叉树</p><p>若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。</p><ul><li>叶子结点只能出现在最下两层</li><li>最下层的叶子一定集中在左部连续位置。</li><li>倒数第二层，若有叶子结点，一定都在右部连续位置。</li><li>如果结点度为1，则该结点只有左孩子</li><li>同样结点树的二叉树，完全二叉树的深度是最小的。</li></ul><p><img src="%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91.png" alt></p></li></ul><h2 id="二叉树的性质"><a href="#二叉树的性质" class="headerlink" title="二叉树的性质"></a>二叉树的性质</h2><p><strong>性质1:</strong> 在二叉树的第i层上至多有2^(i-1)个结点（i&gt;0）<br><strong>性质2:</strong> 深度为k的二叉树至多有2^k - 1个结点（k&gt;0）<br><strong>性质3:</strong> 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;<br><strong>性质4:</strong>具有n个结点的完全二叉树的深度必为 log2(n+1)<br><strong>性质5:</strong>对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）</p><h1 id="三、动态查找树"><a href="#三、动态查找树" class="headerlink" title="三、动态查找树"></a>三、动态查找树</h1><h2 id="一）二叉查找树"><a href="#一）二叉查找树" class="headerlink" title="一）二叉查找树"></a>一）二叉查找树</h2><h2 id="二）平衡二叉树-AVL树"><a href="#二）平衡二叉树-AVL树" class="headerlink" title="二）平衡二叉树(AVL树)"></a>二）平衡二叉树(AVL树)</h2><h2 id="三）红黑树"><a href="#三）红黑树" class="headerlink" title="三）红黑树"></a>三）红黑树</h2><h1 id="四、多路查找树"><a href="#四、多路查找树" class="headerlink" title="四、多路查找树"></a>四、多路查找树</h1><h2 id="一）B树"><a href="#一）B树" class="headerlink" title="一）B树"></a>一）B树</h2><h2 id="二）B-树"><a href="#二）B-树" class="headerlink" title="二）B+树"></a>二）B+树</h2><h2 id="三）B-树"><a href="#三）B-树" class="headerlink" title="三）B*树"></a>三）B*树</h2><h2 id="四）R树"><a href="#四）R树" class="headerlink" title="四）R树"></a>四）R树</h2><h1 id="五、决策树"><a href="#五、决策树" class="headerlink" title="五、决策树"></a>五、决策树</h1><h1 id="六-、LeetCode关于树的题目"><a href="#六-、LeetCode关于树的题目" class="headerlink" title="六 、LeetCode关于树的题目"></a>六 、<code>LeetCode</code>关于树的题目</h1>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tree </tag>
            
            <tag> C/C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMake用法总结</title>
      <link href="/2019/11/18/cmake-yong-fa-zong-jie/"/>
      <url>/2019/11/18/cmake-yong-fa-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="一、CMake的作用"><a href="#一、CMake的作用" class="headerlink" title="一、CMake的作用"></a>一、<code>CMake</code>的作用</h1><p>大家都知道, 源文件的编译步骤为:</p><ul><li>预处理: 宏定义展开, 头文件展开, 条件编译</li><li>编译: 检查语法, 生成编译文件</li><li>汇编: 将汇编文件生成目标文件(二进制文件)</li><li>链接: 将目标文件链接成目标程序</li></ul><p>但如果源文件太多，一个一个编译就会特别麻烦，为什么不批处理编译源文件呢，于是就有了make工具，它是一个自动化编译工具，你可以使用一条命令实现完全编译。还可以指定文件编译的顺序。但是使用make编译源码，需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile，所以编写makefile文件也是一个程序员所必备的技能。<br> 对于一个大工程，编写makefile实在是件复杂的事，于是人们又想，为什么不设计一个工具，读入所有源文件之后，自动生成makefile呢，于是就出现了<code>cmake</code>工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。所以在编程的世界里没有捷径可走，还是要脚踏实地的。</p><p> 原文件－－camkelist —cmake —makefile —make —生成可执行文件</p><h1 id="二、CMake基本语法规则"><a href="#二、CMake基本语法规则" class="headerlink" title="二、CMake基本语法规则"></a>二、<code>CMake基本语法规则</code></h1><ol><li><p>变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名</p></li><li><p>指令(参数1  参数2  …)</p><p>参数使用括弧括起，参数之间使用空格或分号分开</p></li><li><p>指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令</p></li><li><p>关于双引号的疑惑</p><pre class="line-numbers language-shell"><code class="language-shell">SET(SRC_LIST main.c)也可以写成 SET(SRC_LIST “main.c”)是没有区别的，但是假设一个源文件的文件名是 fu nc.c(文件名中间包含了空格)。这时候就必须使用双引号，如果写成了 SET(SRC_LIST fu nc.c)，就会出现错误，提示你找不到 fu 文件和 nc.c 文件。这种情况，就必须写成:SET(SRC_LIST “fu nc.c”)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h1 id="三、内部构建与外部构建"><a href="#三、内部构建与外部构建" class="headerlink" title="三、内部构建与外部构建"></a>三、内部构建与外部构建</h1><p>内部构建就是在项目跟目录直接编译</p><p>引出了我们对外部编译的探讨，外部编译的过程如下：</p><ol><li>首先，请清除 t1 目录中除 main.c CmakeLists.txt 之外的所有中间文件，最关键的是 CMakeCache.txt。</li><li>在 t1 目录中建立 build 目录，当然你也可以在任何地方建立 build 目录，不一定必须在工程目录中。</li><li>进入 build 目录，运行 cmake ..(注意,..代表父目录，因为父目录存在我们需要的CMakeLists.txt，如果你在其他地方建立了 build 目录，需要运行 cmake &lt;工程的全路径&gt;)，查看一下 build 目录，就会发现了生成了编译需要的 Makefile 以及其他的中间文件.</li><li>运行 make 构建工程，就会在当前目录(build 目录)中获得目标文件 hello。</li><li>上述过程就是所谓的 out-of-source 外部编译，一个最大的好处是，对于原有的工程没有任何影响，所有动作全部发生在编译目录。通过这一点，也足以说服我们全部采用外部编译方式构建工程。</li><li>这里需要特别注意的是：<br>通过外部编译进行工程构建，HELLO_SOURCE_DIR 仍然指代工程路径，即/backup/cmake/t1, 而 HELLO_BINARY_DIR 则指代编译路径，即/backup/cmake/t1/build</li></ol><p>#　四、安装库和INSTALL指令</p><p>有两种安装方式，一种是从代码编译后直接 make install 安装，一种是cmake的install 指令安装。</p><h2 id="1、make-install"><a href="#1、make-install" class="headerlink" title="1、make install"></a>1、<code>make install</code></h2><pre class="line-numbers language-shell"><code class="language-shell">DESTDIR=install:    mkdir -p $(DESTDIR)/usr/bin    install -m 755 hello $(DESTDIR)/usr/bin你可以通过:    make install将 hello 直接安装到/usr/bin 目录，也可以通过 make installDESTDIR=/tmp/test 将他安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。稍微复杂一点的是还需要定义 PREFIX，一般 autotools 工程，会运行这样的指令:./configure –prefix=/usr 或者./configure --prefix=/usr/local 来指定PREFIX比如上面的 Makefile 就可以改写成:DESTDIR=PREFIX=/usrinstall:    mkdir -p $(DESTDIR)/$(PREFIX)/bin    install -m 755 hello $(DESTDIR)/$(PREFIX)/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、cmake-INSTALL指令安装"><a href="#2、cmake-INSTALL指令安装" class="headerlink" title="2、cmake INSTALL指令安装"></a>2、<code>cmake INSTALL</code>指令安装</h2><p>这里需要引入一个新的 cmake 指令 INSTALL 和一个非常有用的变量<br>CMAKE_INSTALL_PREFIX。CMAKE_INSTALL_PREFIX 变量类似于 configure 脚本的 –prefix，常见的使用方法看起来是这个样子：<br>    <code>cmake -DCMAKE_INSTALL_PREFIX=/usr ..</code><br>INSTALL 指令用于定义安装规则，安装的内容可以包括目标二进制、动态库、静态库以及文件、目录、脚本等。</p><p>INSTALL 指令包含了各种安装类型，我们需要一个个分开解释：<br>目标文件的安装：</p><pre><code>INSTALL(TARGETS targets...    [[ARCHIVE|LIBRARY|RUNTIME]    [DESTINATION &lt;dir&gt;]    [PERMISSIONS permissions...]    [CONFIGURATIONS [Debug|Release|...]]    [COMPONENT &lt;component&gt;]    [OPTIONAL]] [...])</code></pre><p>参数中的 TARGETS 后面跟的就是我们通过 ADD_EXECUTABLE 或者 ADD_LIBRARY 定义的<br>目标文件，可能是可执行二进制、动态库、静态库。<br>目标类型也就相对应的有三种，ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME<br>特指可执行目标二进制。<br>DESTINATION 定义了安装的路径，如果路径以/开头，那么指的是绝对路径，这时候<br>CMAKE_INSTALL_PREFIX 其实就无效了。如果你希望使用 CMAKE_INSTALL_PREFIX 来<br>定义安装路径，就要写成相对路径，即不要以/开头，那么安装后的路径就是<br>${CMAKE_INSTALL_PREFIX}/&lt;DESTINATION 定义的路径&gt;<br>举个简单的例子：</p><pre class="line-numbers language-shell"><code class="language-shell">INSTALL(TARGETS myrun mylib mystaticlib    RUNTIME DESTINATION bin    LIBRARY DESTINATION lib    ARCHIVE DESTINATION libstatic)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的例子会将：<br>可执行二进制 myrun 安装到${CMAKE_INSTALL_PREFIX}/bin 目录<br>动态库 libmylib 安装到${CMAKE_INSTALL_PREFIX}/lib 目录<br>静态库 libmystaticlib 安装到${CMAKE_INSTALL_PREFIX}/libstatic 目录<br>特别注意的是你不需要关心 TARGETS 具体生成的路径，只需要写上 TARGETS 名称就可以<br>了。  </p><p>普通文件的安装：</p><pre class="line-numbers language-shell"><code class="language-shell">INSTALL(FILES files... DESTINATION <dir>    [PERMISSIONS permissions...]    [CONFIGURATIONS [Debug|Release|...]]    [COMPONENT <component>]    [RENAME <name>] [OPTIONAL])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可用于安装一般文件，并可以指定访问权限，文件名是此指令所在路径下的相对路径。如果<br>默认不定义权限 PERMISSIONS，安装后的权限为：<br>OWNER_WRITE, OWNER_READ, GROUP_READ,和 WORLD_READ，即 644 权限。<br>非目标文件的可执行程序安装(比如脚本之类)：</p><pre><code>INSTALL(PROGRAMS files... DESTINATION &lt;dir&gt;    [PERMISSIONS permissions...]    [CONFIGURATIONS [Debug|Release|...]]    [COMPONENT &lt;component&gt;]    [RENAME &lt;name&gt;] [OPTIONAL])</code></pre><p>跟上面的 FILES 指令使用方法一样，唯一的不同是安装后权限为:<br>OWNER_EXECUTE, GROUP_EXECUTE, 和 WORLD_EXECUTE，即 755 权限<br>目录的安装：</p><pre class="line-numbers language-shell"><code class="language-shell">INSTALL(DIRECTORY dirs... DESTINATION <dir>    [FILE_PERMISSIONS permissions...]    [DIRECTORY_PERMISSIONS permissions...]    [USE_SOURCE_PERMISSIONS]    [CONFIGURATIONS [Debug|Release|...]]    [COMPONENT <component>]    [[PATTERN <pattern> | REGEX <regex>]    [EXCLUDE] [PERMISSIONS permissions...]] [...])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里主要介绍其中的 DIRECTORY、PATTERN 以及 PERMISSIONS 参数。</p><p>DIRECTORY 后面连接的是所在 Source 目录的相对路径，但务必注意：abc 和 abc/有很大的区别。<br>如果目录名不以/结尾，那么这个目录将被安装为目标路径下的 abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。<br>PATTERN 用于使用正则表达式进行过滤，PERMISSIONS 用于指定 PATTERN 过滤后的文件权限。<br>我们来看一个例子:</p><pre class="line-numbers language-shell"><code class="language-shell">INSTALL(DIRECTORY icons scripts/ DESTINATION     share/myprojPATTERN "CVS" EXCLUDEPATTERN "scripts/*"PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READGROUP_EXECUTE GROUP_READ)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这条指令的执行结果是：<br>将 icons 目录安装到 <prefix>/share/myproj，将 scripts/中的内容安装到<prefix>/share/myproj不包含目录名为 CVS 的目录，对于 scripts/*  文件指定权限为 OWNER_EXECUTE   OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ.</prefix></prefix></p><p>安装时 CMAKE 脚本的执行：</p><pre><code>INSTALL([[SCRIPT &lt;file&gt;] [CODE &lt;code&gt;]] [...])SCRIPT 参数用于在安装时调用 cmake 脚本文件（也就是&lt;abc&gt;.cmake 文件）CODE 参数用于执行 CMAKE 指令，必须以双引号括起来。比如：INSTALL(CODE &quot;MESSAGE(\&quot;Sample install message.\&quot;)&quot;)</code></pre><h1 id="五、静态库和动态库构建"><a href="#五、静态库和动态库构建" class="headerlink" title="五、静态库和动态库构建"></a>五、静态库和动态库构建</h1><h2 id="1、ADD-LIBRARY指令"><a href="#1、ADD-LIBRARY指令" class="headerlink" title="1、ADD_LIBRARY指令"></a>1、ADD_LIBRARY指令</h2><pre class="line-numbers language-shell"><code class="language-shell">ADD_LIBRARY(libname [SHARED|STATIC|MODULE]    [EXCLUDE_FROM_ALL]    source1 source2 ... sourceN)# 不需要写全lib<libname>.so, 只需要填写<libname>,cmake系统会自动为你生成，lib<libname>.X# 类型有三种:    SHARED，动态库    .so    STATIC，静态库    .a    MODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。#EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、指定库的生成路径"><a href="#2、指定库的生成路径" class="headerlink" title="2、指定库的生成路径"></a>2、指定库的生成路径</h2><p>​    两种方法</p><ol><li>ADD_SUBDIRECTORY指令来指定一个编译输出位置</li><li>在CMakeLists.txt中添加　SET(LIBRARY_OUTPUT_PATH &lt;路径&gt;)来指定一个新的位置</li></ol><h2 id="3、同时生成动态库和静态库"><a href="#3、同时生成动态库和静态库" class="headerlink" title="3、同时生成动态库和静态库"></a>3、同时生成动态库和静态库</h2><p>因为ADD_SUBDIRECTORY的TARGET(libname)是唯一的，所以生成动态库和静态库不能指定相同的名称，想要有相同的名称需要用到SET_TARGET_PROPERTIES指令。</p><p>SET_TARGET_PROPERTIES，其基本语法是：</p><pre class="line-numbers language-shell"><code class="language-shell">SET_TARGET_PROPERTIES(target1 target2 ...    PROPERTIES prop1 value1    prop2 value2 ...)# 举例ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})　# 动态库ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) # 静态库SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME "hello")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。</p><p>与他对应的指令是：<br>    GET_TARGET_PROPERTY(VAR target property)</p><p>举例</p><pre class="line-numbers language-shell"><code class="language-shell">GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME)MESSAGE(STATUS “This is the hello_staticOUTPUT_NAME:”${OUTPUT_VALUE})# 如果没有这个属性定义，则返回 NOTFOUND.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、动态库版本号"><a href="#4、动态库版本号" class="headerlink" title="4、动态库版本号"></a>4、动态库版本号</h2><pre class="line-numbers language-shell"><code class="language-shell">SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1)# VERSION 指代动态库版本，SOVERSION 指代 API 版本。# 在 build/lib 目录会生成：    libhello.so.1.2    libhello.so.1->libhello.so.1.2    libhello.so -> libhello.so.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="六、使用共享库和头文件"><a href="#六、使用共享库和头文件" class="headerlink" title="六、使用共享库和头文件"></a>六、使用共享库和头文件</h1><h2 id="1-INCLUDE-DIRECTORIES指令"><a href="#1-INCLUDE-DIRECTORIES指令" class="headerlink" title="1.INCLUDE_DIRECTORIES指令"></a>1.<code>INCLUDE_DIRECTORIES</code>指令</h2><p><code>INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)</code><br>这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的<br>后面，你可以通过两种方式来进行控制搜索路径添加的方式：<br>１. CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。<br>２. 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。</p><h2 id="2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES"><a href="#2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES" class="headerlink" title="2. LINK_DIRECTORIES和 TARGET_LINK_LIBRARIES"></a>2. <code>LINK_DIRECTORIES</code>和 <code>TARGET_LINK_LIBRARIES</code></h2><pre class="line-numbers language-shell"><code class="language-shell">LINK_DIRECTORIES(directory1 directory2 ...)# 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制，在编译时就需要指定一下这些共享库的路径。# TARGET_LINK_LIBRARIES 的全部语法是:TARGET_LINK_LIBRARIES(target library1    <debug | optimized> library2...)# 这个指令可以用来为 target 添加需要链接的共享库<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-FIND系列指令"><a href="#3-FIND系列指令" class="headerlink" title="3. FIND系列指令"></a>3. <code>FIND</code>系列指令</h2><ol><li><p>特殊的环境变量<code>CMAKE_INCLUDE_PATH</code> 和<code>CMAKE_LIBRARY_PATH</code></p><p>务必注意，这两个是环境变量而不是 cmake 变量</p></li><li><p><code>CMAKE_INCLUDE_PATH</code>和<code>CMAKE_LIBRARY_PATH</code>是配合<code>FIND_PATH</code>和<code>FIND_LIBRARY</code>指令使用的</p></li><li><p>find_path指令</p><pre class="line-numbers language-shell"><code class="language-shell">find_path (<VAR> NAMES name)# <VAR>查找的库文件路径报存在变量VAR中# 默认搜索路径为`CMAKE_INCLUDE_PATH`find_path (<VAR> NAMES name PATHS paths... [NO_DEFAULT_PATH])#　指定搜索路径# NO_DEFAULT_PATH　不使用默认搜索路径　# 举例为了将程序更智能一点，我们可以使用 CMAKE_INCLUDE_PATH 来进行，使用 bash 的方法如下：export CMAKE_INCLUDE_PATH=/usr/include/hello然后在头文件中将 INCLUDE_DIRECTORIES(/usr/include/hello)替换为：FIND_PATH(myHeader hello.h)IF(myHeader)    INCLUDE_DIRECTORIES(${myHeader})ENDIF(myHeader)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="4-共享库和头文件指令总结"><a href="#4-共享库和头文件指令总结" class="headerlink" title="4. 共享库和头文件指令总结"></a>4. 共享库和头文件指令总结</h2><ol><li><strong>FIND_PATH</strong> 查找头文件所在目录</li><li><strong>INCLUDE_DIRECTORIES</strong>　添加头文件目录</li><li><strong>FIND_LIBRARY</strong> 查找库文件所在目录</li><li><strong>LINK_DIRECTORIES</strong>   添加库文件目录</li><li><strong>LINK_LIBRARIES</strong>　添加需要链接的库文件路径，注意这里是全路径</li><li><em><em>TARGET_LINK_LIBRARIES </em></em>　给TARGET链接库</li></ol><h1 id="七、Find模块"><a href="#七、Find模块" class="headerlink" title="七、Find模块"></a>七、Find模块</h1><h2 id="1-Find模块使用"><a href="#1-Find模块使用" class="headerlink" title="1.Find模块使用"></a>1.Find模块使用</h2><pre class="line-numbers language-shell"><code class="language-shell">FIND_PACKAGE(XXX)IF(XXX_FOUND)    INCLUDE_DIRECTORIES(${XXX_INCLUDE_DIR})    TARGET_LINK_LIBRARIES(xxxtest ${XXX_LIBRARY})ELSE(XXX_FOUND)    MESSAGE(FATAL_ERROR ”XXX library not found”)ENDIF(XXX_FOUND)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于系统预定义的 Find<name>.cmake 模块，使用方法一般如上例所示：<br>每一个模块都会定义以下几个变量<br>    • <name>_FOUND<br>    • <name>_INCLUDE_DIR or <name>_INCLUDES<br>    • <name>_LIBRARY or <name>_LIBRARIES<br>你可以通过<name>_FOUND 来判断模块是否被找到，如果没有找到，按照工程的需要关闭某些特性、给出提醒或者中止编译</name></name></name></name></name></name></name></p><h2 id="2-find-package指令"><a href="#2-find-package指令" class="headerlink" title="2.find_package指令"></a>2.find_package指令</h2><pre class="line-numbers language-shell"><code class="language-shell">find_package(<PackageName> [QUIET] [REQUIRED] [[COMPONENTS] [components...]]             [OPTIONAL_COMPONENTS components...]             [NO_POLICY_SCOPE])# 查找并从外部项目加载设置，# <PackageName>_FOUND 将设置为指示是否找到该软件包, 如果查找到，该变量为true# [QUIET], 设置该变量，不会打印任何消息，且           <PackageName>_FIND_QUIETLY为true# [REQUIRED] 设置该变量，如果找不到软件包，该选项将停止处理并显示一条错误消息，且设置<PackageName>_FIND_REQUIRED为true,不过不指定该参数，即使没有找到，也能编译通过<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>find_package采用两种模式搜索库：</p><ul><li><strong>Module模式</strong>：搜索<strong>CMAKE_MODULE_PATH</strong>指定路径下的<strong>FindXXX.cmake</strong>文件，执行该文件从而找到XXX库。其中，具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由FindXXX.cmake模块完成。</li><li><strong>Config模式</strong>：搜索<strong>XXX_DIR</strong>指定路径下的<strong>XXXConfig.cmake</strong>文件，执行该文件从而找到XXX库。其中具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由XXXConfig.cmake模块完成。</li></ul><p>两种模式看起来似乎差不多，不过cmake默认采取<strong>Module</strong>模式，如果Module模式未找到库，才会采取Config模式。如果<strong>XXX_DIR</strong>路径下找不到XXXConfig.cmake或<code>&lt;lower-case-package-name&gt;</code>config.cmake文件，则会找/usr/local/lib/cmake/XXX/中的XXXConfig.cmake文件。总之，Config模式是一个备选策略。通常，库安装时会拷贝一份XXXConfig.cmake到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。</p><p>总结：CMake搜索的顺序为: 首先在<code>CMAKE_MODULE_PATH</code>中搜索名为<code>Find&lt;PackageName&gt;.cmake</code>的文件，然后在<code>&lt;PackageName&gt;_DIR</code>名为<code>PackageName&gt;Config.cmake</code>或<code>&lt;lower-case-package-name&gt;-config.cmake</code>的文件，如果还是找不到，则会去<code>/usr/local/lib/cmake</code>中查找<code>Find&lt;PackageName&gt;.cmake</code>文件。</p><p>所以我们可以通过<code>CMAKE_MODULE_PATH</code>或<code>&lt;PackageName&gt;_DIR</code>变量指定cmake文件路径。</p><h2 id="3-自定义Find模块"><a href="#3-自定义Find模块" class="headerlink" title="3.自定义Find模块"></a>3.自定义Find模块</h2><pre class="line-numbers language-shell"><code class="language-shell"># 查找HELLO的头文件目录FIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello/usr/local/include/hello)# 查找HELLO的动态库FIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib/usr/local/lib)IF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)    SET(HELLO_FOUND TRUE)ENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)IF (HELLO_FOUND)    # 如果不指定QUIET参数，就打印信息    IF (NOT HELLO_FIND_QUIETLY)        MESSAGE(STATUS "Found Hello: ${HELLO_LIBRARY}")    ENDIF (NOT HELLO_FIND_QUIETLY)ELSE (HELLO_FOUND)    # 如果设置了REQUIRED参数就报错    IF (HELLO_FIND_REQUIRED)        MESSAGE(FATAL_ERROR "Could not find hello library")    ENDIF (HELLO_FIND_REQUIRED)ENDIF (HELLO_FOUND)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="八、CMake常用变量"><a href="#八、CMake常用变量" class="headerlink" title="八、CMake常用变量"></a>八、<code>CMake</code>常用变量</h1><h2 id="1-cmake-变量引用的方式："><a href="#1-cmake-变量引用的方式：" class="headerlink" title="1.cmake 变量引用的方式："></a>1.<code>cmake</code> 变量引用的方式：</h2><p>使用${}进行变量的引用。在 IF 等语句中，是直接使用变量名而不通过${}取值</p><h2 id="2-cmake-自定义变量的方式："><a href="#2-cmake-自定义变量的方式：" class="headerlink" title="2.cmake 自定义变量的方式："></a>2.<code>cmake</code> 自定义变量的方式：</h2><p>主要有隐式定义和显式定义两种，前面举了一个隐式定义的例子，就是 PROJECT 指令，他会隐式的定义<projectname>_BINARY_DIR 和<projectname>_SOURCE_DIR 两个变量。<br>显式定义的例子我们前面也提到了，使用 SET 指令，就可以构建一个自定义变量了。比如:</projectname></projectname></p><p>SET(HELLO_SRC main.SOURCE_PATHc)，就PROJECT_BINARY_DIR 可以通过${HELLO_SRC}来引用这个自定义变量了.</p><h2 id="3-cmake-常用变量"><a href="#3-cmake-常用变量" class="headerlink" title="3.cmake 常用变量"></a>3.<code>cmake</code> 常用变量</h2><h3 id="1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR"><a href="#1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR" class="headerlink" title="1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/_BINARY_DIR_"></a>1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/<projectname>_BINARY_DIR_</projectname></h3><p>这三个变量指代的内容是一致的，如果是 in source 编译，指得就是工程顶层目录，如果是 out-of-source 编译，指的是工程编译发生的目录。PROJECT_BINARY_DIR 跟其他指令稍有区别，现在，你可以理解为他们是一致的。</p><h3 id="2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR"><a href="#2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR" class="headerlink" title="2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/_SOURCE_DIR"></a>2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/<projectname>_SOURCE_DIR</projectname></h3><p>这三个变量指代的内容是一致的，不论采用何种编译方式，都是工程顶层目录。</p><h3 id="3-CMAKE-CURRENT-SOURCE-DIR"><a href="#3-CMAKE-CURRENT-SOURCE-DIR" class="headerlink" title="3. CMAKE_CURRENT_SOURCE_DIR"></a>3. CMAKE_CURRENT_SOURCE_DIR</h3><p>指的是<strong>当前处理的</strong> CMakeLists.txt 所在的路径</p><h3 id="4-CMAKE-CURRRENT-BINARY-DIR"><a href="#4-CMAKE-CURRRENT-BINARY-DIR" class="headerlink" title="4. CMAKE_CURRRENT_BINARY_DIR"></a>4. CMAKE_CURRRENT_BINARY_DIR</h3><p>如果是 in-source 编译，它跟 CMAKE_CURRENT_SOURCE_DIR 一致，如果是 out-ofsource 编译，他指的是 target 编译目录。<br>使用我们上面提到的 ADD_SUBDIRECTORY(src bin)可以更改这个变量的值。<br>使用 SET(EXECUTABLE_OUTPUT_PATH &lt;新路径&gt;)并不会对这个变量造成影响，它仅仅修改了最终目标文件存放的路径。</p><h3 id="５-CMAKE-CURRENT-LIST-FILE"><a href="#５-CMAKE-CURRENT-LIST-FILE" class="headerlink" title="５. CMAKE_CURRENT_LIST_FILE"></a>５. CMAKE_CURRENT_LIST_FILE</h3><p>​    输出调用这个变量的 CMakeLists.txt 的完整路径</p><h3 id="6-CMAKE-CURRENT-LIST-LINE"><a href="#6-CMAKE-CURRENT-LIST-LINE" class="headerlink" title="6. CMAKE_CURRENT_LIST_LINE"></a>6. CMAKE_CURRENT_LIST_LINE</h3><p>​    输出这个变量所在的行</p><h3 id="7-CMAKE-MODULE-PATH"><a href="#7-CMAKE-MODULE-PATH" class="headerlink" title="7. CMAKE_MODULE_PATH"></a>7. CMAKE_MODULE_PATH</h3><p>这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂，有可能会自己编写一些 cmake 模块，这些 cmake 模块是随你的工程发布的，为了让 cmake 在处理CMakeLists.txt 时找到这些模块，你需要通过 SET 指令，将自己的 cmake 模块路径设<br>置一下。比如<br>SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)<br>这时候你就可以通过 INCLUDE 指令来调用自己的模块了。</p><h3 id="8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH"><a href="#8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH" class="headerlink" title="8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH"></a>8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH</h3><p>分别用来重新定义最终结果的存放目录，前面我们已经提到了这两个变量。</p><h3 id="9-PROJECT-NAME"><a href="#9-PROJECT-NAME" class="headerlink" title="9. PROJECT_NAME"></a>9. PROJECT_NAME</h3><p>返回通过 PROJECT 指令定义的项目名称。</p><h2 id="4-cmake-调用环境变量的方式"><a href="#4-cmake-调用环境变量的方式" class="headerlink" title="4. cmake 调用环境变量的方式"></a>4. cmake 调用环境变量的方式</h2><p>使用$ENV{NAME}指令就可以调用系统的环境变量了。<br>比如MESSAGE(STATUS “HOME dir: $ENV{HOME}”)<br>设置环境变量的方式是：SET(ENV{变量名} 值)</p><h3 id="1-CMAKE-INCLUDE-CURRENT-DIR"><a href="#1-CMAKE-INCLUDE-CURRENT-DIR" class="headerlink" title="1. CMAKE_INCLUDE_CURRENT_DIR"></a>1. CMAKE_INCLUDE_CURRENT_DIR</h3><p>自动添加 CMAKE_CURRENT_BINARY_DIR 和 CMAKE_CURRENT_SOURCE_DIR 到当前处理<br>的 CMakeLists.txt。相当于在每个 CMakeLists.txt 加入：<br>INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR}<br>${CMAKE_CURRENT_SOURCE_DIR})</p><h3 id="2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE"><a href="#2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE" class="headerlink" title="2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE"></a>2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE</h3><p>将工程提供的头文件目录始终至于系统头文件目录的前面，当你定义的头文件确实跟系统发生冲突时可以提供一些帮助。</p><h3 id="3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。"><a href="#3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。" class="headerlink" title="3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。"></a>3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。</h3><h2 id="5-系统信息"><a href="#5-系统信息" class="headerlink" title="5. 系统信息"></a>5. 系统信息</h2><ol><li><p>CMAKE_MAJOR_VERSION，CMAKE 主版本号，比如 2.4.6 中的 2</p></li><li><p>CMAKE_MINOR_VERSION，CMAKE 次版本号，比如 2.4.6 中的 4</p></li><li><p>CMAKE_PATCH_VERSION，CMAKE 补丁等级，比如 2.4.6 中的 6</p></li><li><p>CMAKE_SYSTEM，系统名称，比如 Linux-2.6.22</p></li><li><p>CMAKE_SYSTEM_NAME，不包含版本的系统名，比如 Linux</p></li><li><p>CMAKE_SYSTEM_VERSION，系统版本，比如 2.6.22</p></li><li><p>CMAKE_SYSTEM_PROCESSOR，处理器名称，比如 i686.</p></li><li><p>UNIX，在所有的类 UNIX 平台为 TRUE，包括 OS X 和 cygwin</p></li><li><p>WIN32，在所有的 win32 平台为 TRUE，包括 cygwin</p></li></ol><h2 id="6-主要的开关选项："><a href="#6-主要的开关选项：" class="headerlink" title="6.主要的开关选项："></a>6.主要的开关选项：</h2><ol><li><p>CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS，用来控制 IF ELSE 语句的书写方式，在<br>下一节语法部分会讲到。</p></li><li><p>BUILD_SHARED_LIBS<br>这个开关用来控制默认的库编译方式，如果不进行设置，使用 ADD_LIBRARY 并没有指定库类型的情况下，默认编译生成的库都是静态库。<br>如果 SET(BUILD_SHARED_LIBS ON)后，默认生成的为动态</p></li><li><p>CMAKE_C_FLAGS<br>设置 C 编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p></li><li><p>CMAKE_CXX_FLAGS<br>设置 C++编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p></li></ol><h1 id="九、CMake常用指令"><a href="#九、CMake常用指令" class="headerlink" title="九、CMake常用指令"></a>九、<code>CMake</code>常用指令</h1><h2 id="1-基本指令"><a href="#1-基本指令" class="headerlink" title="1. 基本指令"></a>1. 基本指令</h2><h3 id="MESSAGE"><a href="#MESSAGE" class="headerlink" title="MESSAGE"></a>MESSAGE</h3><pre class="line-numbers language-shell"><code class="language-shell">message([<mode>] "message to display" ...)可选<mode>关键字确定消息的类型:FATAL_ERROR    立即终止所有 cmake 过程SEND_ERROR 产生错误，生成过程被跳过WARNINGAUTHOR_WARNINGNOTICESTATUS    输出前缀为—的信息VERBOSEDEBUGTRACE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="PROJECT"><a href="#PROJECT" class="headerlink" title="PROJECT"></a>PROJECT</h3><pre class="line-numbers language-shell"><code class="language-shell">project(<PROJECT-NAME> [<language-name>...])project(<PROJECT-NAME>        [VERSION <major>[.<minor>[.<patch>[.<tweak>]]]]        [LANGUAGES <language-name>...])设置项目的名称，并将其存储在变量中 PROJECT_NAME。从顶层调用时， CMakeLists.txt还将项目名称存储在变量CMAKE_PROJECT_NAME中。同时设置变量PROJECT_SOURCE_DIR， <PROJECT-NAME>_SOURCE_DIRPROJECT_BINARY_DIR， <PROJECT-NAME>_BINARY_DIRhttps://cmake.org/cmake/help/v3.15/command/project.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="SET"><a href="#SET" class="headerlink" title="SET"></a>SET</h3><pre class="line-numbers language-shell"><code class="language-shell">将普通变量，缓存变量或环境变量设置为给定值。指定<value>...占位符的此命令的签名期望零个或多个参数。多个参数将以分号分隔的列表形式加入，以形成要设置的实际变量值。零参数将导致未设置普通变量。unset() 命令显式取消设置变量。1、设置正常变量set(<variable> <value>... [PARENT_SCOPE])<variable>在当前函数或目录范围内设置给定值。如果PARENT_SCOPE给出了该选项，则将在当前作用域上方的作用域中设置变量。2、设置缓存变量set(<variable> <value>... CACHE <type> <docstring> [FORCE])3、设置环境变量set(ENV{<variable>} [<value>])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="add-executable"><a href="#add-executable" class="headerlink" title="add_executable"></a>add_executable</h3><pre class="line-numbers language-shell"><code class="language-shell">使用指定的源文件生成可执行文件add_executable(<name> [WIN32] [MACOSX_BUNDLE]               [EXCLUDE_FROM_ALL]               [source1] [source2 ...])<name>可执行文件名, <name>与逻辑目标名称相对应，并且在项目中必须是全局唯一的。构建的可执行文件的实际文件名是基于本机平台（例如<name>.exe或<name>）的约定构造的 。默认情况下，将在与调用命令的源树目录相对应的构建树目录中创建可执行文件。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="add-subdirectory"><a href="#add-subdirectory" class="headerlink" title="add_subdirectory"></a>add_subdirectory</h3><pre class="line-numbers language-shell"><code class="language-shell">在构建中添加一个子目录。add_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])将一个子目录添加到构建中。source_dir指定源CMakeLists.txt和代码文件所在的目录。binary_dir指定了输出文件放置的目录以及编译输出的路径。EXCLUDE_FROM_ALL 参数的含义是将这个目录从编译过程中排除，比如，工程的 example，可能就需要工程构建完成后，再进入 example 目录单独进行构建(当然，你也可以通过定义依赖来解决此类问题)。如果没有指定binary_dir,那么编译结果(包括中间结果)都将存放在build/source_dir 目录(这个目录跟原有的 source_dir 目录对应)，指定binary_dir 目录后，相当于在编译时将 source_dir 重命名为binary_dir，所有的中间结果和目标二进制都将存放在binary_dir 目录。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="subdirs"><a href="#subdirs" class="headerlink" title="subdirs"></a>subdirs</h3><pre class="line-numbers language-shell"><code class="language-shell">构建多个子目录subdirs(dir1 dir2 ...[EXCLUDE_FROM_ALL exclude_dir1 exclude_dir2 ...]        [PREORDER] )不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录)，我们都可以通过 SET 指令重新定义EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 变量来指定最终的目标二进制的位置(指最终生成的 hello 或者最终的共享库，不包含编译生成的中间文件)SET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)SET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)在第一节我们提到了<projectname>_BINARY_DIR 和 PROJECT_BINARY_DIR 变量，他们指的编译发生的当前目录，如果是内部编译，就相当于 PROJECT_SOURCE_DIR 也就是工程代码所在目录，如果是外部编译，指的是外部编译所在目录，也就是本例中的两个指令分别定义了：可执行二进制的输出路径为 build/bin 和库的输出路径为 build/lib.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="add-library"><a href="#add-library" class="headerlink" title="add_library"></a>add_library</h3><pre class="line-numbers language-shell"><code class="language-shell">ADD_LIBRARY(libname [SHARED|STATIC|MODULE][EXCLUDE_FROM_ALL]source1 source2 ... sourceN)你不需要写全 libhello.so，只需要填写 hello 即可，cmake 系统会自动为你生成libhello.X类型有三种:SHARED，动态库STATIC，静态库MODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="include-directories"><a href="#include-directories" class="headerlink" title="include_directories"></a>include_directories</h3><pre class="line-numbers language-shell"><code class="language-shell">将include目录添加到构建中include_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...])将给定目录添加到编译器用于搜索头文件的路径中。这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的后面，你可以通过两种方式来进行控制搜索路径添加的方式：１，CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。２，通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="target-link-libraries-amp-link-directories"><a href="#target-link-libraries-amp-link-directories" class="headerlink" title="target_link_libraries &amp; link_directories"></a>target_link_libraries &amp; link_directories</h3><pre class="line-numbers language-shell"><code class="language-shell">TARGET_LINK_LIBRARIES(target library1<debug | optimized> library2...)这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样可以用于为自己编写的共享库添加共享库链接。为了解决我们前面遇到的 HelloFunc 未定义错误，我们需要作的是向src/CMakeLists.txt 中添加如下指令：TARGET_LINK_LIBRARIES(main hello)也可以写成TARGET_LINK_LIBRARIES(main libhello.so)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ADD-DEFINITIONS"><a href="#ADD-DEFINITIONS" class="headerlink" title="ADD_DEFINITIONS"></a>ADD_DEFINITIONS</h3><pre class="line-numbers language-shell"><code class="language-shell">向 C/C++编译器添加-D 定义，比如:ADD_DEFINITIONS(-DENABLE_DEBUG -DABC)，参数之间用空格分割。如果你的代码中定义了#ifdef ENABLE_DEBUG #endif，这个代码块就会生效。如果要添加其他的编译器开关，可以通过 CMAKE_C_FLAGS 变量和 CMAKE_CXX_FLAGS 变量设置。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="ADD-DEPENDENCIES"><a href="#ADD-DEPENDENCIES" class="headerlink" title="ADD_DEPENDENCIES"></a>ADD_DEPENDENCIES</h3><pre class="line-numbers language-shell"><code class="language-shell">定义 target 依赖的其他 target，确保在编译本 target 之前，其他的 target 已经被构建。ADD_DEPENDENCIES(target-name depend-target1depend-target2 ...)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="ADD-TEST-与-ENABLE-TESTING-指令。"><a href="#ADD-TEST-与-ENABLE-TESTING-指令。" class="headerlink" title="ADD_TEST 与 ENABLE_TESTING 指令。"></a>ADD_TEST 与 ENABLE_TESTING 指令。</h3><pre class="line-numbers language-shell"><code class="language-shell">ENABLE_TESTING 指令用来控制 Makefile 是否构建 test 目标，涉及工程所有目录。语法很简单，没有任何参数，ENABLE_TESTING()，一般情况这个指令放在工程的主CMakeLists.txt 中.ADD_TEST 指令的语法是:    `ADD_TEST(testname Exename arg1 arg2 ...)`testname 是自定义的 test 名称，Exename 可以是构建的目标文件也可以是外部脚本等等。后面连接传递给可执行文件的参数。如果没有在同一个 CMakeLists.txt 中打开    ENABLE_TESTING()指令，任何 ADD_TEST 都是无效的。比如我们前面的 Helloworld 例子，可以在工程主 CMakeLists.txt 中添加ADD_TEST(mytest ${PROJECT_BINARY_DIR}/bin/main)ENABLE_TESTING()生成 Makefile 后，就可以运行 make test 来执行测试了。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="AUX-SOURCE-DIRECTORY"><a href="#AUX-SOURCE-DIRECTORY" class="headerlink" title="AUX_SOURCE_DIRECTORY"></a>AUX_SOURCE_DIRECTORY</h3><pre class="line-numbers language-shell"><code class="language-shell">基本语法是：AUX_SOURCE_DIRECTORY(dir VARIABLE)作用是发现一个目录下所有的源代码文件并将列表存储在一个变量中，这个指令临时被用来自动构建源文件列表。因为目前 cmake 还不能自动发现新添加的源文件。比如AUX_SOURCE_DIRECTORY(. SRC_LIST)ADD_EXECUTABLE(main ${SRC_LIST})你也可以通过后面提到的 FOREACH 指令来处理这个 LIST<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###　CMAKE_MINIMUM_REQUIRED</p><pre class="line-numbers language-sehll"><code class="language-sehll">其语法为 CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])比如 CMAKE_MINIMUM_REQUIRED(VERSION 2.5 FATAL_ERROR)如果 cmake 版本小与 2.5，则出现严重错误，整个过程中止。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="EXEC-PROGRAM"><a href="#EXEC-PROGRAM" class="headerlink" title="EXEC_PROGRAM"></a>EXEC_PROGRAM</h3><p>在 CMakeLists.txt 处理过程中执行命令，并不会在生成的 Makefile 中执行。具体语法为：</p><pre class="line-numbers language-shell"><code class="language-shell">EXEC_PROGRAM(Executable [directory in which to run][ARGS <arguments to executable>][OUTPUT_VARIABLE <var>][RETURN_VALUE <var>])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>用于在指定的目录运行某个程序，通过 ARGS 添加参数，如果要获取输出和返回值，可通过OUTPUT_VARIABLE 和 RETURN_VALUE 分别定义两个变量.<br>这个指令可以帮助你在 CMakeLists.txt 处理过程中支持任何命令，比如根据系统情况去修改代码文件等等。<br>举个简单的例子，我们要在 src 目录执行 ls 命令，并把结果和返回值存下来。<br>可以直接在 src/CMakeLists.txt 中添加：<br>EXEC_PROGRAM(ls ARGS “<em>.c” OUTPUT_VARIABLE LS_OUTPUT RETURN_VALUE LS_RVALUE)<br>IF(not LS_RVALUE)<br>    MESSAGE(STATUS “ls result: “ ${LS_OUTPUT})<br>ENDIF(not LS_RVALUE)<br>在 cmake 生成 Makefile 的过程中，就会执行 ls 命令，如果返回 0，则说明成功执行，<br>那么就输出 ls </em>.c 的结果。关于 IF 语句，后面的控制指令会提到。</p><h3 id="FILE-指令"><a href="#FILE-指令" class="headerlink" title="FILE 指令"></a>FILE 指令</h3><p>文件操作指令，基本语法为:</p><pre class="line-numbers language-shell"><code class="language-shell">FILE(WRITE filename "message to write"... )FILE(APPEND filename "message to write"... )FILE(READ filename variable)FILE(GLOB variable [RELATIVE path] [globbingexpressions]...)FILE(GLOB_RECURSE variable [RELATIVE path][globbing expressions]...)FILE(REMOVE [directory]...)FILE(REMOVE_RECURSE [directory]...)FILE(MAKE_DIRECTORY [directory]...)FILE(RELATIVE_PATH variable directory file)FILE(TO_CMAKE_PATH path result)FILE(TO_NATIVE_PATH path result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的语法都比较简单，不在展开介绍了。</p><h3 id="INCLUDE-指令"><a href="#INCLUDE-指令" class="headerlink" title="INCLUDE 指令"></a>INCLUDE 指令</h3><pre class="line-numbers language-shell"><code class="language-shell">用来载入 CMakeLists.txt 文件，也用于载入预定义的 cmake 模块.    INCLUDE(file1 [OPTIONAL])    INCLUDE(module [OPTIONAL])OPTIONAL 参数的作用是文件不存在也不会产生错误。你可以指定载入一个文件，如果定义的是一个模块，那么将在 CMAKE_MODULE_PATH 中搜索这个模块并载入。载入的内容将在处理到 INCLUDE 语句是直接执行。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-控制指令："><a href="#2-控制指令：" class="headerlink" title="2. 控制指令："></a>2. 控制指令：</h2><h3 id="1-IF-指令"><a href="#1-IF-指令" class="headerlink" title="1. IF 指令"></a>1. IF 指令</h3><p>基本语法为：</p><pre class="line-numbers language-shell"><code class="language-shell">IF(expression)# THEN section.COMMAND1(ARGS ...)COMMAND2(ARGS ...)...ELSE(expression)# ELSE section.COMMAND1(ARGS ...)COMMAND2(ARGS ...)...ENDIF(expression)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>另外一个指令是 ELSEIF，总体把握一个原则，凡是出现 IF 的地方一定要有对应的<br>ENDIF.出现 ELSEIF 的地方，ENDIF 是可选的。<br>表达式的使用方法如下:<br>IF(var)，如果变量不是：空，0，N, NO, OFF, FALSE, NOTFOUND 或<br><var>_NOTFOUND 时，表达式为真。<br>IF(NOT var )，与上述条件相反。<br>IF(var1 AND var2)，当两个变量都为真是为真。<br>IF(var1 OR var2)，当两个变量其中一个为真时为真。<br>IF(COMMAND cmd)，当给定的 cmd 确实是命令并可以调用是为真。<br>IF(EXISTS dir)或者 IF(EXISTS file)，当目录名或者文件名存在时为真。<br>IF(file1 IS_NEWER_THAN file2)，当 file1 比 file2 新，或者 file1/file2 其中有一个不存在时为真，文件名请使用完整路径。<br>IF(IS_DIRECTORY dirname)，当 dirname 是目录时，为真。<br>IF(variable MATCHES regex)<br>IF(string MATCHES regex)<br>当给定的变量或者字符串能够匹配正则表达式 regex 时为真。比如：<br>IF(“hello” MATCHES “ell”)<br>MESSAGE(“true”)<br>ENDIF(“hello” MATCHES “ell”)<br>IF(variable LESS number)<br>IF(string LESS number)<br>IF(variable GREATER number)<br>IF(string GREATER number)<br>IF(variable EQUAL number)<br>IF(string EQUAL number)<br>数字比较表达式<br>IF(variable STRLESS string)<br>IF(string STRLESS string)<br>IF(variable STRGREATER string)<br>IF(string STRGREATER string)<br>IF(variable STREQUAL string)<br>IF(string STREQUAL string)<br>按照字母序的排列进行比较.<br>IF(DEFINED variable)，如果变量被定义，为真。<br>一个小例子，用来判断平台差异：<br>IF(WIN32)<br>MESSAGE(STATUS “This is windows.”)</var></p><p>#作一些 Windows 相关的操作<br>ELSE(WIN32)<br>MESSAGE(STATUS “This is not windows”)</p><p>#作一些非 Windows 相关的操作<br>ENDIF(WIN32)<br>上述代码用来控制在不同的平台进行不同的控制，但是，阅读起来却并不是那么舒服，<br>ELSE(WIN32)之类的语句很容易引起歧义。<br>这就用到了我们在“常用变量”一节提到的 CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS 开<br>关。<br>可以 SET(CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS ON)<br>这时候就可以写成:<br>IF(WIN32)<br>ELSE()<br>ENDIF()<br>如果配合 ELSEIF 使用，可能的写法是这样:<br>IF(WIN32)</p><p>#do something related to WIN32<br>ELSEIF(UNIX)</p><p>#do something related to UNIX<br>ELSEIF(APPLE)</p><p>#do something related to APPLE<br>ENDIF(WIN32)</p><h3 id="2-WHILE"><a href="#2-WHILE" class="headerlink" title="2. WHILE"></a>2. WHILE</h3><p>WHILE 指令的语法是：</p><pre class="line-numbers language-shell"><code class="language-shell">WHILE(condition)COMMAND1(ARGS ...)COMMAND2(ARGS ...)...ENDWHILE(condition)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其真假判断条件可以参考 IF 指令。</p><h3 id="3-FOREACH"><a href="#3-FOREACH" class="headerlink" title="3. FOREACH"></a>3. FOREACH</h3><p>FOREACH 指令的使用方法有三种形式：</p><pre class="line-numbers language-shell"><code class="language-shell">1，列表FOREACH(loop_var arg1 arg2 ...)COMMAND1(ARGS ...)COMMAND2(ARGS ...)...ENDFOREACH(loop_var)像我们前面使用的 AUX_SOURCE_DIRECTORY 的例子AUX_SOURCE_DIRECTORY(. SRC_LIST)FOREACH(F ${SRC_LIST})MESSAGE(${F})ENDFOREACH(F)2，范围FOREACH(loop_var RANGE total)ENDFOREACH(loop_var)从 0 到 total 以１为步进举例如下：FOREACH(VAR RANGE 10)MESSAGE(${VAR})ENDFOREACH(VAR)最终得到的输出是：0 1 2 3 4 5 6 7 8 910３，范围和步进FOREACH(loop_var RANGE start stop [step])ENDFOREACH(loop_var)从 start 开始到 stop 结束，以 step 为步进，举例如下FOREACH(A RANGE 5 15 3)MESSAGE(${A})ENDFOREACH(A)最终得到的结果是：5 81114这个指令需要注意的是，知道遇到 ENDFOREACH 指令，整个语句块才会得到真正的执行。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="十、CMakeLists配置模板"><a href="#十、CMakeLists配置模板" class="headerlink" title="十、CMakeLists配置模板"></a>十、<code>CMakeLists</code>配置模板</h1><h2 id="１-基本配置"><a href="#１-基本配置" class="headerlink" title="１.基本配置"></a>１.基本配置</h2><pre class="line-numbers language-shell"><code class="language-shell">cmake_minimum_required(VERSION 3.14)project(XXX_Project)# 设置CMAKE版本set(CMAKE_CXX_STANDARD 14)# 设置输出目录为 build/Debug/bin build/Debug/lib# 并缓存路径set(OUTPUT_DIRECTORY_ROOT ${CMAKE_CURRENT_SOURCE_DIR}/build/${CMAKE_BUILD_TYPE})set(CMAKE_RUNTIME_OUTPUT_DIRECTORY "${OUTPUT_DIRECTORY_ROOT}/bin" CACHE PATH "Runtime directory" FORCE)set(CMAKE_LIBRARY_OUTPUT_DIRECTORY "${OUTPUT_DIRECTORY_ROOT}/lib" CACHE PATH "Library directory" FORCE)set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY "${OUTPUT_DIRECTORY_ROOT}/lib" CACHE PATH "Archive directory" FORCE)# 添加src子目录add_subdirectory(src)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="２-依赖库相关配置"><a href="#２-依赖库相关配置" class="headerlink" title="２.依赖库相关配置"></a>２.依赖库相关配置</h2><p><strong><code>OPenCV</code>依赖库</strong></p><p>将<code>OpenCV</code>依赖库下的<code>share/OpenCV</code>中，<code>OpenCVConfig.cmake</code>复制一份叫<code>FindOpenCV.cmake</code>，然后在根目录的CMakeLists.txt添加如下配置</p><pre class="line-numbers language-shell"><code class="language-shell">#　添加make文件搜索路径set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ~/3rdparty/OpenCV-3.4.7/share/OpenCV)# 查找cmake文件，并初始化变量find_package(OpenCV REQUIRED)# 添加头文件搜索路径include_directories(${OpenCV_INCLUDE_DIRS})# 给执行程序添加链接库add_executable(XXXXMain main.cpp)target_link_libraries(XXXXMain ${OpenCV_LIBS})<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="十一、参考"><a href="#十一、参考" class="headerlink" title="十一、参考"></a>十一、参考</h1><ol><li>[<a href="http://file.ncnynl.com/ros/CMake%20Practice.pdf]" target="_blank" rel="noopener">http://file.ncnynl.com/ros/CMake%20Practice.pdf]</a>(<a href="http://file.ncnynl.com/ros/CMake" target="_blank" rel="noopener">http://file.ncnynl.com/ros/CMake</a> Practice.pdf)</li><li><a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html" target="_blank" rel="noopener">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMake </tag>
            
            <tag> Make </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github博客搭建</title>
      <link href="/2019/11/15/hexo-github-bo-ke-da-jian/"/>
      <url>/2019/11/15/hexo-github-bo-ke-da-jian/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=4913023&auto=1&height=66"></iframe><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>​    <strong>准备工作</strong></p><ul><li>Github账号</li><li>node.js、hexo、npm安装</li></ul><h1 id="一、安装node-js"><a href="#一、安装node-js" class="headerlink" title="一、安装node.js"></a>一、安装node.js</h1><ol><li><p>下载windows版node.js</p><p>下载地址: <a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a></p><p>选择Windows Installer(.msi) 64-bit</p></li><li><p>双击node-v12.13.0-x64.msi, 一直next安装完成</p></li><li><p>测试是否安装成功</p><p>win+R键，输入cmd,然后回车，打开cmd窗口</p><p>输入node -v     显示node.js版本</p><p>输入npm -v     显示npm版本</p><p>安装完成</p></li></ol><h1 id="二、安装hexo"><a href="#二、安装hexo" class="headerlink" title="二、安装hexo"></a>二、安装hexo</h1><ol><li><p>先创建hexo的安装目录, 例如:  F:\LearnSpace\Blog</p></li><li><p>cd Blob  进入Blob目录</p></li><li><p>npm install hexo-cli -g    安装hexo</p></li><li><p>hexo -v  验证是否安装成功</p></li><li><p>npm init blog    初始化blog文件夹，存放博客</p></li><li><p>npm install 安装必备组件</p></li><li><p>cd blog</p></li><li><p>hexo g    生成静态网页</p></li><li><p>hexo s     打开本地服务器</p></li><li><p><a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a>    打开网页</p></li><li><p>ctrl + c   关闭本地服务器</p></li></ol><h1 id="三、连接Github与本地"><a href="#三、连接Github与本地" class="headerlink" title="三、连接Github与本地"></a>三、连接Github与本地</h1><ol><li><p>新建一个名为<code>你的github用户名.github.io</code>的仓库，比如说，如果你的<code>Github</code>用户名是test，那么你就新建<code>test.github.io</code>的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是<code>http://test.github.io</code> 了。</p><p>点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>, 就会像下面一样</p><p><img src="2.png" alt></p></li><li><p>修改配置文件</p><p>编辑blog根目录下的<code>_config.yml</code>, 修改最后一行的配置</p></li></ol><pre><code>deploy:  type: git  repository: https://github.com/981935539/981935539.github.io.git  branch: master</code></pre><ol start="3"><li>安装Git部署插件: <code>npm install hexo-deployer-git --save</code></li></ol><h1 id="四、编辑第一篇博客"><a href="#四、编辑第一篇博客" class="headerlink" title="四、编辑第一篇博客"></a>四、编辑第一篇博客</h1><pre><code>hexo new post &quot;first-article&quot;  # 创建第一篇博客hexo g  # 生成静态网页hexo s  # 本地预览效果hexo d  # 上传github</code></pre><p>此时可以在github.io主页就能看到发布的文章啦。</p><h1 id="五、绑定域名"><a href="#五、绑定域名" class="headerlink" title="五、绑定域名"></a>五、绑定域名</h1><ol><li>以阿里云为例，如下图所示，添加两条解析记录:</li></ol><p>​    <img src="1.png" alt></p><ol start="2"><li><p>然后打开你的Github博客项目，点击<code>settings</code>，拉到下面<code>Custom domain</code>处，填上你自己的域名，保存</p></li><li><p>这时候你的<code>F:\LearnSpace\Blog\blob\source</code> 会出现一个CNAME的文件</p></li><li><p>如果没有CNAME文件</p><p>打开你本地博客<code>/source</code>目录，我的是<code>F:\LearnSpace\Blog\blob\source</code>，新建<code>CNAME</code>文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行<code>hexo g</code>、<code>hexo d</code>上传到Github。</p></li></ol><h1 id="六、hexo常用命令"><a href="#六、hexo常用命令" class="headerlink" title="六、hexo常用命令"></a>六、hexo常用命令</h1><pre><code>npm install hexo-cli -g      # 安装hexonpm uninstall hexo-cli -g      # 卸载hexohexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&#39;ctrl + c&#39;关闭server）hexo deploy #部署到GitHubhexo help  # 查看帮助hexo version  #查看Hexo的版本# 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy# 组合hexo s -g #生成并本地预览hexo d -g #生成并上传</code></pre><h1 id="七、写博客的规范"><a href="#七、写博客的规范" class="headerlink" title="七、写博客的规范"></a>七、写博客的规范</h1><ol><li><p>_config.yml</p><p>冒号后面必须有一个空格，否则会出问题</p></li><li><p>图片</p><p>引用图片需要把图片放在对应的文件夹中，只需要写文件名就可以了</p></li><li><p>文章头设置</p><p>模板在/scaffolds/post.md</p><pre><code>--- title: {{ title }} # 文章名称date: {{ date }} # 文章生成时间top: false cover: false password: toc: true mathjax: true summary: tags:-- [tag1]-- [tag2]-- [tag3]categories: -- [cat1]---</code></pre></li></ol><h1 id="八、备份博客源文件"><a href="#八、备份博客源文件" class="headerlink" title="八、备份博客源文件"></a>八、备份博客源文件</h1><p>​    博客已经搭建完成，但是博客仓库只是保存生成的静态网页文件，是没有博客源文件的，如果电脑出现了问题，那就麻烦了，所以源文件也需要备份一下。</p><ol><li><p>在<code>Github</code>上创建一个与本地仓库同名的仓库, 我的是<code>hexo-matery</code></p></li><li><p>初始化本地仓库</p><pre class="line-numbers language-shell"><code class="language-shell">git init       添加.gitignore文件.gitignore    .DS_Store    Thumbs.db    *.log    public/    .deploy*/    .vscode/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><ol start="3"><li><p>连接到远程<code>Github</code>,</p><pre class="line-numbers language-shell"><code class="language-shell">git remote add github git@github.com:981935539/hexo-matery.gitgit fetchgit merge --allow-unrelated-histories github/master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>推送本地源文件到<code>Github</code></p><pre class="line-numbers language-shell"><code class="language-shell">git add .git commit -m "第一次备份本地仓库"git push --set-upstream github master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><ol start="5"><li><p>现在在任何一台电脑上, 执行<code>git clonegit@github.com:981935539/hexo-matery.git</code></p><p>就可以把博客源文件复制到本地。</p></li></ol><h1 id="九、Ubuntu安装node-js和hexo"><a href="#九、Ubuntu安装node-js和hexo" class="headerlink" title="九、Ubuntu安装node.js和hexo"></a>九、Ubuntu安装node.js和hexo</h1><pre class="line-numbers language-shell"><code class="language-shell">tar -xvf node-v12.13.0-linux-x64.tar.xzsudo mv node-v12.13.0-linux-x64 /usr/localsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/node /usr/local/bin/nodesudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/npm /usr/local/bin/npmsudo npm install -g hexosudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/hexo /usr/local/bin/hexo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="十、参考"><a href="#十、参考" class="headerlink" title="十、参考"></a>十、参考</h1><p>​    <a href="https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9" target="_blank" rel="noopener">https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9</a></p><p>​    <a href="https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html" target="_blank" rel="noopener">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Github </tag>
            
            <tag> Hexo </tag>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
